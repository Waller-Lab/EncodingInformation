{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 10000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 425.96\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 1.577e-01\n",
      "best loss: 427.36\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 7.887e-01\n",
      "best loss: 427.71\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 5.784e-01\n",
      "best loss: 427.99\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 4.732e-01\n",
      "best loss: 428.01\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 9.990e-01\n",
      "best loss: 429.00\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 4.206e-01\n",
      "best loss: 430.66\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 430.86\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 2.629e-01\n",
      "best loss: 431.77\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 436.76\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "best loss: 437.97\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 5.784e-01\n",
      "best loss: 443.25\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 5.258e-02\n",
      "best loss: 443.54\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 0.000e+00\n",
      "best loss: 443.76\t\tLearning rate: 1.000e+01, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 443.79\t\tLearning rate: 7.848e-07, Batch size: 17, Momentum: 7.361e-01\n",
      "best loss: 444.71\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 444.87\t\tLearning rate: 2.069e-05, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 444.96\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 2.103e-01\n",
      "best loss: 445.00\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 445.00\t\tLearning rate: 7.848e-07, Batch size: 32, Momentum: 7.887e-01\n",
      "best loss: 445.16\t\tLearning rate: 2.336e-06, Batch size: 12, Momentum: 8.413e-01\n",
      "best loss: 445.37\t\tLearning rate: 1.000e-08, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 445.57\t\tLearning rate: 2.069e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 445.64\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 9.990e-01\n",
      "best loss: 446.28\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 5.258e-02\n",
      "best loss: 446.35\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 1.052e-01\n",
      "best loss: 446.46\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 446.52\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 5.258e-01\n",
      "best loss: 446.84\t\tLearning rate: 2.069e-05, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 447.19\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "best loss: 447.23\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 5.258e-01\n",
      "best loss: 447.34\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 9.990e-01\n",
      "best loss: 447.40\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 1.577e-01\n",
      "best loss: 447.59\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 8.938e-01\n",
      "best loss: 447.64\t\tLearning rate: 2.976e-08, Batch size: 32, Momentum: 3.155e-01\n",
      "best loss: 447.83\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 5.258e-01\n",
      "best loss: 447.97\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 448.06\t\tLearning rate: 2.976e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "best loss: 448.21\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 5.258e-01\n",
      "best loss: 448.22\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 8.413e-01\n",
      "best loss: 448.28\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 0.000e+00\n",
      "best loss: 448.34\t\tLearning rate: 1.438e-02, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 448.92\t\tLearning rate: 2.069e-05, Batch size: 37, Momentum: 5.784e-01\n",
      "best loss: 448.95\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.464e-01\n",
      "best loss: 448.99\t\tLearning rate: 4.833e-03, Batch size: 37, Momentum: 9.990e-01\n",
      "best loss: 449.98\t\tLearning rate: 2.336e-06, Batch size: 42, Momentum: 6.835e-01\n",
      "best loss: 450.16\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 5.258e-02\n",
      "best loss: 450.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "best loss: 451.26\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "best loss: 451.53\t\tLearning rate: 2.069e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 451.91\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 9.464e-01\n",
      "best loss: 452.03\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 452.39\t\tLearning rate: 8.859e-08, Batch size: 42, Momentum: 8.938e-01\n",
      "best loss: 452.65\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 3.155e-01\n",
      "best loss: 452.75\t\tLearning rate: 2.976e-08, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 453.74\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 454.62\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.206e-01\n",
      "best loss: 455.06\t\tLearning rate: 4.281e-02, Batch size: 37, Momentum: 5.258e-01\n",
      "best loss: 456.06\t\tLearning rate: 6.952e-06, Batch size: 12, Momentum: 8.938e-01\n",
      "best loss: 458.75\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.938e-01\n",
      "best loss: 460.00\t\tLearning rate: 2.336e-06, Batch size: 47, Momentum: 6.309e-01\n",
      "best loss: 462.72\t\tLearning rate: 5.456e-04, Batch size: 34, Momentum: 9.464e-01\n",
      "best loss: 467.84\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 470.54\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 472.83\t\tLearning rate: 7.848e-07, Batch size: 9, Momentum: 2.103e-01\n",
      "best loss: 475.86\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.258e-01\n",
      "best loss: 476.28\t\tLearning rate: 8.859e-08, Batch size: 17, Momentum: 6.309e-01\n",
      "best loss: 485.76\t\tLearning rate: 8.859e-08, Batch size: 24, Momentum: 4.206e-01\n",
      "best loss: 525.40\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 529.74\t\tLearning rate: 2.637e-07, Batch size: 4, Momentum: 0.000e+00\n",
      "best loss: 557.71\t\tLearning rate: 1.000e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 607.52\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 3.681e-01\n",
      "best loss: 665.82\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 5.258e-01\n",
      "best loss: 720.64\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 9.990e-01\n",
      "best loss: 779.94\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "best loss: 802.81\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 819.65\t\tLearning rate: 1.833e-04, Batch size: 47, Momentum: 4.206e-01\n",
      "best loss: 823.32\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 9.990e-01\n",
      "best loss: 854.09\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 877.76\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 9.464e-01\n",
      "best loss: 885.35\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 5.258e-01\n",
      "best loss: 961.51\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 7.361e-01\n",
      "best loss: 987.54\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 1.052e-01\n",
      "best loss: 995.84\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 1015.67\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 1092.16\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 7.887e-01\n",
      "best loss: 1094.73\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "best loss: 1117.94\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "best loss: 1150.17\t\tLearning rate: 4.281e-02, Batch size: 32, Momentum: 8.938e-01\n",
      "best loss: 1154.92\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 5.258e-02\n",
      "best loss: 1159.80\t\tLearning rate: 4.281e-02, Batch size: 2, Momentum: 8.938e-01\n",
      "best loss: 1205.95\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 7.361e-01\n",
      "best loss: 1222.31\t\tLearning rate: 1.129e+00, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 1289.70\t\tLearning rate: 1.129e+00, Batch size: 34, Momentum: 6.835e-01\n",
      "best loss: 1309.10\t\tLearning rate: 3.360e+00, Batch size: 34, Momentum: 6.309e-01\n",
      "best loss: 1316.60\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 1.052e-01\n",
      "best loss: 1359.06\t\tLearning rate: 3.360e+00, Batch size: 50, Momentum: 8.413e-01\n",
      "best loss: 1389.44\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 9.990e-01\n",
      "best loss: 1456.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 9.990e-01\n",
      "best loss: 16947.87\t\tLearning rate: 3.360e+00, Batch size: 14, Momentum: 5.258e-02\n",
      "best loss: 30308.49\t\tLearning rate: 1.624e-03, Batch size: 37, Momentum: 5.258e-02\n",
      "best loss: 33262.88\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 38894.08\t\tLearning rate: 2.976e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "best loss: 42054.76\t\tLearning rate: 1.000e-08, Batch size: 42, Momentum: 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
