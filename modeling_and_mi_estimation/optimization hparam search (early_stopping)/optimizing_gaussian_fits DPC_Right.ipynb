{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 1000\n",
    "channel = 'DPC_Right'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  2406530.5159880687\n",
      "best loss: 893.70\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  2757627.764051999\n",
      "best loss: 498.60\t\tLearning rate: 7.848e-07, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  1297362.5932994783\n",
      "best loss: 1336.25\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  609355.4151318023\n",
      "best loss: 499.57\t\tLearning rate: 6.952e-06, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  1525201.8966025258\n",
      "best loss: 1110.92\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 5.784e-01\n",
      "Initial loss:  3085218.4325907985\n",
      "best loss: 849.17\t\tLearning rate: 1.833e-04, Batch size: 37, Momentum: 9.464e-01\n",
      "Initial loss:  252622.9955842237\n",
      "best loss: 1229.99\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 8.938e-01\n",
      "Initial loss:  1102906.2787131583\n",
      "best loss: 1457.13\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 4.732e-01\n",
      "Initial loss:  2729193.9829202136\n",
      "best loss: 831.46\t\tLearning rate: 5.456e-04, Batch size: 19, Momentum: 5.258e-02\n",
      "Initial loss:  3196259.4266143474\n",
      "best loss: 1371.53\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 4.206e-01\n",
      "Initial loss:  1163757.1796027084\n",
      "best loss: 616.77\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 7.887e-01\n",
      "Initial loss:  2818928.521602185\n",
      "best loss: 790.23\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 3.681e-01\n",
      "Initial loss:  742960.3694319957\n",
      "best loss: 1329.82\t\tLearning rate: 3.793e-01, Batch size: 17, Momentum: 8.938e-01\n",
      "Initial loss:  2396211.5847927406\n",
      "best loss: 1332.14\t\tLearning rate: 3.793e-01, Batch size: 12, Momentum: 8.938e-01\n",
      "Initial loss:  1188992.451562881\n",
      "best loss: 775.47\t\tLearning rate: 6.158e-05, Batch size: 32, Momentum: 8.413e-01\n",
      "Initial loss:  3427502.8172015017\n",
      "best loss: 503.24\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.732e-01\n",
      "Initial loss:  1838475.218912858\n",
      "best loss: 951490.44\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 0.000e+00\n",
      "Initial loss:  1432251.5581115098\n",
      "best loss: 1083.47\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.052e-01\n",
      "Initial loss:  2007286.5122003555\n",
      "best loss: 1011.62\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.732e-01\n",
      "Initial loss:  499.68277405763484\n",
      "best loss: 497.31\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 2.103e-01\n",
      "Initial loss:  976379.1254014015\n",
      "best loss: 1082.89\t\tLearning rate: 4.281e-02, Batch size: 50, Momentum: 3.155e-01\n",
      "Initial loss:  529343.3624759289\n",
      "best loss: 1961788.59\t\tLearning rate: 1.624e-03, Batch size: 2, Momentum: 1.052e-01\n",
      "Initial loss:  2122002.8551010373\n",
      "best loss: 811.45\t\tLearning rate: 1.833e-04, Batch size: 39, Momentum: 7.887e-01\n",
      "Initial loss:  3489824.148267012\n",
      "best loss: 1017.71\t\tLearning rate: 4.833e-03, Batch size: 50, Momentum: 5.258e-01\n",
      "Initial loss:  348459.45499135874\n",
      "best loss: 503.31\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 5.258e-02\n",
      "Initial loss:  3222037.0021153037\n",
      "best loss: 1429211.91\t\tLearning rate: 8.859e-08, Batch size: 34, Momentum: 1.577e-01\n",
      "Initial loss:  514.3762887391034\n",
      "best loss: 503.08\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 3.155e-01\n",
      "Initial loss:  899062.5463539646\n",
      "best loss: 1031.06\t\tLearning rate: 1.438e-02, Batch size: 7, Momentum: 6.309e-01\n",
      "Initial loss:  4973532.317081867\n",
      "best loss: 758.56\t\tLearning rate: 6.158e-05, Batch size: 19, Momentum: 6.835e-01\n",
      "Initial loss:  526.2449860189126\n",
      "best loss: 514.63\t\tLearning rate: 1.624e-03, Batch size: 29, Momentum: 6.309e-01\n",
      "Initial loss:  2706148.7407471575\n",
      "best loss: 782.18\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.681e-01\n",
      "Initial loss:  1370610.5730774668\n",
      "best loss: 965.93\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 2.103e-01\n",
      "Initial loss:  1047365.2746200443\n",
      "best loss: 649.98\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 0.000e+00\n",
      "Initial loss:  2620433.900835756\n",
      "best loss: 552.68\t\tLearning rate: 6.952e-06, Batch size: 27, Momentum: 8.938e-01\n",
      "Initial loss:  1100409.4774535238\n",
      "best loss: 930.44\t\tLearning rate: 4.833e-03, Batch size: 27, Momentum: 0.000e+00\n",
      "Initial loss:  4230049.265487791\n",
      "best loss: 730.28\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 4.732e-01\n",
      "Initial loss:  1947140.3950052666\n",
      "best loss: 477736.71\t\tLearning rate: 2.976e-08, Batch size: 50, Momentum: 5.258e-02\n",
      "Initial loss:  3486078.7574924286\n",
      "best loss: 1254.40\t\tLearning rate: 1.129e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  1560845.0100877236\n",
      "best loss: 1212.88\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 0.000e+00\n",
      "Initial loss:  2706635.2565690866\n",
      "best loss: 903.00\t\tLearning rate: 5.456e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "Initial loss:  3541346.0103013404\n",
      "best loss: 1264.99\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 3.681e-01\n",
      "Initial loss:  1456258.21979106\n",
      "best loss: 524.62\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 2.629e-01\n",
      "Initial loss:  3694796.842592657\n",
      "best loss: 1451923.50\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 8.413e-01\n",
      "Initial loss:  497.7558770931458\n",
      "best loss: 499.99\t\tLearning rate: 4.281e-02, Batch size: 34, Momentum: 8.938e-01\n",
      "Initial loss:  3574251.694228961\n",
      "best loss: 230503.86\t\tLearning rate: 2.976e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "Initial loss:  423814.3761548673\n",
      "best loss: 1324.44\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 0.000e+00\n",
      "Initial loss:  3047703.492007182\n",
      "best loss: 1451073.65\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 7.361e-01\n",
      "Initial loss:  1292303.465612641\n",
      "best loss: 500.34\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 3.155e-01\n",
      "Initial loss:  4699319.756822717\n",
      "best loss: 525.06\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "Initial loss:  1121429.8444792668\n",
      "best loss: 1257.72\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 2.629e-01\n",
      "Initial loss:  3393433.9454462216\n",
      "best loss: 1205.63\t\tLearning rate: 1.274e-01, Batch size: 14, Momentum: 4.206e-01\n",
      "Initial loss:  1121992.3155469273\n",
      "best loss: 499.23\t\tLearning rate: 6.952e-06, Batch size: 44, Momentum: 8.938e-01\n",
      "Initial loss:  729312.4554926557\n",
      "best loss: 508.05\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 4.206e-01\n",
      "Initial loss:  2544179.4167872295\n",
      "best loss: 533.25\t\tLearning rate: 2.336e-06, Batch size: 34, Momentum: 8.413e-01\n",
      "Initial loss:  450960.52570801985\n",
      "best loss: 707.34\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 1.577e-01\n",
      "Initial loss:  3685259.206267618\n",
      "best loss: 1333.33\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 7.361e-01\n",
      "Initial loss:  1620427.7319106988\n",
      "best loss: 1357.85\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.577e-01\n",
      "Initial loss:  2647520.2179755378\n",
      "best loss: 1401.16\t\tLearning rate: 3.360e+00, Batch size: 7, Momentum: 9.990e-01\n",
      "Initial loss:  6653354.377021938\n",
      "best loss: 501.57\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 9.464e-01\n",
      "Initial loss:  3325703.332145863\n",
      "best loss: 719.13\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 6.835e-01\n",
      "Initial loss:  5654716.49827205\n",
      "best loss: 1190.85\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 4.206e-01\n",
      "Initial loss:  2024874.1613240745\n",
      "best loss: 502.64\t\tLearning rate: 6.952e-06, Batch size: 47, Momentum: 7.887e-01\n",
      "Initial loss:  4888598.1547627095\n",
      "best loss: 924.30\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "Initial loss:  2909690.989844498\n",
      "best loss: 1330.49\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 5.258e-01\n",
      "Initial loss:  1006292.0795242579\n",
      "best loss: 653.89\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 7.361e-01\n",
      "Initial loss:  3810716.3682139684\n",
      "best loss: 1132.64\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 5.784e-01\n",
      "Initial loss:  582622.7707849623\n",
      "best loss: 1036.50\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 7.887e-01\n",
      "Initial loss:  2447590.884893547\n",
      "best loss: 507.60\t\tLearning rate: 2.336e-06, Batch size: 50, Momentum: 1.052e-01\n",
      "Initial loss:  1229758.9849572899\n",
      "best loss: 1127.20\t\tLearning rate: 1.274e-01, Batch size: 12, Momentum: 0.000e+00\n",
      "Initial loss:  1955968.774687723\n",
      "best loss: 507.76\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 9.990e-01\n",
      "Initial loss:  2664787.931789782\n",
      "best loss: 1176.02\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "Initial loss:  431939.2307911911\n",
      "best loss: 1066.65\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 7.887e-01\n",
      "Initial loss:  818254.9183508337\n",
      "best loss: 755.05\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  1305929.8877355799\n",
      "best loss: 843.22\t\tLearning rate: 1.833e-04, Batch size: 7, Momentum: 1.052e-01\n",
      "Initial loss:  1690592.9308670643\n",
      "best loss: 1697.60\t\tLearning rate: 8.859e-08, Batch size: 12, Momentum: 0.000e+00\n",
      "Initial loss:  4016325.2497134656\n",
      "best loss: 504.06\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 3.681e-01\n",
      "Initial loss:  1423968.4056727549\n",
      "best loss: 1214.74\t\tLearning rate: 3.793e-01, Batch size: 44, Momentum: 3.155e-01\n",
      "Initial loss:  2985626.2467905832\n",
      "best loss: 902.22\t\tLearning rate: 5.456e-04, Batch size: 27, Momentum: 6.835e-01\n",
      "Initial loss:  3404250.555959734\n",
      "best loss: 1126.51\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 8.413e-01\n",
      "Initial loss:  4077719.348355159\n",
      "best loss: 538.14\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 6.309e-01\n",
      "Initial loss:  1469805.6756928554\n",
      "best loss: 922.70\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  1436840.0647010868\n",
      "best loss: 568.93\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 8.413e-01\n",
      "Initial loss:  840175.0160551206\n",
      "best loss: 652973.93\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 5.258e-01\n",
      "Initial loss:  790917.4991514422\n",
      "best loss: 1247.12\t\tLearning rate: 1.274e-01, Batch size: 37, Momentum: 7.361e-01\n",
      "Initial loss:  3837108.7461982607\n",
      "best loss: 1322.01\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 7.361e-01\n",
      "Initial loss:  1940201.6550700585\n",
      "best loss: 502.00\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 7.361e-01\n",
      "Initial loss:  2846109.1494822255\n",
      "best loss: 537.29\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "Initial loss:  1160265.4893784253\n",
      "best loss: 251148.48\t\tLearning rate: 2.336e-06, Batch size: 32, Momentum: 2.629e-01\n",
      "Initial loss:  1420340.5974645372\n",
      "best loss: 628.24\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 6.309e-01\n",
      "Initial loss:  1279027.5658239264\n",
      "best loss: 985.83\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  4066359.311802798\n",
      "best loss: 587.89\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 3.155e-01\n",
      "Initial loss:  483584.254298569\n",
      "best loss: 268269.90\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  2140466.531290736\n",
      "best loss: 1318.22\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  1999506.623768252\n",
      "best loss: 783.04\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "Initial loss:  1270814.2076040616\n",
      "best loss: 595.90\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "Initial loss:  4089493.6744924514\n",
      "best loss: 1332.35\t\tLearning rate: 1.000e+01, Batch size: 7, Momentum: 1.577e-01\n",
      "Initial loss:  501329.5110805488\n",
      "best loss: 1036.05\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  2309149.1444410416\n",
      "best loss: 1128.00\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.887e-01\n",
      "Initial loss:  2010493.6899856706\n",
      "best loss: 504.79\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 6.835e-01\n",
      "Initial loss:  1907438.0656736486\n",
      "best loss: 1148.95\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "Initial loss:  1707458.3949151998\n",
      "best loss: 907.97\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 6.835e-01\n",
      "Initial loss:  2283042.592774346\n",
      "best loss: 780.82\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 5.258e-01\n",
      "Initial loss:  404391.0345608996\n",
      "best loss: 762.84\t\tLearning rate: 6.158e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  870635.8640126783\n",
      "best loss: 497.69\t\tLearning rate: 2.637e-07, Batch size: 42, Momentum: 9.990e-01\n",
      "Initial loss:  694128.9514557469\n",
      "best loss: 819.73\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 4.732e-01\n",
      "Initial loss:  1840296.004436644\n",
      "best loss: 500.48\t\tLearning rate: 8.859e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "Initial loss:  1446668.7400220456\n",
      "best loss: 585.93\t\tLearning rate: 6.952e-06, Batch size: 37, Momentum: 8.413e-01\n",
      "Initial loss:  914776.9061124143\n",
      "best loss: 18101921.87\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 1.577e-01\n",
      "Initial loss:  784642.3895316073\n",
      "best loss: 495.63\t\tLearning rate: 6.952e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  479876.00576177833\n",
      "best loss: 1207.05\t\tLearning rate: 1.274e-01, Batch size: 22, Momentum: 9.464e-01\n",
      "Initial loss:  2540595.8412696635\n",
      "best loss: 1405.23\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 9.990e-01\n",
      "Initial loss:  1087153.9962012973\n",
      "best loss: 515.64\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 3.681e-01\n",
      "Initial loss:  791426.382880604\n",
      "best loss: 807.95\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  1620919.700459596\n",
      "best loss: 1327.73\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 5.784e-01\n",
      "Initial loss:  913520.4312821175\n",
      "best loss: 833.84\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 8.413e-01\n",
      "Initial loss:  1253813.0763416218\n",
      "best loss: 1107.82\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.835e-01\n",
      "Initial loss:  3710404.4296622197\n",
      "best loss: 988.39\t\tLearning rate: 1.624e-03, Batch size: 27, Momentum: 5.784e-01\n",
      "Initial loss:  1249912.7259528912\n",
      "best loss: 1137.10\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 7.361e-01\n",
      "Initial loss:  245981.02987225438\n",
      "best loss: 1088.26\t\tLearning rate: 1.274e-01, Batch size: 29, Momentum: 1.577e-01\n",
      "Initial loss:  3478118.2591166385\n",
      "best loss: 1393.20\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 8.938e-01\n",
      "Initial loss:  2008930.6485558562\n",
      "best loss: 994.93\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 5.258e-02\n",
      "Initial loss:  971400.2618923286\n",
      "best loss: 1126.31\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 9.464e-01\n",
      "Initial loss:  2994766.6926169284\n",
      "best loss: 1189.33\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  2650017.9646483436\n",
      "best loss: 1063107.17\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 3.681e-01\n",
      "Initial loss:  5958895.449251397\n",
      "best loss: 527.97\t\tLearning rate: 2.637e-07, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  142024.13162913857\n",
      "best loss: 498.82\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 4.206e-01\n",
      "Initial loss:  1926846.112762626\n",
      "best loss: 872.51\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 1.052e-01\n",
      "Initial loss:  2045274.2230552512\n",
      "best loss: 1463.78\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 6.309e-01\n",
      "Initial loss:  1079094.1527177393\n",
      "best loss: 468479.64\t\tLearning rate: 8.859e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "Initial loss:  2701359.3809694164\n",
      "best loss: 833.68\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  1503575.9905882424\n",
      "best loss: 1147147.14\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "Initial loss:  3295754.087895749\n",
      "best loss: 515.65\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 5.258e-01\n",
      "Initial loss:  1425660.0066660778\n",
      "best loss: 1056.77\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 9.990e-01\n",
      "Initial loss:  501.48497224184854\n",
      "best loss: 501.72\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 8.413e-01\n",
      "Initial loss:  1777944.648741897\n",
      "best loss: 1119.57\t\tLearning rate: 1.274e-01, Batch size: 47, Momentum: 2.103e-01\n",
      "Initial loss:  1486597.7698234231\n",
      "best loss: 1170.60\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 1.052e-01\n",
      "Initial loss:  2088470.2435811935\n",
      "best loss: 512.98\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  1551872.9393004063\n",
      "best loss: 499.91\t\tLearning rate: 7.848e-07, Batch size: 12, Momentum: 5.258e-01\n",
      "Initial loss:  499.4732347488225\n",
      "best loss: 501.46\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 2.629e-01\n",
      "Initial loss:  1465524.0434796663\n",
      "best loss: 562.85\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 5.784e-01\n",
      "Initial loss:  1249548.4015801079\n",
      "best loss: 595.30\t\tLearning rate: 2.069e-05, Batch size: 29, Momentum: 1.577e-01\n",
      "Initial loss:  3870522.4930483354\n",
      "best loss: 736.16\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 9.990e-01\n",
      "Initial loss:  2272021.955536993\n",
      "best loss: 500.78\t\tLearning rate: 6.952e-06, Batch size: 4, Momentum: 2.103e-01\n",
      "Initial loss:  2775689.829286577\n",
      "best loss: 943.94\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 1.052e-01\n",
      "Initial loss:  1002378.7810937745\n",
      "best loss: 1377.60\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 7.361e-01\n",
      "Initial loss:  494.71799256586587\n",
      "best loss: 498.29\t\tLearning rate: 2.976e-08, Batch size: 17, Momentum: 3.155e-01\n",
      "Initial loss:  1501468.4802368153\n",
      "best loss: 579.57\t\tLearning rate: 2.637e-07, Batch size: 44, Momentum: 2.629e-01\n",
      "Initial loss:  999514.4940356555\n",
      "best loss: 1084.84\t\tLearning rate: 1.438e-02, Batch size: 22, Momentum: 4.206e-01\n",
      "Initial loss:  1067564.0528875878\n",
      "best loss: 1092.53\t\tLearning rate: 4.281e-02, Batch size: 14, Momentum: 3.681e-01\n",
      "Initial loss:  5301509.482191629\n",
      "best loss: 511.82\t\tLearning rate: 2.336e-06, Batch size: 14, Momentum: 4.732e-01\n",
      "Initial loss:  1375484.9955553643\n",
      "best loss: 502.85\t\tLearning rate: 2.336e-06, Batch size: 4, Momentum: 9.990e-01\n",
      "Initial loss:  1293717.068510303\n",
      "best loss: 504.32\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 7.361e-01\n",
      "Initial loss:  746080.1911929419\n",
      "best loss: 849.24\t\tLearning rate: 5.456e-04, Batch size: 9, Momentum: 1.577e-01\n",
      "Initial loss:  1148353.0785270163\n",
      "best loss: 1243.97\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "Initial loss:  754963.6198902983\n",
      "best loss: 580.27\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 0.000e+00\n",
      "Initial loss:  1895060.6459648653\n",
      "best loss: 1139.42\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 7.361e-01\n",
      "Initial loss:  3199116.1655867323\n",
      "best loss: 980.37\t\tLearning rate: 4.833e-03, Batch size: 17, Momentum: 5.258e-01\n",
      "Initial loss:  2487260.7031935826\n",
      "best loss: 1483.58\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 9.464e-01\n",
      "Initial loss:  2784938.3204421643\n",
      "best loss: 1163.22\t\tLearning rate: 1.274e-01, Batch size: 19, Momentum: 3.681e-01\n",
      "Initial loss:  5071649.361447398\n",
      "best loss: 883.62\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 8.938e-01\n",
      "Initial loss:  2046711.7137472418\n",
      "best loss: 1104.32\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 9.990e-01\n",
      "Initial loss:  1354950.4783283914\n",
      "best loss: 568912.67\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 6.309e-01\n",
      "Initial loss:  871790.2536627513\n",
      "best loss: 996.16\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 0.000e+00\n",
      "Initial loss:  3159338.041005481\n",
      "best loss: 1161.13\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.309e-01\n",
      "Initial loss:  1708195.2502715988\n",
      "best loss: 689.69\t\tLearning rate: 6.952e-06, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  4737782.170666984\n",
      "best loss: 1315573.56\t\tLearning rate: 2.976e-08, Batch size: 44, Momentum: 1.052e-01\n",
      "Initial loss:  1913584.504084183\n",
      "best loss: 923318.83\t\tLearning rate: 1.000e-08, Batch size: 34, Momentum: 5.784e-01\n",
      "Initial loss:  2299102.11820204\n",
      "best loss: 818129.48\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 3.681e-01\n",
      "Initial loss:  1074203.0985413569\n",
      "best loss: 237492.91\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 5.784e-01\n",
      "Initial loss:  557152.4962632239\n",
      "best loss: 243829.29\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 3.155e-01\n",
      "Initial loss:  4024010.374448763\n",
      "best loss: 660.80\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 5.258e-02\n",
      "Initial loss:  1204713.929715581\n",
      "best loss: 506.45\t\tLearning rate: 7.848e-07, Batch size: 50, Momentum: 1.052e-01\n",
      "Initial loss:  2715861.064358417\n",
      "best loss: 507.19\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 5.258e-01\n",
      "Initial loss:  2725390.233588424\n",
      "best loss: 508.93\t\tLearning rate: 6.952e-06, Batch size: 19, Momentum: 9.464e-01\n",
      "Initial loss:  3274177.1104208827\n",
      "best loss: 1028.34\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 8.413e-01\n",
      "Initial loss:  3304087.991374482\n",
      "best loss: 636.72\t\tLearning rate: 8.859e-08, Batch size: 29, Momentum: 3.155e-01\n",
      "Initial loss:  783758.4224247943\n",
      "best loss: 1318.66\t\tLearning rate: 1.000e+01, Batch size: 42, Momentum: 5.258e-02\n",
      "Initial loss:  1454849.5604379778\n",
      "best loss: 560.06\t\tLearning rate: 2.976e-08, Batch size: 19, Momentum: 7.361e-01\n",
      "Initial loss:  1487994.2617125823\n",
      "best loss: 1081.17\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 5.258e-02\n",
      "Initial loss:  1088202.839630092\n",
      "best loss: 737860.59\t\tLearning rate: 2.637e-07, Batch size: 7, Momentum: 2.103e-01\n",
      "Initial loss:  1778497.615500904\n",
      "best loss: 1324.61\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 9.464e-01\n",
      "Initial loss:  1254580.960290614\n",
      "best loss: 1351.66\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 5.784e-01\n",
      "Initial loss:  361511.0820298077\n",
      "best loss: 553.49\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 6.835e-01\n",
      "Initial loss:  1247132.6382919324\n",
      "best loss: 1397.65\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 3.681e-01\n",
      "Initial loss:  5287926.936106058\n",
      "best loss: 976.40\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 3.681e-01\n",
      "Initial loss:  174026.35835129453\n",
      "best loss: 1254.57\t\tLearning rate: 1.129e+00, Batch size: 12, Momentum: 2.629e-01\n",
      "Initial loss:  1906438.2140187419\n",
      "best loss: 1045.42\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 6.309e-01\n",
      "Initial loss:  5342704.816559744\n",
      "best loss: 563.45\t\tLearning rate: 6.952e-06, Batch size: 2, Momentum: 9.464e-01\n",
      "Initial loss:  914933.492693711\n",
      "best loss: 997.54\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 4.732e-01\n",
      "Initial loss:  2940001.690709397\n",
      "best loss: 1438.53\t\tLearning rate: 3.360e+00, Batch size: 4, Momentum: 6.835e-01\n",
      "Initial loss:  3878517.027894292\n",
      "best loss: 1019.59\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 5.258e-02\n",
      "Initial loss:  775870.723669066\n",
      "best loss: 1495.67\t\tLearning rate: 3.360e+00, Batch size: 44, Momentum: 9.464e-01\n",
      "Initial loss:  2095141.518021748\n",
      "best loss: 1157.65\t\tLearning rate: 1.438e-02, Batch size: 29, Momentum: 9.464e-01\n",
      "Initial loss:  1176534.011475821\n",
      "best loss: 1157.62\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 1.052e-01\n",
      "Initial loss:  10105876.895043038\n",
      "best loss: 1266.37\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  489.96470332092514\n",
      "best loss: 498.53\t\tLearning rate: 1.000e+01, Batch size: 4, Momentum: 2.629e-01\n",
      "Initial loss:  809048.5626945818\n",
      "best loss: 1424.59\t\tLearning rate: 1.000e+01, Batch size: 14, Momentum: 7.361e-01\n",
      "Initial loss:  1134309.3180977926\n",
      "best loss: 991.57\t\tLearning rate: 1.624e-03, Batch size: 29, Momentum: 6.309e-01\n",
      "Initial loss:  1453979.7997112605\n",
      "best loss: 1056.52\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  2687528.510214267\n",
      "best loss: 810.08\t\tLearning rate: 5.456e-04, Batch size: 37, Momentum: 1.577e-01\n",
      "Initial loss:  3999299.9620868107\n",
      "best loss: 1200.05\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 6.835e-01\n",
      "Initial loss:  1089019.170057949\n",
      "best loss: 1076.45\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 5.258e-01\n",
      "Initial loss:  2684162.743717428\n",
      "best loss: 1321.97\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 5.258e-02\n",
      "Initial loss:  2005723.3023691578\n",
      "best loss: 1173.40\t\tLearning rate: 1.438e-02, Batch size: 4, Momentum: 9.990e-01\n",
      "Initial loss:  1769884.1471905229\n",
      "best loss: 960.82\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 1.577e-01\n",
      "Initial loss:  508.40213878513447\n",
      "best loss: 498.22\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 7.887e-01\n",
      "Initial loss:  1143336.281112167\n",
      "best loss: 509.42\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 9.464e-01\n",
      "Initial loss:  2845726.140256651\n",
      "best loss: 903.89\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 0.000e+00\n",
      "Initial loss:  1807769.1888556953\n",
      "best loss: 505.67\t\tLearning rate: 8.859e-08, Batch size: 37, Momentum: 9.990e-01\n",
      "Initial loss:  2550892.2117812154\n",
      "best loss: 904.59\t\tLearning rate: 5.456e-04, Batch size: 22, Momentum: 7.361e-01\n",
      "Initial loss:  1849747.0489714541\n",
      "best loss: 995060.74\t\tLearning rate: 2.976e-08, Batch size: 12, Momentum: 4.732e-01\n",
      "Initial loss:  2318846.6899609147\n",
      "best loss: 1354.01\t\tLearning rate: 1.000e+01, Batch size: 27, Momentum: 5.258e-02\n",
      "Initial loss:  3146976.791502763\n",
      "best loss: 1160.53\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 5.784e-01\n",
      "Initial loss:  4575164.8925439995\n",
      "best loss: 1009.90\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 4.732e-01\n",
      "Initial loss:  1844989.4143022485\n",
      "best loss: 993.13\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 3.155e-01\n",
      "Initial loss:  2863662.1906024916\n",
      "best loss: 849.37\t\tLearning rate: 1.833e-04, Batch size: 19, Momentum: 2.629e-01\n",
      "Initial loss:  1874950.1539171918\n",
      "best loss: 998.52\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 1.052e-01\n",
      "Initial loss:  7625154.273417108\n",
      "best loss: 897.03\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  2994887.0893797497\n",
      "best loss: 1269.61\t\tLearning rate: 3.793e-01, Batch size: 37, Momentum: 6.309e-01\n",
      "Initial loss:  2033262.7288595212\n",
      "best loss: 687369.59\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 2.629e-01\n",
      "Initial loss:  1641166.5283527442\n",
      "best loss: 604367.69\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 5.784e-01\n",
      "Initial loss:  1141355.138219496\n",
      "best loss: 498.52\t\tLearning rate: 2.336e-06, Batch size: 4, Momentum: 4.206e-01\n",
      "Initial loss:  385784.7131815423\n",
      "best loss: 1151.33\t\tLearning rate: 1.274e-01, Batch size: 19, Momentum: 0.000e+00\n",
      "Initial loss:  3328719.703156071\n",
      "best loss: 508.00\t\tLearning rate: 7.848e-07, Batch size: 4, Momentum: 3.681e-01\n",
      "Initial loss:  1396937.0982418773\n",
      "best loss: 1345.95\t\tLearning rate: 3.360e+00, Batch size: 37, Momentum: 6.835e-01\n",
      "Initial loss:  2317053.4694519816\n",
      "best loss: 722.62\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 6.309e-01\n",
      "Initial loss:  4031082.2715738183\n",
      "best loss: 2781934.51\t\tLearning rate: 2.976e-08, Batch size: 14, Momentum: 1.577e-01\n",
      "Initial loss:  4659731.492058985\n",
      "best loss: 593.51\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 1.577e-01\n",
      "Initial loss:  282543.41865422035\n",
      "best loss: 1143.42\t\tLearning rate: 4.281e-02, Batch size: 4, Momentum: 6.309e-01\n",
      "Initial loss:  572884.3905910365\n",
      "best loss: 797.43\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 0.000e+00\n",
      "Initial loss:  5211630.275692258\n",
      "best loss: 449358.39\t\tLearning rate: 7.848e-07, Batch size: 22, Momentum: 3.155e-01\n",
      "Initial loss:  1986835.0814689372\n",
      "best loss: 743.32\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 0.000e+00\n",
      "Initial loss:  584012.9177852727\n",
      "best loss: 1189.07\t\tLearning rate: 1.438e-02, Batch size: 4, Momentum: 8.413e-01\n",
      "Initial loss:  508.9451535149282\n",
      "best loss: 493.50\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  759845.1427721282\n",
      "best loss: 538.98\t\tLearning rate: 7.848e-07, Batch size: 4, Momentum: 5.258e-02\n",
      "Initial loss:  1195887.3517381898\n",
      "best loss: 1060.19\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 8.413e-01\n",
      "Initial loss:  3963628.720255227\n",
      "best loss: 244597.38\t\tLearning rate: 2.976e-08, Batch size: 34, Momentum: 6.835e-01\n",
      "Initial loss:  1988011.9928823174\n",
      "best loss: 629.48\t\tLearning rate: 2.069e-05, Batch size: 50, Momentum: 3.681e-01\n",
      "Initial loss:  3896159.2647331003\n",
      "best loss: 1260.39\t\tLearning rate: 1.129e+00, Batch size: 39, Momentum: 2.629e-01\n",
      "Initial loss:  462731.2968337764\n",
      "best loss: 1014.43\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 2.103e-01\n",
      "Initial loss:  1674233.5680984308\n",
      "best loss: 1383.60\t\tLearning rate: 1.129e+00, Batch size: 50, Momentum: 7.361e-01\n",
      "Initial loss:  1122719.1284894568\n",
      "best loss: 1305.58\t\tLearning rate: 3.360e+00, Batch size: 2, Momentum: 2.629e-01\n",
      "Initial loss:  3811133.906902671\n",
      "best loss: 1025.35\t\tLearning rate: 4.833e-03, Batch size: 42, Momentum: 9.464e-01\n",
      "Initial loss:  2458281.530461139\n",
      "best loss: 659.89\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 5.784e-01\n",
      "Initial loss:  1967031.8114487424\n",
      "best loss: 801.57\t\tLearning rate: 5.456e-04, Batch size: 14, Momentum: 5.258e-02\n",
      "Initial loss:  2256585.180272911\n",
      "best loss: 1436.99\t\tLearning rate: 3.360e+00, Batch size: 50, Momentum: 9.464e-01\n",
      "Initial loss:  1683306.6403850233\n",
      "best loss: 515.76\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 9.464e-01\n",
      "Initial loss:  497.10728011705964\n",
      "best loss: 497.02\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 7.361e-01\n",
      "Initial loss:  2043448.4795223721\n",
      "best loss: 571.79\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  1651678.0420524061\n",
      "best loss: 505.78\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 2.629e-01\n",
      "Initial loss:  5114436.0761778\n",
      "best loss: 1332.35\t\tLearning rate: 1.000e+01, Batch size: 47, Momentum: 5.258e-02\n",
      "Initial loss:  2777769.4743125625\n",
      "Iteration 916, validation loss: 506.81897864530943\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization hparam search (early_stopping)/optimizing_gaussian_fits DPC_Right.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits%20DPC_Right.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits%20DPC_Right.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m patches \u001b[39m=\u001b[39m extract_patches(images, patch_size, num_patches\u001b[39m=\u001b[39mnum_patches, seed\u001b[39m=\u001b[39mi)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits%20DPC_Right.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits%20DPC_Right.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits%20DPC_Right.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m best_loss\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:479\u001b[0m, in \u001b[0;36mrun_optimization\u001b[0;34m(data, momentum, learning_rate, batch_size, eigenvalue_floor, patience, validation_fraction, max_iters)\u001b[0m\n\u001b[1;32m    474\u001b[0m eigvals, eig_vecs, velocity, train_loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m    475\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor, patch_size)\n\u001b[1;32m    477\u001b[0m train_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m--> 479\u001b[0m validation_loss \u001b[39m=\u001b[39m loss_function(eigvals, eig_vecs, mean_vec, validation_data)   \n\u001b[1;32m    480\u001b[0m validation_loss_history\u001b[39m.\u001b[39mappend(validation_loss)\n\u001b[1;32m    482\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, validation loss: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:409\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(eigvals, eig_vecs, mean_vec, data)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_function\u001b[39m(eigvals, eig_vecs, mean_vec, data):\n\u001b[1;32m    408\u001b[0m     cov_mat \u001b[39m=\u001b[39m eig_vecs \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mdiag(eigvals) \u001b[39m@\u001b[39m eig_vecs\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 409\u001b[0m     ll \u001b[39m=\u001b[39m gaussian_likelihood(cov_mat, mean_vec, data)\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_nll(ll)\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:400\u001b[0m, in \u001b[0;36mgaussian_likelihood\u001b[0;34m(cov_mat, mean_vec, batch)\u001b[0m\n\u001b[1;32m    398\u001b[0m log_likelihoods \u001b[39m=\u001b[39m []\n\u001b[1;32m    399\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m--> 400\u001b[0m     ll \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mscipy\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mmultivariate_normal\u001b[39m.\u001b[39;49mlogpdf(sample\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), mean\u001b[39m=\u001b[39;49mmean_vec, cov\u001b[39m=\u001b[39;49mcov_mat)\n\u001b[1;32m    401\u001b[0m     log_likelihoods\u001b[39m.\u001b[39mappend(ll)\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(log_likelihoods)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/scipy/stats/multivariate_normal.py:46\u001b[0m, in \u001b[0;36mlogpdf\u001b[0;34m(x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmultivariate_normal.logpdf got incompatible shapes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m L \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcholesky(cov)\n\u001b[0;32m---> 46\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mvectorize(\n\u001b[1;32m     47\u001b[0m   partial(lax\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mtriangular_solve, lower\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     48\u001b[0m   signature\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(n,n),(n)->(n)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     49\u001b[0m )(L, x \u001b[39m-\u001b[39;49m mean)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...i,...i->...\u001b[39m\u001b[39m'\u001b[39m, y, y) \u001b[39m-\u001b[39m n\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mpi)\n\u001b[1;32m     51\u001b[0m         \u001b[39m-\u001b[39m jnp\u001b[39m.\u001b[39mlog(L\u001b[39m.\u001b[39mdiagonal(axis1\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis2\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/vectorize.py:298\u001b[0m, in \u001b[0;36mvectorize.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    295\u001b[0m   rev_filled_shapes\u001b[39m.\u001b[39mappend(filled_shape[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    297\u001b[0m   squeeze_indices \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(i \u001b[39mfor\u001b[39;00m i, size \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(noncore_shape) \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 298\u001b[0m   squeezed_arg \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49msqueeze(arg, axis\u001b[39m=\u001b[39;49msqueeze_indices)\n\u001b[1;32m    299\u001b[0m   squeezed_args\u001b[39m.\u001b[39mappend(squeezed_arg)\n\u001b[1;32m    301\u001b[0m vectorized_func \u001b[39m=\u001b[39m checked_func\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:853\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39m@util\u001b[39m\u001b[39m.\u001b[39m_wraps(np\u001b[39m.\u001b[39msqueeze, lax_description\u001b[39m=\u001b[39m_ARRAY_VIEW_DOC)\n\u001b[1;32m    851\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msqueeze\u001b[39m(a: ArrayLike, axis: Optional[Union[\u001b[39mint\u001b[39m, Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    852\u001b[0m   util\u001b[39m.\u001b[39mcheck_arraylike(\u001b[39m\"\u001b[39m\u001b[39msqueeze\u001b[39m\u001b[39m\"\u001b[39m, a)\n\u001b[0;32m--> 853\u001b[0m   \u001b[39mreturn\u001b[39;00m _squeeze(asarray(a), _ensure_index_tuple(axis) \u001b[39mif\u001b[39;49;00m axis \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/pjit.py:248\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39m@api_boundary\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcache_miss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 248\u001b[0m   outs, out_flat, out_tree, args_flat \u001b[39m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    249\u001b[0m       fun, infer_params_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m   executable \u001b[39m=\u001b[39m _read_most_recent_pjit_call_executable()\n\u001b[1;32m    253\u001b[0m   use_fastpath \u001b[39m=\u001b[39m (\n\u001b[1;32m    254\u001b[0m       executable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    255\u001b[0m       \u001b[39misinstance\u001b[39m(executable, pxla\u001b[39m.\u001b[39mMeshExecutable) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, xc\u001b[39m.\u001b[39mArrayImpl) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out_flat)\n\u001b[1;32m    262\u001b[0m   )\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/pjit.py:195\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m   dispatch\u001b[39m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m   out_flat \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs_flat, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    196\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    197\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:2586\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2585\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m-> 2586\u001b[0m   top_trace \u001b[39m=\u001b[39m find_top_trace(args)\n\u001b[1;32m   2587\u001b[0m   axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[1;32m   2588\u001b[0m                   default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   2589\u001b[0m   top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   2590\u001b[0m                \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:1230\u001b[0m, in \u001b[0;36mfind_top_trace\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m dynamic \u001b[39m=\u001b[39m thread_local_state\u001b[39m.\u001b[39mtrace_state\u001b[39m.\u001b[39mtrace_stack\u001b[39m.\u001b[39mdynamic\n\u001b[1;32m   1228\u001b[0m top_main \u001b[39m=\u001b[39m (dynamic \u001b[39mif\u001b[39;00m top_main \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m dynamic\u001b[39m.\u001b[39mlevel \u001b[39m>\u001b[39m top_main\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   1229\u001b[0m             \u001b[39melse\u001b[39;00m top_main)\n\u001b[0;32m-> 1230\u001b[0m \u001b[39mreturn\u001b[39;00m top_main\u001b[39m.\u001b[39;49mwith_cur_sublevel()\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:861\u001b[0m, in \u001b[0;36mMainTrace.with_cur_sublevel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwith_cur_sublevel\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 861\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrace_type(\u001b[39mself\u001b[39;49m, cur_sublevel(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpayload)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:444\u001b[0m, in \u001b[0;36mTrace.__init__\u001b[0;34m(self, main, sublevel)\u001b[0m\n\u001b[1;32m    441\u001b[0m level: \u001b[39mint\u001b[39m\n\u001b[1;32m    442\u001b[0m sublevel: Sublevel\n\u001b[0;32m--> 444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, main: MainTrace, sublevel: Sublevel) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain \u001b[39m=\u001b[39m main\n\u001b[1;32m    446\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39mlevel\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 493.50\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 495.63\t\tLearning rate: 6.952e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 497.02\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 7.361e-01\n",
      "best loss: 497.31\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 2.103e-01\n",
      "best loss: 497.69\t\tLearning rate: 2.637e-07, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 498.22\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 7.887e-01\n",
      "best loss: 498.29\t\tLearning rate: 2.976e-08, Batch size: 17, Momentum: 3.155e-01\n",
      "best loss: 498.52\t\tLearning rate: 2.336e-06, Batch size: 4, Momentum: 4.206e-01\n",
      "best loss: 498.53\t\tLearning rate: 1.000e+01, Batch size: 4, Momentum: 2.629e-01\n",
      "best loss: 498.60\t\tLearning rate: 7.848e-07, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 498.82\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 499.23\t\tLearning rate: 6.952e-06, Batch size: 44, Momentum: 8.938e-01\n",
      "best loss: 499.91\t\tLearning rate: 7.848e-07, Batch size: 12, Momentum: 5.258e-01\n",
      "best loss: 499.99\t\tLearning rate: 4.281e-02, Batch size: 34, Momentum: 8.938e-01\n",
      "best loss: 500.34\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 3.155e-01\n",
      "best loss: 500.48\t\tLearning rate: 8.859e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "best loss: 500.78\t\tLearning rate: 6.952e-06, Batch size: 4, Momentum: 2.103e-01\n",
      "best loss: 501.46\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 2.629e-01\n",
      "best loss: 501.57\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 501.72\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 8.413e-01\n",
      "best loss: 502.00\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 7.361e-01\n",
      "best loss: 502.64\t\tLearning rate: 6.952e-06, Batch size: 47, Momentum: 7.887e-01\n",
      "best loss: 502.85\t\tLearning rate: 2.336e-06, Batch size: 4, Momentum: 9.990e-01\n",
      "best loss: 503.08\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 3.155e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.732e-01\n",
      "best loss: 503.31\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 5.258e-02\n",
      "best loss: 504.06\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 3.681e-01\n",
      "best loss: 504.32\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 504.79\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 6.835e-01\n",
      "best loss: 505.67\t\tLearning rate: 8.859e-08, Batch size: 37, Momentum: 9.990e-01\n",
      "best loss: 505.78\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 2.629e-01\n",
      "best loss: 506.45\t\tLearning rate: 7.848e-07, Batch size: 50, Momentum: 1.052e-01\n",
      "best loss: 507.19\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 5.258e-01\n",
      "best loss: 507.60\t\tLearning rate: 2.336e-06, Batch size: 50, Momentum: 1.052e-01\n",
      "best loss: 507.76\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 9.990e-01\n",
      "best loss: 508.00\t\tLearning rate: 7.848e-07, Batch size: 4, Momentum: 3.681e-01\n",
      "best loss: 508.05\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 4.206e-01\n",
      "best loss: 508.93\t\tLearning rate: 6.952e-06, Batch size: 19, Momentum: 9.464e-01\n",
      "best loss: 509.42\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 511.82\t\tLearning rate: 2.336e-06, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 512.98\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 515.64\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 3.681e-01\n",
      "best loss: 515.65\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 5.258e-01\n",
      "best loss: 515.76\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 9.464e-01\n",
      "best loss: 524.62\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 525.06\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 527.97\t\tLearning rate: 2.637e-07, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 533.25\t\tLearning rate: 2.336e-06, Batch size: 34, Momentum: 8.413e-01\n",
      "best loss: 537.29\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "best loss: 538.14\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 6.309e-01\n",
      "best loss: 538.98\t\tLearning rate: 7.848e-07, Batch size: 4, Momentum: 5.258e-02\n",
      "best loss: 552.68\t\tLearning rate: 6.952e-06, Batch size: 27, Momentum: 8.938e-01\n",
      "best loss: 553.49\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 6.835e-01\n",
      "best loss: 560.06\t\tLearning rate: 2.976e-08, Batch size: 19, Momentum: 7.361e-01\n",
      "best loss: 562.85\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 5.784e-01\n",
      "best loss: 563.45\t\tLearning rate: 6.952e-06, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 568.93\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 8.413e-01\n",
      "best loss: 571.79\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 579.57\t\tLearning rate: 2.637e-07, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 580.27\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 0.000e+00\n",
      "best loss: 585.93\t\tLearning rate: 6.952e-06, Batch size: 37, Momentum: 8.413e-01\n",
      "best loss: 587.89\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 593.51\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 1.577e-01\n",
      "best loss: 595.30\t\tLearning rate: 2.069e-05, Batch size: 29, Momentum: 1.577e-01\n",
      "best loss: 595.90\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "best loss: 616.77\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 7.887e-01\n",
      "best loss: 628.24\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 6.309e-01\n",
      "best loss: 629.48\t\tLearning rate: 2.069e-05, Batch size: 50, Momentum: 3.681e-01\n",
      "best loss: 636.72\t\tLearning rate: 8.859e-08, Batch size: 29, Momentum: 3.155e-01\n",
      "best loss: 649.98\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 0.000e+00\n",
      "best loss: 653.89\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 7.361e-01\n",
      "best loss: 659.89\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 5.784e-01\n",
      "best loss: 660.80\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 5.258e-02\n",
      "best loss: 689.69\t\tLearning rate: 6.952e-06, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 707.34\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 1.577e-01\n",
      "best loss: 719.13\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 6.835e-01\n",
      "best loss: 722.62\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 6.309e-01\n",
      "best loss: 730.28\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 4.732e-01\n",
      "best loss: 736.16\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 9.990e-01\n",
      "best loss: 743.32\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 0.000e+00\n",
      "best loss: 755.05\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 758.56\t\tLearning rate: 6.158e-05, Batch size: 19, Momentum: 6.835e-01\n",
      "best loss: 762.84\t\tLearning rate: 6.158e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 775.47\t\tLearning rate: 6.158e-05, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 780.82\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 5.258e-01\n",
      "best loss: 782.18\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.681e-01\n",
      "best loss: 783.04\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 790.23\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 3.681e-01\n",
      "best loss: 797.43\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 0.000e+00\n",
      "best loss: 801.57\t\tLearning rate: 5.456e-04, Batch size: 14, Momentum: 5.258e-02\n",
      "best loss: 807.95\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "best loss: 810.08\t\tLearning rate: 5.456e-04, Batch size: 37, Momentum: 1.577e-01\n",
      "best loss: 811.45\t\tLearning rate: 1.833e-04, Batch size: 39, Momentum: 7.887e-01\n",
      "best loss: 819.73\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 4.732e-01\n",
      "best loss: 831.46\t\tLearning rate: 5.456e-04, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 833.68\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "best loss: 833.84\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 8.413e-01\n",
      "best loss: 843.22\t\tLearning rate: 1.833e-04, Batch size: 7, Momentum: 1.052e-01\n",
      "best loss: 849.17\t\tLearning rate: 1.833e-04, Batch size: 37, Momentum: 9.464e-01\n",
      "best loss: 849.24\t\tLearning rate: 5.456e-04, Batch size: 9, Momentum: 1.577e-01\n",
      "best loss: 849.37\t\tLearning rate: 1.833e-04, Batch size: 19, Momentum: 2.629e-01\n",
      "best loss: 872.51\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 1.052e-01\n",
      "best loss: 883.62\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 8.938e-01\n",
      "best loss: 897.03\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 902.22\t\tLearning rate: 5.456e-04, Batch size: 27, Momentum: 6.835e-01\n",
      "best loss: 903.00\t\tLearning rate: 5.456e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 903.89\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 0.000e+00\n",
      "best loss: 904.59\t\tLearning rate: 5.456e-04, Batch size: 22, Momentum: 7.361e-01\n",
      "best loss: 907.97\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 6.835e-01\n",
      "best loss: 922.70\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 924.30\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "best loss: 930.44\t\tLearning rate: 4.833e-03, Batch size: 27, Momentum: 0.000e+00\n",
      "best loss: 943.94\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 1.052e-01\n",
      "best loss: 960.82\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 1.577e-01\n",
      "best loss: 965.93\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 2.103e-01\n",
      "best loss: 976.40\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 3.681e-01\n",
      "best loss: 980.37\t\tLearning rate: 4.833e-03, Batch size: 17, Momentum: 5.258e-01\n",
      "best loss: 985.83\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 988.39\t\tLearning rate: 1.624e-03, Batch size: 27, Momentum: 5.784e-01\n",
      "best loss: 991.57\t\tLearning rate: 1.624e-03, Batch size: 29, Momentum: 6.309e-01\n",
      "best loss: 993.13\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 3.155e-01\n",
      "best loss: 994.93\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 5.258e-02\n",
      "best loss: 996.16\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 0.000e+00\n",
      "best loss: 997.54\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 998.52\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 1.052e-01\n",
      "best loss: 1009.90\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 4.732e-01\n",
      "best loss: 1011.62\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.732e-01\n",
      "best loss: 1014.43\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 2.103e-01\n",
      "best loss: 1017.71\t\tLearning rate: 4.833e-03, Batch size: 50, Momentum: 5.258e-01\n",
      "best loss: 1019.59\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 5.258e-02\n",
      "best loss: 1025.35\t\tLearning rate: 4.833e-03, Batch size: 42, Momentum: 9.464e-01\n",
      "best loss: 1028.34\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 1031.06\t\tLearning rate: 1.438e-02, Batch size: 7, Momentum: 6.309e-01\n",
      "best loss: 1036.05\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 1036.50\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 7.887e-01\n",
      "best loss: 1045.42\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 6.309e-01\n",
      "best loss: 1056.52\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 1056.77\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 1060.19\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 1066.65\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 7.887e-01\n",
      "best loss: 1076.45\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 5.258e-01\n",
      "best loss: 1081.17\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 5.258e-02\n",
      "best loss: 1082.89\t\tLearning rate: 4.281e-02, Batch size: 50, Momentum: 3.155e-01\n",
      "best loss: 1083.47\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.052e-01\n",
      "best loss: 1084.84\t\tLearning rate: 1.438e-02, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 1088.26\t\tLearning rate: 1.274e-01, Batch size: 29, Momentum: 1.577e-01\n",
      "best loss: 1092.53\t\tLearning rate: 4.281e-02, Batch size: 14, Momentum: 3.681e-01\n",
      "best loss: 1104.32\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 9.990e-01\n",
      "best loss: 1107.82\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.835e-01\n",
      "best loss: 1110.92\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 5.784e-01\n",
      "best loss: 1119.57\t\tLearning rate: 1.274e-01, Batch size: 47, Momentum: 2.103e-01\n",
      "best loss: 1126.31\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 9.464e-01\n",
      "best loss: 1126.51\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 8.413e-01\n",
      "best loss: 1127.20\t\tLearning rate: 1.274e-01, Batch size: 12, Momentum: 0.000e+00\n",
      "best loss: 1128.00\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.887e-01\n",
      "best loss: 1132.64\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 5.784e-01\n",
      "best loss: 1137.10\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 7.361e-01\n",
      "best loss: 1139.42\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 7.361e-01\n",
      "best loss: 1143.42\t\tLearning rate: 4.281e-02, Batch size: 4, Momentum: 6.309e-01\n",
      "best loss: 1148.95\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "best loss: 1151.33\t\tLearning rate: 1.274e-01, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 1157.62\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 1.052e-01\n",
      "best loss: 1157.65\t\tLearning rate: 1.438e-02, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 1160.53\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 5.784e-01\n",
      "best loss: 1161.13\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.309e-01\n",
      "best loss: 1163.22\t\tLearning rate: 1.274e-01, Batch size: 19, Momentum: 3.681e-01\n",
      "best loss: 1170.60\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 1.052e-01\n",
      "best loss: 1173.40\t\tLearning rate: 1.438e-02, Batch size: 4, Momentum: 9.990e-01\n",
      "best loss: 1176.02\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "best loss: 1189.07\t\tLearning rate: 1.438e-02, Batch size: 4, Momentum: 8.413e-01\n",
      "best loss: 1189.33\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 1190.85\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 4.206e-01\n",
      "best loss: 1200.05\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 6.835e-01\n",
      "best loss: 1205.63\t\tLearning rate: 1.274e-01, Batch size: 14, Momentum: 4.206e-01\n",
      "best loss: 1207.05\t\tLearning rate: 1.274e-01, Batch size: 22, Momentum: 9.464e-01\n",
      "best loss: 1212.88\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 0.000e+00\n",
      "best loss: 1214.74\t\tLearning rate: 3.793e-01, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 1229.99\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 8.938e-01\n",
      "best loss: 1243.97\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "best loss: 1247.12\t\tLearning rate: 1.274e-01, Batch size: 37, Momentum: 7.361e-01\n",
      "best loss: 1254.40\t\tLearning rate: 1.129e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 1254.57\t\tLearning rate: 1.129e+00, Batch size: 12, Momentum: 2.629e-01\n",
      "best loss: 1257.72\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 2.629e-01\n",
      "best loss: 1260.39\t\tLearning rate: 1.129e+00, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 1264.99\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 3.681e-01\n",
      "best loss: 1266.37\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 1269.61\t\tLearning rate: 3.793e-01, Batch size: 37, Momentum: 6.309e-01\n",
      "best loss: 1305.58\t\tLearning rate: 3.360e+00, Batch size: 2, Momentum: 2.629e-01\n",
      "best loss: 1318.22\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 2.629e-01\n",
      "best loss: 1318.66\t\tLearning rate: 1.000e+01, Batch size: 42, Momentum: 5.258e-02\n",
      "best loss: 1321.97\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 5.258e-02\n",
      "best loss: 1322.01\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 7.361e-01\n",
      "best loss: 1324.44\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 1324.61\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 9.464e-01\n",
      "best loss: 1327.73\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 5.784e-01\n",
      "best loss: 1329.82\t\tLearning rate: 3.793e-01, Batch size: 17, Momentum: 8.938e-01\n",
      "best loss: 1330.49\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 5.258e-01\n",
      "best loss: 1332.14\t\tLearning rate: 3.793e-01, Batch size: 12, Momentum: 8.938e-01\n",
      "best loss: 1332.35\t\tLearning rate: 1.000e+01, Batch size: 47, Momentum: 5.258e-02\n",
      "best loss: 1332.35\t\tLearning rate: 1.000e+01, Batch size: 7, Momentum: 1.577e-01\n",
      "best loss: 1333.33\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 7.361e-01\n",
      "best loss: 1336.25\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 1345.95\t\tLearning rate: 3.360e+00, Batch size: 37, Momentum: 6.835e-01\n",
      "best loss: 1351.66\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 5.784e-01\n",
      "best loss: 1354.01\t\tLearning rate: 1.000e+01, Batch size: 27, Momentum: 5.258e-02\n",
      "best loss: 1357.85\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.577e-01\n",
      "best loss: 1371.53\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 1377.60\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 7.361e-01\n",
      "best loss: 1383.60\t\tLearning rate: 1.129e+00, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 1393.20\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 8.938e-01\n",
      "best loss: 1397.65\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 3.681e-01\n",
      "best loss: 1401.16\t\tLearning rate: 3.360e+00, Batch size: 7, Momentum: 9.990e-01\n",
      "best loss: 1405.23\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 9.990e-01\n",
      "best loss: 1424.59\t\tLearning rate: 1.000e+01, Batch size: 14, Momentum: 7.361e-01\n",
      "best loss: 1436.99\t\tLearning rate: 3.360e+00, Batch size: 50, Momentum: 9.464e-01\n",
      "best loss: 1438.53\t\tLearning rate: 3.360e+00, Batch size: 4, Momentum: 6.835e-01\n",
      "best loss: 1457.13\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 1463.78\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 6.309e-01\n",
      "best loss: 1483.58\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 9.464e-01\n",
      "best loss: 1495.67\t\tLearning rate: 3.360e+00, Batch size: 44, Momentum: 9.464e-01\n",
      "best loss: 1697.60\t\tLearning rate: 8.859e-08, Batch size: 12, Momentum: 0.000e+00\n",
      "best loss: 230503.86\t\tLearning rate: 2.976e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 237492.91\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 5.784e-01\n",
      "best loss: 243829.29\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 3.155e-01\n",
      "best loss: 244597.38\t\tLearning rate: 2.976e-08, Batch size: 34, Momentum: 6.835e-01\n",
      "best loss: 251148.48\t\tLearning rate: 2.336e-06, Batch size: 32, Momentum: 2.629e-01\n",
      "best loss: 449358.39\t\tLearning rate: 7.848e-07, Batch size: 22, Momentum: 3.155e-01\n",
      "best loss: 468479.64\t\tLearning rate: 8.859e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 477736.71\t\tLearning rate: 2.976e-08, Batch size: 50, Momentum: 5.258e-02\n",
      "best loss: 568912.67\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 6.309e-01\n",
      "best loss: 604367.69\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 5.784e-01\n",
      "best loss: 652973.93\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 5.258e-01\n",
      "best loss: 687369.59\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 2.629e-01\n",
      "best loss: 737860.59\t\tLearning rate: 2.637e-07, Batch size: 7, Momentum: 2.103e-01\n",
      "best loss: 818129.48\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 3.681e-01\n",
      "best loss: 923318.83\t\tLearning rate: 1.000e-08, Batch size: 34, Momentum: 5.784e-01\n",
      "best loss: 951490.44\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 0.000e+00\n",
      "best loss: 995060.74\t\tLearning rate: 2.976e-08, Batch size: 12, Momentum: 4.732e-01\n",
      "best loss: 1063107.17\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 3.681e-01\n",
      "best loss: 1147147.14\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 1315573.56\t\tLearning rate: 2.976e-08, Batch size: 44, Momentum: 1.052e-01\n",
      "best loss: 1429211.91\t\tLearning rate: 8.859e-08, Batch size: 34, Momentum: 1.577e-01\n",
      "best loss: 1451073.65\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 7.361e-01\n",
      "best loss: 1451923.50\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 8.413e-01\n",
      "best loss: 1961788.59\t\tLearning rate: 1.624e-03, Batch size: 2, Momentum: 1.052e-01\n",
      "best loss: 2781934.51\t\tLearning rate: 2.976e-08, Batch size: 14, Momentum: 1.577e-01\n",
      "best loss: 18101921.87\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 1.577e-01\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
