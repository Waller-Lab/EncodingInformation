{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Opening BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 1000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.linalg import toeplitz\n",
    "from jax import grad, jit, value_and_grad\n",
    "\n",
    "def make_doubly_toeplitz(top_row, patch_size):\n",
    "    \"\"\"\n",
    "    Make a doubly toeplitz matrix from its top row, which is the\n",
    "    minimum number of parameters needed to specify the matrix.\n",
    "    \"\"\"\n",
    "    # split into rows\n",
    "    top_rows = np.split(top_row, patch_size)\n",
    "    # make into toeplitz blocks\n",
    "    blocks = []\n",
    "    for tr in top_rows:\n",
    "        blocks.append(toeplitz(tr))\n",
    "    # use blocks to construct doubl\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(blocks)):\n",
    "        row_blocks = [blocks[abs(i - j)] for j in range(len(blocks))]\n",
    "        row = np.hstack(row_blocks)\n",
    "        rows.append(row)\n",
    "    doubly_toeplitz_mat = np.vstack(rows)\n",
    "    return doubly_toeplitz_mat\n",
    "\n",
    "\n",
    "def gaussian_likelihood(cov_mat, mean_vec, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the log likelihood of a multivariate gaussian\n",
    "    for a batch of NxWXH samples.\n",
    "    \"\"\"\n",
    "    log_likelihoods = []\n",
    "    for sample in batch:\n",
    "        ll = jax.scipy.stats.multivariate_normal.logpdf(sample.reshape(-1), mean=mean_vec, cov=cov_mat)\n",
    "        log_likelihoods.append(ll)\n",
    "    return np.array(log_likelihoods)\n",
    "\n",
    "def batch_nll(log_likelihoods):\n",
    "    return -np.mean(log_likelihoods)\n",
    "\n",
    "def loss_function(eigvals, eig_vecs, mean_vec, data):\n",
    "    cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    # cov_mat = make_doubly_toeplitz(cov_mat_row, patch_size)\n",
    "    # cov_mat = make_positive_definite(cov_mat, eigenvalue_floor)\n",
    "    ll = gaussian_likelihood(cov_mat, mean_vec, data)\n",
    "    return batch_nll(ll)\n",
    "\n",
    "def make_valid_stationary(eigvals, eig_vecs, eigenvalue_floor):\n",
    "    eigvals = np.where(eigvals < eigenvalue_floor, eigenvalue_floor, eigvals)\n",
    "    cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    dt_cov_mat = make_doubly_toeplitz(cov_mat[0], patch_size)\n",
    "    eigvals, eig_vecs = np.linalg.eigh(dt_cov_mat)\n",
    "    eigvals = np.where(eigvals < eigenvalue_floor, eigenvalue_floor, eigvals)\n",
    "    return eigvals, eig_vecs\n",
    "\n",
    "@jit\n",
    "def optmization_step(eigvals, eig_vecs, velocity, data, mean_vec, momentum, learning_rate, eigenvalue_floor):\n",
    "    grad_fn = grad(loss_function, argnums=0)\n",
    "    eigenvalues_grad = grad_fn(eigvals, eig_vecs, mean_vec, data)\n",
    "    new_velocity = momentum * velocity - learning_rate * eigenvalues_grad\n",
    "    eigvals = eigvals + new_velocity\n",
    "    # prox operator: make sure make sure positive definite, make sure doubly toeplitz\n",
    "    eigvals, eig_vecs = make_valid_stationary(eigvals, eig_vecs, eigenvalue_floor)\n",
    "    loss = loss_function(eigvals, eig_vecs, mean_vec, data)\n",
    "    return eigvals, eig_vecs, new_velocity, loss\n",
    "\n",
    "\n",
    "def run_optimization(data, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3):\n",
    "    patch_size = int(np.sqrt(np.prod(np.array(data.shape)[1:])))\n",
    "    # Initialize parameters, hyperparameters\n",
    "    mean_vec = np.ones(patch_size**2) * np.mean(data)\n",
    "\n",
    "    # initialize covariance matrix so likelihood is not nan\n",
    "    cov_mat_initial = make_positive_definite(compute_stationary_cov_mat(data), eigenvalue_floor=eigenvalue_floor)\n",
    "\n",
    "    initial_evs, initial_eig_vecs = make_valid_stationary(*np.linalg.eigh(cov_mat_initial), eigenvalue_floor)\n",
    "    print('Initial loss: ', loss_function(initial_evs, initial_eig_vecs, mean_vec, data[:batch_size]))\n",
    "\n",
    "    cov_mat_initial = initial_eig_vecs @ np.diag(initial_evs) @ initial_eig_vecs.T\n",
    "\n",
    "    if np.isnan(jax.scipy.stats.multivariate_normal.logpdf(patches[0].flatten(), mean=mean_vec, cov=cov_mat_initial)):\n",
    "        raise ValueError(\"Initial likelihood is nan\")\n",
    "    \n",
    "    # Training loop\n",
    "    eigvals = initial_evs\n",
    "    eig_vecs = initial_eig_vecs\n",
    "    velocity = np.zeros_like(eigvals)\n",
    "    best_loss = np.inf\n",
    "    key = jax.random.PRNGKey(onp.random.randint(0, 100000))\n",
    "    for i in range(1000):\n",
    "        # select a random batch\n",
    "        batch_indices = jax.random.randint(key, shape=(batch_size,), minval=0, maxval=len(data))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        batch = data[batch_indices]\n",
    "        \n",
    "        eigvals, eig_vecs, velocity, loss = optmization_step(eigvals, eig_vecs, velocity, \n",
    "                                                             batch, mean_vec, momentum, learning_rate, eigenvalue_floor)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_eigvals = eigvals\n",
    "            best_eig_vecs = eig_vecs\n",
    "        print(f\"Iteration {i+1}, Loss: {loss}\", end='\\r')\n",
    "    eigvals, eig_vecs = make_valid_stationary(best_eigvals, best_eig_vecs, eigenvalue_floor)\n",
    "    best_cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    return best_cov_mat, cov_mat_initial, mean_vec, best_loss\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  637002.4989774039\n",
      "best loss: 1038.01\t\tLearning rate: 4.281e-02, Batch size: 12, Momentum: 2.629e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m patches \u001b[39m=\u001b[39m extract_patches(images, patch_size, num_patches\u001b[39m=\u001b[39mnum_patches, seed\u001b[39m=\u001b[39mi)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m best_cov_mat, cov_mat_initial, mean_vec, best_loss \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m best_loss\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m cov_mat_initial \u001b[39m=\u001b[39m make_positive_definite(compute_stationary_cov_mat(data), eigenvalue_floor\u001b[39m=\u001b[39meigenvalue_floor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m initial_evs, initial_eig_vecs \u001b[39m=\u001b[39m make_valid_stationary(\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(cov_mat_initial), eigenvalue_floor)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInitial loss: \u001b[39m\u001b[39m'\u001b[39m, loss_function(initial_evs, initial_eig_vecs, mean_vec, data[:batch_size]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m cov_mat_initial \u001b[39m=\u001b[39m initial_eig_vecs \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mdiag(initial_evs) \u001b[39m@\u001b[39m initial_eig_vecs\u001b[39m.\u001b[39mT\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misnan(jax\u001b[39m.\u001b[39mscipy\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mmultivariate_normal\u001b[39m.\u001b[39mlogpdf(patches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mflatten(), mean\u001b[39m=\u001b[39mmean_vec, cov\u001b[39m=\u001b[39mcov_mat_initial)):\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m cov_mat \u001b[39m=\u001b[39m eig_vecs \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mdiag(eigvals) \u001b[39m@\u001b[39m eig_vecs\u001b[39m.\u001b[39mT\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# cov_mat = make_doubly_toeplitz(cov_mat_row, patch_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# cov_mat = make_positive_definite(cov_mat, eigenvalue_floor)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m ll \u001b[39m=\u001b[39m gaussian_likelihood(cov_mat, mean_vec, data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch_nll(ll)\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m log_likelihoods \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     ll \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mscipy\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mmultivariate_normal\u001b[39m.\u001b[39;49mlogpdf(sample\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), mean\u001b[39m=\u001b[39;49mmean_vec, cov\u001b[39m=\u001b[39;49mcov_mat)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     log_likelihoods\u001b[39m.\u001b[39mappend(ll)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits_20_pixel.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(log_likelihoods)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/scipy/stats/multivariate_normal.py:46\u001b[0m, in \u001b[0;36mlogpdf\u001b[0;34m(x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmultivariate_normal.logpdf got incompatible shapes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m L \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcholesky(cov)\n\u001b[0;32m---> 46\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mvectorize(\n\u001b[1;32m     47\u001b[0m   partial(lax\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mtriangular_solve, lower\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     48\u001b[0m   signature\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(n,n),(n)->(n)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     49\u001b[0m )(L, x \u001b[39m-\u001b[39;49m mean)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...i,...i->...\u001b[39m\u001b[39m'\u001b[39m, y, y) \u001b[39m-\u001b[39m n\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mpi)\n\u001b[1;32m     51\u001b[0m         \u001b[39m-\u001b[39m jnp\u001b[39m.\u001b[39mlog(L\u001b[39m.\u001b[39mdiagonal(axis1\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis2\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/vectorize.py:309\u001b[0m, in \u001b[0;36mvectorize.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     vectorized_func \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mvmap(vectorized_func, in_axes)\n\u001b[0;32m--> 309\u001b[0m result \u001b[39m=\u001b[39m vectorized_func(\u001b[39m*\u001b[39;49msqueezed_args)\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dims_to_expand:\n\u001b[1;32m    312\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/vectorize.py:136\u001b[0m, in \u001b[0;36m_check_output_dims.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 136\u001b[0m   out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    137\u001b[0m   out_shapes \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(jnp\u001b[39m.\u001b[39mshape, out \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m [out])\n\u001b[1;32m    139\u001b[0m   \u001b[39mif\u001b[39;00m expected_output_core_dims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/lax/linalg.py:103\u001b[0m, in \u001b[0;36m_warn_on_positional_kwargs.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m   warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is now a keyword-only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39margument. Support for passing it positionally will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mremoved in an upcoming JAX release.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m                 \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m   kwargs[name] \u001b[39m=\u001b[39m value\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mpos_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/lax/linalg.py:335\u001b[0m, in \u001b[0;36mtriangular_solve\u001b[0;34m(a, b, left_side, lower, transpose_a, conjugate_a, unit_diagonal)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m singleton:\n\u001b[1;32m    334\u001b[0m   b \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mexpand_dims(b, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m left_side \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 335\u001b[0m out \u001b[39m=\u001b[39m triangular_solve_p\u001b[39m.\u001b[39;49mbind(\n\u001b[1;32m    336\u001b[0m     a, b, left_side\u001b[39m=\u001b[39;49mleft_side, lower\u001b[39m=\u001b[39;49mlower, transpose_a\u001b[39m=\u001b[39;49mtranspose_a,\n\u001b[1;32m    337\u001b[0m     conjugate_a\u001b[39m=\u001b[39;49mconjugate_a, unit_diagonal\u001b[39m=\u001b[39;49munit_diagonal)\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m singleton:\n\u001b[1;32m    339\u001b[0m   out \u001b[39m=\u001b[39m out[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m left_side \u001b[39melse\u001b[39;00m out[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:359\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    357\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    358\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 359\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:1221\u001b[0m, in \u001b[0;36mfind_top_trace\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_top_trace\u001b[39m(xs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Trace:\n\u001b[1;32m   1220\u001b[0m   top_tracer \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m xs \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Tracer)),\n\u001b[0;32m-> 1221\u001b[0m                     default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39mattrgetter(\u001b[39m'\u001b[39;49m\u001b[39m_trace.level\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1222\u001b[0m   \u001b[39mif\u001b[39;00m top_tracer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     top_tracer\u001b[39m.\u001b[39m_assert_live()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 438.59\t\tLearning rate: 6.952e-05, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 438.59\t\tLearning rate: 1.833e+00, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 438.59\t\tLearning rate: 6.158e-02, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 6.158e-02, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 7.361e-01\n",
      "best loss: 443.39\t\tLearning rate: 5.456e-06, Batch size: 4, Momentum: 5.784e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 2.103e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.833e+00, Batch size: 32, Momentum: 2.629e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.274e-05, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 450.95\t\tLearning rate: 1.438e-01, Batch size: 47, Momentum: 7.887e-01\n",
      "best loss: 450.95\t\tLearning rate: 3.793e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 451.99\t\tLearning rate: 7.848e-01, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.624e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.336e-06, Batch size: 29, Momentum: 2.103e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.438e-01, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 5.456e-06, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 6.835e-01\n",
      "best loss: 451.99\t\tLearning rate: 6.158e-02, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 4.732e-01\n",
      "best loss: 452.84\t\tLearning rate: 1.129e-02, Batch size: 7, Momentum: 4.206e-01\n",
      "best loss: 459.16\t\tLearning rate: 4.833e-03, Batch size: 12, Momentum: 7.887e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 7.361e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 1.577e-01\n",
      "best loss: 460.83\t\tLearning rate: 1.129e-02, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 460.83\t\tLearning rate: 3.360e-01, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 3.155e-01\n",
      "best loss: 467.16\t\tLearning rate: 2.336e-06, Batch size: 27, Momentum: 5.258e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 7.848e-01, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.274e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 6.952e-05, Batch size: 27, Momentum: 9.990e-01\n",
      "best loss: 469.16\t\tLearning rate: 2.069e-03, Batch size: 9, Momentum: 0.000e+00\n",
      "best loss: 469.16\t\tLearning rate: 1.000e+01, Batch size: 9, Momentum: 4.206e-01\n",
      "best loss: 469.16\t\tLearning rate: 6.952e-05, Batch size: 9, Momentum: 8.938e-01\n",
      "best loss: 469.16\t\tLearning rate: 7.848e-01, Batch size: 9, Momentum: 8.413e-01\n",
      "best loss: 476.95\t\tLearning rate: 1.129e-02, Batch size: 37, Momentum: 0.000e+00\n",
      "best loss: 476.95\t\tLearning rate: 6.158e-02, Batch size: 37, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 2.069e-03, Batch size: 24, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 4.281e+00, Batch size: 24, Momentum: 4.206e-01\n",
      "best loss: 503.24\t\tLearning rate: 8.859e-04, Batch size: 22, Momentum: 2.629e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.952e-05, Batch size: 22, Momentum: 5.258e-02\n",
      "best loss: 503.24\t\tLearning rate: 1.274e-05, Batch size: 22, Momentum: 7.887e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.158e-02, Batch size: 22, Momentum: 8.938e-01\n",
      "best loss: 523.34\t\tLearning rate: 2.976e-05, Batch size: 19, Momentum: 8.413e-01\n",
      "best loss: 523.34\t\tLearning rate: 1.624e-04, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 523.34\t\tLearning rate: 6.952e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 525.32\t\tLearning rate: 1.274e-05, Batch size: 17, Momentum: 3.155e-01\n",
      "best loss: 525.32\t\tLearning rate: 4.281e+00, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 525.32\t\tLearning rate: 1.438e-01, Batch size: 17, Momentum: 9.990e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 1.577e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 542.14\t\tLearning rate: 6.952e-05, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 6.952e-05, Batch size: 42, Momentum: 1.052e-01\n",
      "best loss: 547.94\t\tLearning rate: 3.793e-04, Batch size: 42, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 7.848e-01, Batch size: 42, Momentum: 1.577e-01\n",
      "best loss: 547.94\t\tLearning rate: 5.456e-06, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 570.19\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 6.835e-01\n",
      "best loss: 570.19\t\tLearning rate: 7.848e-01, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 1.833e+00, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 6.952e-05, Batch size: 44, Momentum: 6.309e-01\n",
      "best loss: 570.19\t\tLearning rate: 8.859e-04, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 576.42\t\tLearning rate: 2.336e-06, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.000e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 576.42\t\tLearning rate: 5.456e-06, Batch size: 39, Momentum: 8.413e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.624e-04, Batch size: 39, Momentum: 8.938e-01\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize with best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  324759.2031881873\n",
      "Iteration 350, Loss: 560.0870344734547\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m momentum \u001b[39m=\u001b[39m \u001b[39m5.258e-02\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m27\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m cov_mat, cov_mat_initial, mean_vec \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m key, subkey \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m batch \u001b[39m=\u001b[39m data[batch_indices]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m eigvals, eig_vecs, velocity, loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     best_loss \u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, value_and_grad\n",
    "\n",
    "\n",
    "\n",
    "# learning_rate = 1e0\n",
    "# momentum = 0.5\n",
    "# batch_size = 4\n",
    "# best loss: 533.36\t\tLearning rate: 2.976e-05, Batch size: 27, Momentum: 5.258e-02\n",
    "learning_rate = 2.976e-05\n",
    "momentum = 5.258e-02\n",
    "batch_size = 27\n",
    "\n",
    "cov_mat, cov_mat_initial, mean_vec = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
