{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 10000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  453.32249310984736\n",
      "best loss: 443.79\t\tLearning rate: 7.848e-07, Batch size: 17, Momentum: 7.361e-01\n",
      "Initial loss:  458.22739480976895\n",
      "best loss: 444.71\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  448.0507361037731\n",
      "best loss: 443.25\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 5.258e-02\n",
      "Initial loss:  449.77734191092804\n",
      "best loss: 444.96\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 2.103e-01\n",
      "Initial loss:  83460.90946897563\n",
      "best loss: 450.16\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 5.258e-02\n",
      "Initial loss:  455.73026079140755\n",
      "best loss: 447.83\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 5.258e-01\n",
      "Initial loss:  480.3869335389575\n",
      "best loss: 470.54\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 4.206e-01\n",
      "Initial loss:  505008.2005505451\n",
      "best loss: 1159.80\t\tLearning rate: 4.281e-02, Batch size: 2, Momentum: 8.938e-01\n",
      "Initial loss:  275163.4479727426\n",
      "best loss: 1389.44\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 9.990e-01\n",
      "Initial loss:  84564.43392754\n",
      "best loss: 819.65\t\tLearning rate: 1.833e-04, Batch size: 47, Momentum: 4.206e-01\n",
      "Initial loss:  465031.90375327447\n",
      "best loss: 476.28\t\tLearning rate: 8.859e-08, Batch size: 17, Momentum: 6.309e-01\n",
      "Initial loss:  454.8894532930815\n",
      "best loss: 447.97\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 5.258e-02\n",
      "Initial loss:  454.0726188416371\n",
      "best loss: 448.92\t\tLearning rate: 2.069e-05, Batch size: 37, Momentum: 5.784e-01\n",
      "Initial loss:  455.06142135113106\n",
      "best loss: 446.46\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 9.464e-01\n",
      "Initial loss:  459.36194979542563\n",
      "best loss: 448.99\t\tLearning rate: 4.833e-03, Batch size: 37, Momentum: 9.990e-01\n",
      "Initial loss:  462.1474115857302\n",
      "best loss: 453.74\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "Initial loss:  132783.80516951304\n",
      "best loss: 877.76\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 9.464e-01\n",
      "Initial loss:  454.4303693118522\n",
      "best loss: 445.57\t\tLearning rate: 2.069e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "Initial loss:  474.8187592521563\n",
      "best loss: 451.91\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 9.464e-01\n",
      "Initial loss:  94523.43131848474\n",
      "best loss: 448.22\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 8.413e-01\n",
      "Initial loss:  130439.70024045461\n",
      "best loss: 1289.70\t\tLearning rate: 1.129e+00, Batch size: 34, Momentum: 6.835e-01\n",
      "Initial loss:  530054.3383553537\n",
      "best loss: 1092.16\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 7.887e-01\n",
      "Initial loss:  457.8921591246896\n",
      "best loss: 443.54\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 0.000e+00\n",
      "Initial loss:  524833.3787127145\n",
      "best loss: 885.35\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 5.258e-01\n",
      "Initial loss:  122920.43360084902\n",
      "best loss: 1309.10\t\tLearning rate: 3.360e+00, Batch size: 34, Momentum: 6.309e-01\n",
      "Initial loss:  463.13633950117\n",
      "best loss: 431.77\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  417589.0556647834\n",
      "best loss: 444.87\t\tLearning rate: 2.069e-05, Batch size: 14, Momentum: 4.732e-01\n",
      "Initial loss:  426927.5953538918\n",
      "best loss: 607.52\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 3.681e-01\n",
      "Initial loss:  476703.58580239123\n",
      "best loss: 1117.94\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "Initial loss:  730.5450952890121\n",
      "best loss: 445.64\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 9.990e-01\n",
      "Initial loss:  284162.798901227\n",
      "best loss: 1456.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 9.990e-01\n",
      "Initial loss:  371452.1018781069\n",
      "best loss: 987.54\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  638.4851530125396\n",
      "best loss: 462.72\t\tLearning rate: 5.456e-04, Batch size: 34, Momentum: 9.464e-01\n",
      "Initial loss:  115438.36448887897\n",
      "best loss: 425.96\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 1.577e-01\n",
      "Initial loss:  463.2182791778339\n",
      "best loss: 454.62\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.206e-01\n",
      "Initial loss:  286135.17571040074\n",
      "best loss: 1205.95\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 7.361e-01\n",
      "Initial loss:  373593.2877516155\n",
      "best loss: 1015.67\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 9.464e-01\n",
      "Initial loss:  481.7528691676558\n",
      "best loss: 430.66\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  395059.6321400896\n",
      "best loss: 1154.92\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 5.258e-02\n",
      "Initial loss:  132338.67637997822\n",
      "best loss: 557.71\t\tLearning rate: 1.000e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "Initial loss:  453.3027833364895\n",
      "best loss: 446.35\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 1.052e-01\n",
      "Initial loss:  465.15606400334195\n",
      "best loss: 437.97\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 5.784e-01\n",
      "Initial loss:  462.7215597407529\n",
      "best loss: 428.01\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 9.990e-01\n",
      "Initial loss:  457.284535297058\n",
      "best loss: 447.40\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 1.577e-01\n",
      "Initial loss:  97547.46417064531\n",
      "best loss: 33262.88\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 2.103e-01\n",
      "Initial loss:  302158.02991510846\n",
      "best loss: 16947.87\t\tLearning rate: 3.360e+00, Batch size: 14, Momentum: 5.258e-02\n",
      "Initial loss:  141840.8137190121\n",
      "best loss: 447.23\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 5.258e-01\n",
      "Initial loss:  304182.6815328581\n",
      "best loss: 525.40\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  290918.44231283874\n",
      "best loss: 42054.76\t\tLearning rate: 1.000e-08, Batch size: 42, Momentum: 0.000e+00\n",
      "Initial loss:  213490.45058177\n",
      "best loss: 1222.31\t\tLearning rate: 1.129e+00, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  470.4696285798963\n",
      "best loss: 455.06\t\tLearning rate: 4.281e-02, Batch size: 37, Momentum: 5.258e-01\n",
      "Initial loss:  334507.9082716952\n",
      "best loss: 823.32\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 9.990e-01\n",
      "Initial loss:  454.27247452115745\n",
      "best loss: 448.95\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.464e-01\n",
      "Initial loss:  459.8007414423235\n",
      "best loss: 448.28\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 0.000e+00\n",
      "Initial loss:  449.1581308412051\n",
      "best loss: 446.52\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 5.258e-01\n",
      "Initial loss:  480.2233099915713\n",
      "best loss: 448.21\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 5.258e-01\n",
      "Initial loss:  332584.01988119236\n",
      "best loss: 1316.60\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 1.052e-01\n",
      "Initial loss:  456.47613980026426\n",
      "best loss: 447.59\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 8.938e-01\n",
      "Initial loss:  248469.0440934593\n",
      "best loss: 529.74\t\tLearning rate: 2.637e-07, Batch size: 4, Momentum: 0.000e+00\n",
      "Initial loss:  454.5771169325637\n",
      "best loss: 445.37\t\tLearning rate: 1.000e-08, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  115928.61303975845\n",
      "best loss: 451.26\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "Initial loss:  96479.64959112003\n",
      "best loss: 445.00\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 2.103e-01\n",
      "Initial loss:  272607.55126226204\n",
      "best loss: 1150.17\t\tLearning rate: 4.281e-02, Batch size: 32, Momentum: 8.938e-01\n",
      "Initial loss:  430624.0578614391\n",
      "best loss: 452.39\t\tLearning rate: 8.859e-08, Batch size: 42, Momentum: 8.938e-01\n",
      "Initial loss:  131779.7119393675\n",
      "best loss: 802.81\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 4.732e-01\n",
      "Initial loss:  461.90468390964014\n",
      "best loss: 452.65\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 3.155e-01\n",
      "Initial loss:  455.01821342664783\n",
      "best loss: 451.53\t\tLearning rate: 2.069e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "Initial loss:  209805.0662294715\n",
      "best loss: 472.83\t\tLearning rate: 7.848e-07, Batch size: 9, Momentum: 2.103e-01\n",
      "Initial loss:  88272.6472282479\n",
      "best loss: 1094.73\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "Initial loss:  456.29272080124326\n",
      "best loss: 447.34\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 9.990e-01\n",
      "Initial loss:  450.32929752974144\n",
      "best loss: 445.00\t\tLearning rate: 7.848e-07, Batch size: 32, Momentum: 7.887e-01\n",
      "Initial loss:  239095.8226997394\n",
      "best loss: 961.51\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 7.361e-01\n",
      "Initial loss:  118805.6399209651\n",
      "best loss: 450.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "Initial loss:  438.47741465309014\n",
      "best loss: 427.99\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 4.732e-01\n",
      "Initial loss:  452.19375621496414\n",
      "best loss: 448.34\t\tLearning rate: 1.438e-02, Batch size: 44, Momentum: 2.629e-01\n",
      "Initial loss:  454.8106391701049\n",
      "best loss: 447.19\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "Initial loss:  206115.79595754136\n",
      "best loss: 779.94\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "Initial loss:  150250.74232100384\n",
      "best loss: 30308.49\t\tLearning rate: 1.624e-03, Batch size: 37, Momentum: 5.258e-02\n",
      "Initial loss:  170871.98405428507\n",
      "best loss: 485.76\t\tLearning rate: 8.859e-08, Batch size: 24, Momentum: 4.206e-01\n",
      "Initial loss:  58637.68045519264\n",
      "best loss: 445.16\t\tLearning rate: 2.336e-06, Batch size: 12, Momentum: 8.413e-01\n",
      "Initial loss:  459.44985221077826\n",
      "best loss: 427.71\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 5.784e-01\n",
      "Initial loss:  167828.01289430313\n",
      "best loss: 720.64\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 9.990e-01\n",
      "Initial loss:  81922.19189851476\n",
      "best loss: 665.82\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 5.258e-01\n",
      "Initial loss:  444.4902594371316\n",
      "best loss: 430.86\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 2.629e-01\n",
      "Initial loss:  515238.3395577945\n",
      "best loss: 1359.06\t\tLearning rate: 3.360e+00, Batch size: 50, Momentum: 8.413e-01\n",
      "Initial loss:  140478.2948176836\n",
      "best loss: 448.06\t\tLearning rate: 2.976e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "Initial loss:  448.10745760179645\n",
      "best loss: 447.64\t\tLearning rate: 2.976e-08, Batch size: 32, Momentum: 3.155e-01\n",
      "Initial loss:  516.8844421418873\n",
      "best loss: 467.84\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.206e-01\n",
      "Initial loss:  474.5367369436425\n",
      "best loss: 452.03\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.413e-01\n",
      "Initial loss:  493.6300678905059\n",
      "best loss: 475.86\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.258e-01\n",
      "Initial loss:  458.0466972721841\n",
      "best loss: 446.28\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 5.258e-02\n",
      "Initial loss:  457.23332927228313\n",
      "best loss: 443.76\t\tLearning rate: 1.000e+01, Batch size: 14, Momentum: 5.258e-01\n",
      "Initial loss:  469.61134111836645\n",
      "best loss: 458.75\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.938e-01\n",
      "Initial loss:  164573.09081852908\n",
      "best loss: 456.06\t\tLearning rate: 6.952e-06, Batch size: 12, Momentum: 8.938e-01\n",
      "Initial loss:  463.07449512816015\n",
      "best loss: 427.36\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 7.887e-01\n",
      "Initial loss:  463.1030162698439\n",
      "best loss: 452.75\t\tLearning rate: 2.976e-08, Batch size: 34, Momentum: 4.732e-01\n",
      "Initial loss:  19984.090040738178\n",
      "best loss: 436.76\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "Initial loss:  444942.1240341818\n",
      "best loss: 854.09\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 2.103e-01\n",
      "Initial loss:  454.25107604545104\n",
      "best loss: 446.84\t\tLearning rate: 2.069e-05, Batch size: 44, Momentum: 4.206e-01\n",
      "Initial loss:  440.56495233458054\n",
      "best loss: 429.00\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 4.206e-01\n",
      "Initial loss:  75058.05359297068\n",
      "best loss: 995.84\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 2.103e-01\n",
      "Initial loss:  116299.64944714918\n",
      "best loss: 449.98\t\tLearning rate: 2.336e-06, Batch size: 42, Momentum: 6.835e-01\n",
      "Initial loss:  465.70952252020294\n",
      "best loss: 460.00\t\tLearning rate: 2.336e-06, Batch size: 47, Momentum: 6.309e-01\n",
      "Initial loss:  146768.2773574457\n",
      "best loss: 38894.08\t\tLearning rate: 2.976e-08, Batch size: 7, Momentum: 2.103e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (learning_rate, batch_size, momentum) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(hyperparameter_tuples):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     patches \u001b[39m=\u001b[39m extract_patches(images, patch_size, num_patches\u001b[39m=\u001b[39;49mnum_patches, seed\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     best_cov_mat, cov_mat_initial, mean_vec, best_loss \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/image_utils.py:67\u001b[0m, in \u001b[0;36mextract_patches\u001b[0;34m(stack, patch_size, num_patches, seed, verbose)\u001b[0m\n\u001b[1;32m     65\u001b[0m     iterator \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(num_patches)\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m---> 67\u001b[0m     patches\u001b[39m.\u001b[39mappend(stack[image_indices[i], x_indices[i]:x_indices[i]\u001b[39m+\u001b[39;49mpatch_size, y_indices[i]:y_indices[i]\u001b[39m+\u001b[39;49mpatch_size])\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mnumpy\u001b[39m.\u001b[39marray(patches)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/array.py:349\u001b[0m, in \u001b[0;36mArrayImpl.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m   \u001b[39mif\u001b[39;00m (dispatch\u001b[39m.\u001b[39mis_single_device_sharding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msharding) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fully_replicated \u001b[39mor\u001b[39;00m _is_reduced_on_dim(idx)):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m lax_numpy\u001b[39m.\u001b[39;49m_rewriting_take(\u001b[39mself\u001b[39;49m, idx)\n\u001b[1;32m    350\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m api\u001b[39m.\u001b[39mdevice_put(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value[idx])\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3908\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3905\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3907\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39mshape)\n\u001b[0;32m-> 3908\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3909\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3917\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3914\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3915\u001b[0m             unique_indices, mode, fill_value):\n\u001b[1;32m   3916\u001b[0m   idx \u001b[39m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m-> 3917\u001b[0m   indexer \u001b[39m=\u001b[39m _index_to_gather(shape(arr), idx)  \u001b[39m# shared with _scatter_update\u001b[39;00m\n\u001b[1;32m   3918\u001b[0m   y \u001b[39m=\u001b[39m arr\n\u001b[1;32m   3920\u001b[0m   \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4218\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   4216\u001b[0m   last_dim \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(gather_indices_shape)\n\u001b[1;32m   4217\u001b[0m   gather_indices_shape\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 4218\u001b[0m   gather_indices_array \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mconcatenate([\n\u001b[1;32m   4219\u001b[0m     lax\u001b[39m.\u001b[39mbroadcast_in_dim(g, gather_indices_shape, \u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(i, i \u001b[39m+\u001b[39m g\u001b[39m.\u001b[39mndim)))\n\u001b[1;32m   4220\u001b[0m     \u001b[39mfor\u001b[39;00m g, i \u001b[39min\u001b[39;00m gather_indices],\n\u001b[1;32m   4221\u001b[0m     last_dim)\n\u001b[1;32m   4223\u001b[0m dnums \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mGatherDimensionNumbers(\n\u001b[1;32m   4224\u001b[0m   offset_dims \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(offset_dims),\n\u001b[1;32m   4225\u001b[0m   collapsed_slice_dims \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(collapsed_slice_dims)),\n\u001b[1;32m   4226\u001b[0m   start_index_map \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(start_index_map)\n\u001b[1;32m   4227\u001b[0m )\n\u001b[1;32m   4228\u001b[0m \u001b[39mreturn\u001b[39;00m _Indexer(\n\u001b[1;32m   4229\u001b[0m   slice_shape\u001b[39m=\u001b[39mslice_shape,\n\u001b[1;32m   4230\u001b[0m   newaxis_dims\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(newaxis_dims),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4235\u001b[0m   unique_indices\u001b[39m=\u001b[39madvanced_indexes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4236\u001b[0m   indices_are_sorted\u001b[39m=\u001b[39madvanced_indexes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4219\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4216\u001b[0m   last_dim \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(gather_indices_shape)\n\u001b[1;32m   4217\u001b[0m   gather_indices_shape\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[1;32m   4218\u001b[0m   gather_indices_array \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mconcatenate([\n\u001b[0;32m-> 4219\u001b[0m     lax\u001b[39m.\u001b[39;49mbroadcast_in_dim(g, gather_indices_shape, \u001b[39mtuple\u001b[39;49m(\u001b[39mrange\u001b[39;49m(i, i \u001b[39m+\u001b[39;49m g\u001b[39m.\u001b[39;49mndim)))\n\u001b[1;32m   4220\u001b[0m     \u001b[39mfor\u001b[39;00m g, i \u001b[39min\u001b[39;00m gather_indices],\n\u001b[1;32m   4221\u001b[0m     last_dim)\n\u001b[1;32m   4223\u001b[0m dnums \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mGatherDimensionNumbers(\n\u001b[1;32m   4224\u001b[0m   offset_dims \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(offset_dims),\n\u001b[1;32m   4225\u001b[0m   collapsed_slice_dims \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(collapsed_slice_dims)),\n\u001b[1;32m   4226\u001b[0m   start_index_map \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(start_index_map)\n\u001b[1;32m   4227\u001b[0m )\n\u001b[1;32m   4228\u001b[0m \u001b[39mreturn\u001b[39;00m _Indexer(\n\u001b[1;32m   4229\u001b[0m   slice_shape\u001b[39m=\u001b[39mslice_shape,\n\u001b[1;32m   4230\u001b[0m   newaxis_dims\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(newaxis_dims),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4235\u001b[0m   unique_indices\u001b[39m=\u001b[39madvanced_indexes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4236\u001b[0m   indices_are_sorted\u001b[39m=\u001b[39madvanced_indexes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/lax/lax.py:776\u001b[0m, in \u001b[0;36mbroadcast_in_dim\u001b[0;34m(operand, shape, broadcast_dimensions)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_in_dim\u001b[39m(operand: ArrayLike, shape: Shape,\n\u001b[1;32m    759\u001b[0m                      broadcast_dimensions: Sequence[\u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    760\u001b[0m   \u001b[39m\"\"\"Wraps XLA's `BroadcastInDim\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[39m  <https://www.tensorflow.org/xla/operation_semantics#broadcastindim>`_\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[39m  operator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39m    jax.lax.broadcast : simpler interface to add new leading dimensions.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m   \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49mndim(operand) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(shape) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(broadcast_dimensions) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(operand, Array):\n\u001b[1;32m    777\u001b[0m     \u001b[39mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    778\u001b[0m   \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mjax_dynamic_shapes:\n\u001b[1;32m    779\u001b[0m     \u001b[39m# We must gate this behavior under a flag because otherwise the errors\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[39m# raised are different (and have worse source provenance information).\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3154\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3124\u001b[0m \u001b[39mReturn the number of dimensions of an array.\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3151\u001b[0m \n\u001b[1;32m   3152\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3154\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39;49mndim\n\u001b[1;32m   3155\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[39mreturn\u001b[39;00m asarray(a)\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/array.py:260\u001b[0m, in \u001b[0;36mArrayImpl.ndim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdtype\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    258\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maval\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 260\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mndim\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    262\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    264\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msize\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 425.96\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 1.577e-01\n",
      "best loss: 427.36\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 7.887e-01\n",
      "best loss: 427.71\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 5.784e-01\n",
      "best loss: 427.99\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 4.732e-01\n",
      "best loss: 428.01\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 9.990e-01\n",
      "best loss: 429.00\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 4.206e-01\n",
      "best loss: 430.66\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 430.86\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 2.629e-01\n",
      "best loss: 431.77\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 436.76\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "best loss: 437.97\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 5.784e-01\n",
      "best loss: 443.25\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 5.258e-02\n",
      "best loss: 443.54\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 0.000e+00\n",
      "best loss: 443.76\t\tLearning rate: 1.000e+01, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 443.79\t\tLearning rate: 7.848e-07, Batch size: 17, Momentum: 7.361e-01\n",
      "best loss: 444.71\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 444.87\t\tLearning rate: 2.069e-05, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 444.96\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 2.103e-01\n",
      "best loss: 445.00\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 445.00\t\tLearning rate: 7.848e-07, Batch size: 32, Momentum: 7.887e-01\n",
      "best loss: 445.16\t\tLearning rate: 2.336e-06, Batch size: 12, Momentum: 8.413e-01\n",
      "best loss: 445.37\t\tLearning rate: 1.000e-08, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 445.57\t\tLearning rate: 2.069e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 445.64\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 9.990e-01\n",
      "best loss: 446.28\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 5.258e-02\n",
      "best loss: 446.35\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 1.052e-01\n",
      "best loss: 446.46\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 446.52\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 5.258e-01\n",
      "best loss: 446.84\t\tLearning rate: 2.069e-05, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 447.19\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "best loss: 447.23\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 5.258e-01\n",
      "best loss: 447.34\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 9.990e-01\n",
      "best loss: 447.40\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 1.577e-01\n",
      "best loss: 447.59\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 8.938e-01\n",
      "best loss: 447.64\t\tLearning rate: 2.976e-08, Batch size: 32, Momentum: 3.155e-01\n",
      "best loss: 447.83\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 5.258e-01\n",
      "best loss: 447.97\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 448.06\t\tLearning rate: 2.976e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "best loss: 448.21\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 5.258e-01\n",
      "best loss: 448.22\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 8.413e-01\n",
      "best loss: 448.28\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 0.000e+00\n",
      "best loss: 448.34\t\tLearning rate: 1.438e-02, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 448.92\t\tLearning rate: 2.069e-05, Batch size: 37, Momentum: 5.784e-01\n",
      "best loss: 448.95\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.464e-01\n",
      "best loss: 448.99\t\tLearning rate: 4.833e-03, Batch size: 37, Momentum: 9.990e-01\n",
      "best loss: 449.98\t\tLearning rate: 2.336e-06, Batch size: 42, Momentum: 6.835e-01\n",
      "best loss: 450.16\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 5.258e-02\n",
      "best loss: 450.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "best loss: 451.26\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "best loss: 451.53\t\tLearning rate: 2.069e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 451.91\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 9.464e-01\n",
      "best loss: 452.03\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 452.39\t\tLearning rate: 8.859e-08, Batch size: 42, Momentum: 8.938e-01\n",
      "best loss: 452.65\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 3.155e-01\n",
      "best loss: 452.75\t\tLearning rate: 2.976e-08, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 453.74\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 454.62\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.206e-01\n",
      "best loss: 455.06\t\tLearning rate: 4.281e-02, Batch size: 37, Momentum: 5.258e-01\n",
      "best loss: 456.06\t\tLearning rate: 6.952e-06, Batch size: 12, Momentum: 8.938e-01\n",
      "best loss: 458.75\t\tLearning rate: 6.952e-06, Batch size: 32, Momentum: 8.938e-01\n",
      "best loss: 460.00\t\tLearning rate: 2.336e-06, Batch size: 47, Momentum: 6.309e-01\n",
      "best loss: 462.72\t\tLearning rate: 5.456e-04, Batch size: 34, Momentum: 9.464e-01\n",
      "best loss: 467.84\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 470.54\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 472.83\t\tLearning rate: 7.848e-07, Batch size: 9, Momentum: 2.103e-01\n",
      "best loss: 475.86\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.258e-01\n",
      "best loss: 476.28\t\tLearning rate: 8.859e-08, Batch size: 17, Momentum: 6.309e-01\n",
      "best loss: 485.76\t\tLearning rate: 8.859e-08, Batch size: 24, Momentum: 4.206e-01\n",
      "best loss: 525.40\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 529.74\t\tLearning rate: 2.637e-07, Batch size: 4, Momentum: 0.000e+00\n",
      "best loss: 557.71\t\tLearning rate: 1.000e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 607.52\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 3.681e-01\n",
      "best loss: 665.82\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 5.258e-01\n",
      "best loss: 720.64\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 9.990e-01\n",
      "best loss: 779.94\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "best loss: 802.81\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 819.65\t\tLearning rate: 1.833e-04, Batch size: 47, Momentum: 4.206e-01\n",
      "best loss: 823.32\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 9.990e-01\n",
      "best loss: 854.09\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 877.76\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 9.464e-01\n",
      "best loss: 885.35\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 5.258e-01\n",
      "best loss: 961.51\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 7.361e-01\n",
      "best loss: 987.54\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 1.052e-01\n",
      "best loss: 995.84\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 2.103e-01\n",
      "best loss: 1015.67\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 1092.16\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 7.887e-01\n",
      "best loss: 1094.73\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "best loss: 1117.94\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "best loss: 1150.17\t\tLearning rate: 4.281e-02, Batch size: 32, Momentum: 8.938e-01\n",
      "best loss: 1154.92\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 5.258e-02\n",
      "best loss: 1159.80\t\tLearning rate: 4.281e-02, Batch size: 2, Momentum: 8.938e-01\n",
      "best loss: 1205.95\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 7.361e-01\n",
      "best loss: 1222.31\t\tLearning rate: 1.129e+00, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 1289.70\t\tLearning rate: 1.129e+00, Batch size: 34, Momentum: 6.835e-01\n",
      "best loss: 1309.10\t\tLearning rate: 3.360e+00, Batch size: 34, Momentum: 6.309e-01\n",
      "best loss: 1316.60\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 1.052e-01\n",
      "best loss: 1359.06\t\tLearning rate: 3.360e+00, Batch size: 50, Momentum: 8.413e-01\n",
      "best loss: 1389.44\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 9.990e-01\n",
      "best loss: 1456.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 9.990e-01\n",
      "best loss: 16947.87\t\tLearning rate: 3.360e+00, Batch size: 14, Momentum: 5.258e-02\n",
      "best loss: 30308.49\t\tLearning rate: 1.624e-03, Batch size: 37, Momentum: 5.258e-02\n",
      "best loss: 33262.88\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 38894.08\t\tLearning rate: 2.976e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "best loss: 42054.76\t\tLearning rate: 1.000e-08, Batch size: 42, Momentum: 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
