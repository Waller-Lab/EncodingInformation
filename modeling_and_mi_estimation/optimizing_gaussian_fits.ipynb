{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Opening BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 1000\n",
    "num_patches = 1000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)\n",
    "patches = extract_patches(images, patch_size, num_patches=num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.linalg import toeplitz\n",
    "from jax import grad, jit, value_and_grad\n",
    "\n",
    "def make_doubly_toeplitz(top_row, patch_size):\n",
    "    \"\"\"\n",
    "    Make a doubly toeplitz matrix from its top row, which is the\n",
    "    minimum number of parameters needed to specify the matrix.\n",
    "    \"\"\"\n",
    "    # split into rows\n",
    "    top_rows = np.split(top_row, patch_size)\n",
    "    # make into toeplitz blocks\n",
    "    blocks = []\n",
    "    for tr in top_rows:\n",
    "        blocks.append(toeplitz(tr))\n",
    "    # use blocks to construct doubl\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(blocks)):\n",
    "        row_blocks = [blocks[abs(i - j)] for j in range(len(blocks))]\n",
    "        row = np.hstack(row_blocks)\n",
    "        rows.append(row)\n",
    "    doubly_toeplitz_mat = np.vstack(rows)\n",
    "    return doubly_toeplitz_mat\n",
    "\n",
    "\n",
    "def gaussian_likelihood(cov_mat, mean_vec, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the log likelihood of a multivariate gaussian\n",
    "    for a batch of NxWXH samples.\n",
    "    \"\"\"\n",
    "    log_likelihoods = []\n",
    "    for sample in batch:\n",
    "        ll = jax.scipy.stats.multivariate_normal.logpdf(sample.reshape(-1), mean=mean_vec, cov=cov_mat)\n",
    "        log_likelihoods.append(ll)\n",
    "    return np.array(log_likelihoods)\n",
    "\n",
    "def batch_nll(log_likelihoods):\n",
    "    return -np.mean(log_likelihoods)\n",
    "\n",
    "def loss_function(eigvals, eig_vecs, mean_vec, data):\n",
    "    cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    # cov_mat = make_doubly_toeplitz(cov_mat_row, patch_size)\n",
    "    # cov_mat = make_positive_definite(cov_mat, eigenvalue_floor)\n",
    "    ll = gaussian_likelihood(cov_mat, mean_vec, data)\n",
    "    return batch_nll(ll)\n",
    "\n",
    "def make_valid_stationary(eigvals, eig_vecs, eigenvalue_floor):\n",
    "    eigvals = np.where(eigvals < eigenvalue_floor, eigenvalue_floor, eigvals)\n",
    "    cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    dt_cov_mat = make_doubly_toeplitz(cov_mat[0], patch_size)\n",
    "    eigvals, eig_vecs = np.linalg.eigh(dt_cov_mat)\n",
    "    eigvals = np.where(eigvals < eigenvalue_floor, eigenvalue_floor, eigvals)\n",
    "    return eigvals, eig_vecs\n",
    "\n",
    "@jit\n",
    "def optmization_step(eigvals, eig_vecs, velocity, data, mean_vec, momentum, learning_rate, eigenvalue_floor):\n",
    "    grad_fn = grad(loss_function, argnums=0)\n",
    "    eigenvalues_grad = grad_fn(eigvals, eig_vecs, mean_vec, data)\n",
    "    new_velocity = momentum * velocity - learning_rate * eigenvalues_grad\n",
    "    eigvals = eigvals + new_velocity\n",
    "    # prox operator: make sure make sure positive definite, make sure doubly toeplitz\n",
    "    eigvals, eig_vecs = make_valid_stationary(eigvals, eig_vecs, eigenvalue_floor)\n",
    "    loss = loss_function(eigvals, eig_vecs, mean_vec, data)\n",
    "    return eigvals, eig_vecs, new_velocity, loss\n",
    "\n",
    "\n",
    "def run_optimization(data, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3):\n",
    "    patch_size = int(np.sqrt(np.prod(np.array(data.shape)[1:])))\n",
    "    # Initialize parameters, hyperparameters\n",
    "    mean_vec = np.ones(patch_size**2) * np.mean(data)\n",
    "\n",
    "    # initialize covariance matrix so likelihood is not nan\n",
    "    cov_mat_initial = make_positive_definite(compute_stationary_cov_mat(data), eigenvalue_floor=eigenvalue_floor)\n",
    "\n",
    "    initial_evs, initial_eig_vecs = make_valid_stationary(*np.linalg.eigh(cov_mat_initial), eigenvalue_floor)\n",
    "    print('Initial loss: ', loss_function(initial_evs, initial_eig_vecs, mean_vec, data[:batch_size]))\n",
    "\n",
    "    cov_mat_initial = initial_eig_vecs @ np.diag(initial_evs) @ initial_eig_vecs.T\n",
    "\n",
    "    if np.isnan(jax.scipy.stats.multivariate_normal.logpdf(patches[0].flatten(), mean=mean_vec, cov=cov_mat_initial)):\n",
    "        raise ValueError(\"Initial likelihood is nan\")\n",
    "    \n",
    "    # Training loop\n",
    "    eigvals = initial_evs\n",
    "    eig_vecs = initial_eig_vecs\n",
    "    velocity = np.zeros_like(eigvals)\n",
    "    best_loss = np.inf\n",
    "    key = jax.random.PRNGKey(onp.random.randint(0, 100000))\n",
    "    for i in range(1000):\n",
    "        # select a random batch\n",
    "        batch_indices = jax.random.randint(key, shape=(batch_size,), minval=0, maxval=len(data))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        batch = data[batch_indices]\n",
    "        \n",
    "        eigvals, eig_vecs, velocity, loss = optmization_step(eigvals, eig_vecs, velocity, \n",
    "                                                             batch, mean_vec, momentum, learning_rate, eigenvalue_floor)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_eigvals = eigvals\n",
    "            best_eig_vecs = eig_vecs\n",
    "        print(f\"Iteration {i+1}, Loss: {loss}\", end='\\r')\n",
    "    eigvals, eig_vecs = make_valid_stationary(best_eigvals, best_eig_vecs, eigenvalue_floor)\n",
    "    best_cov_mat = eig_vecs @ np.diag(eigvals) @ eig_vecs.T\n",
    "    return best_cov_mat, cov_mat_initial, mean_vec, best_loss\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  323799.07958974945\n",
      "Iteration 72, Loss: 797.44085060267017\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m learning_rate, batch_size, momentum \u001b[39min\u001b[39;00m hyperparameter_tuples:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     best_cov_mat, cov_mat_initial, mean_vec, best_loss \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         best_hp_loss \u001b[39m=\u001b[39m best_loss\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m key, subkey \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m batch \u001b[39m=\u001b[39m data[batch_indices]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m eigvals, eig_vecs, velocity, loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     best_loss \u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for learning_rate, batch_size, momentum in hyperparameter_tuples:\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 438.59\t\tLearning rate: 6.952e-05, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 438.59\t\tLearning rate: 1.833e+00, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 438.59\t\tLearning rate: 6.158e-02, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 6.158e-02, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 7.361e-01\n",
      "best loss: 443.39\t\tLearning rate: 5.456e-06, Batch size: 4, Momentum: 5.784e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 2.103e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.833e+00, Batch size: 32, Momentum: 2.629e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.274e-05, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 450.95\t\tLearning rate: 1.438e-01, Batch size: 47, Momentum: 7.887e-01\n",
      "best loss: 450.95\t\tLearning rate: 3.793e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 451.99\t\tLearning rate: 7.848e-01, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.624e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.336e-06, Batch size: 29, Momentum: 2.103e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.438e-01, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 5.456e-06, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 6.835e-01\n",
      "best loss: 451.99\t\tLearning rate: 6.158e-02, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 4.732e-01\n",
      "best loss: 452.84\t\tLearning rate: 1.129e-02, Batch size: 7, Momentum: 4.206e-01\n",
      "best loss: 459.16\t\tLearning rate: 4.833e-03, Batch size: 12, Momentum: 7.887e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 7.361e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 1.577e-01\n",
      "best loss: 460.83\t\tLearning rate: 1.129e-02, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 460.83\t\tLearning rate: 3.360e-01, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 3.155e-01\n",
      "best loss: 467.16\t\tLearning rate: 2.336e-06, Batch size: 27, Momentum: 5.258e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 7.848e-01, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.274e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 6.952e-05, Batch size: 27, Momentum: 9.990e-01\n",
      "best loss: 469.16\t\tLearning rate: 2.069e-03, Batch size: 9, Momentum: 0.000e+00\n",
      "best loss: 469.16\t\tLearning rate: 1.000e+01, Batch size: 9, Momentum: 4.206e-01\n",
      "best loss: 469.16\t\tLearning rate: 6.952e-05, Batch size: 9, Momentum: 8.938e-01\n",
      "best loss: 469.16\t\tLearning rate: 7.848e-01, Batch size: 9, Momentum: 8.413e-01\n",
      "best loss: 476.95\t\tLearning rate: 1.129e-02, Batch size: 37, Momentum: 0.000e+00\n",
      "best loss: 476.95\t\tLearning rate: 6.158e-02, Batch size: 37, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 2.069e-03, Batch size: 24, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 4.281e+00, Batch size: 24, Momentum: 4.206e-01\n",
      "best loss: 503.24\t\tLearning rate: 8.859e-04, Batch size: 22, Momentum: 2.629e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.952e-05, Batch size: 22, Momentum: 5.258e-02\n",
      "best loss: 503.24\t\tLearning rate: 1.274e-05, Batch size: 22, Momentum: 7.887e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.158e-02, Batch size: 22, Momentum: 8.938e-01\n",
      "best loss: 523.34\t\tLearning rate: 2.976e-05, Batch size: 19, Momentum: 8.413e-01\n",
      "best loss: 523.34\t\tLearning rate: 1.624e-04, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 523.34\t\tLearning rate: 6.952e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 525.32\t\tLearning rate: 1.274e-05, Batch size: 17, Momentum: 3.155e-01\n",
      "best loss: 525.32\t\tLearning rate: 4.281e+00, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 525.32\t\tLearning rate: 1.438e-01, Batch size: 17, Momentum: 9.990e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 1.577e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 542.14\t\tLearning rate: 6.952e-05, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 6.952e-05, Batch size: 42, Momentum: 1.052e-01\n",
      "best loss: 547.94\t\tLearning rate: 3.793e-04, Batch size: 42, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 7.848e-01, Batch size: 42, Momentum: 1.577e-01\n",
      "best loss: 547.94\t\tLearning rate: 5.456e-06, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 570.19\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 6.835e-01\n",
      "best loss: 570.19\t\tLearning rate: 7.848e-01, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 1.833e+00, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 6.952e-05, Batch size: 44, Momentum: 6.309e-01\n",
      "best loss: 570.19\t\tLearning rate: 8.859e-04, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 576.42\t\tLearning rate: 2.336e-06, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.000e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 576.42\t\tLearning rate: 5.456e-06, Batch size: 39, Momentum: 8.413e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.624e-04, Batch size: 39, Momentum: 8.938e-01\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize with best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  324759.2031881873\n",
      "Iteration 350, Loss: 560.0870344734547\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m momentum \u001b[39m=\u001b[39m \u001b[39m5.258e-02\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m27\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m cov_mat, cov_mat_initial, mean_vec \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m key, subkey \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m batch \u001b[39m=\u001b[39m data[batch_indices]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m eigvals, eig_vecs, velocity, loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimizing_gaussian_fits.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     best_loss \u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, value_and_grad\n",
    "\n",
    "\n",
    "\n",
    "# learning_rate = 1e0\n",
    "# momentum = 0.5\n",
    "# batch_size = 4\n",
    "# best loss: 533.36\t\tLearning rate: 2.976e-05, Batch size: 27, Momentum: 5.258e-02\n",
    "learning_rate = 2.976e-05\n",
    "momentum = 5.258e-02\n",
    "batch_size = 27\n",
    "\n",
    "cov_mat, cov_mat_initial, mean_vec = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO draw before after samples from optimized covariance mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
