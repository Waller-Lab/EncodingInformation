{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 10000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  453.32249310984736\n",
      "best loss: 443.79\t\tLearning rate: 7.848e-07, Batch size: 17, Momentum: 7.361e-01\n",
      "Initial loss:  458.22739480976895\n",
      "best loss: 444.71\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  448.0507361037731\n",
      "best loss: 443.25\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 5.258e-02\n",
      "Initial loss:  449.77734191092804\n",
      "best loss: 444.96\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 2.103e-01\n",
      "Initial loss:  83460.90946897563\n",
      "best loss: 450.16\t\tLearning rate: 2.069e-05, Batch size: 24, Momentum: 5.258e-02\n",
      "Initial loss:  455.73026079140755\n",
      "best loss: 447.83\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 5.258e-01\n",
      "Initial loss:  480.3869335389575\n",
      "best loss: 470.54\t\tLearning rate: 8.859e-08, Batch size: 44, Momentum: 4.206e-01\n",
      "Initial loss:  505008.2005505451\n",
      "best loss: 1159.80\t\tLearning rate: 4.281e-02, Batch size: 2, Momentum: 8.938e-01\n",
      "Initial loss:  275163.4479727426\n",
      "best loss: 1389.44\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 9.990e-01\n",
      "Initial loss:  84564.43392754\n",
      "best loss: 819.65\t\tLearning rate: 1.833e-04, Batch size: 47, Momentum: 4.206e-01\n",
      "Initial loss:  465031.90375327447\n",
      "best loss: 476.28\t\tLearning rate: 8.859e-08, Batch size: 17, Momentum: 6.309e-01\n",
      "Initial loss:  454.8894532930815\n",
      "best loss: 447.97\t\tLearning rate: 1.624e-03, Batch size: 32, Momentum: 5.258e-02\n",
      "Initial loss:  454.0726188416371\n",
      "best loss: 448.92\t\tLearning rate: 2.069e-05, Batch size: 37, Momentum: 5.784e-01\n",
      "Initial loss:  455.06142135113106\n",
      "best loss: 446.46\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 9.464e-01\n",
      "Initial loss:  459.36194979542563\n",
      "best loss: 448.99\t\tLearning rate: 4.833e-03, Batch size: 37, Momentum: 9.990e-01\n",
      "Initial loss:  462.1474115857302\n",
      "best loss: 453.74\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "Initial loss:  132783.80516951304\n",
      "best loss: 877.76\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 9.464e-01\n",
      "Initial loss:  454.4303693118522\n",
      "best loss: 445.57\t\tLearning rate: 2.069e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "Initial loss:  474.8187592521563\n",
      "best loss: 451.91\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 9.464e-01\n",
      "Initial loss:  94523.43131848474\n",
      "best loss: 448.22\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 8.413e-01\n",
      "Initial loss:  130439.70024045461\n",
      "best loss: 1289.70\t\tLearning rate: 1.129e+00, Batch size: 34, Momentum: 6.835e-01\n",
      "Initial loss:  530054.3383553537\n",
      "best loss: 1092.16\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 7.887e-01\n",
      "Initial loss:  457.8921591246896\n",
      "best loss: 443.54\t\tLearning rate: 7.848e-07, Batch size: 7, Momentum: 0.000e+00\n",
      "Initial loss:  524833.3787127145\n",
      "best loss: 885.35\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 5.258e-01\n",
      "Initial loss:  122920.43360084902\n",
      "best loss: 1309.10\t\tLearning rate: 3.360e+00, Batch size: 34, Momentum: 6.309e-01\n",
      "Initial loss:  463.13633950117\n",
      "best loss: 431.77\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  417589.0556647834\n",
      "best loss: 444.87\t\tLearning rate: 2.069e-05, Batch size: 14, Momentum: 4.732e-01\n",
      "Initial loss:  426927.5953538918\n",
      "best loss: 607.52\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 3.681e-01\n",
      "Initial loss:  476703.58580239123\n",
      "best loss: 1117.94\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "Initial loss:  730.5450952890121\n",
      "best loss: 445.64\t\tLearning rate: 5.456e-04, Batch size: 12, Momentum: 9.990e-01\n",
      "Initial loss:  284162.798901227\n",
      "best loss: 1456.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 9.990e-01\n",
      "Initial loss:  371452.1018781069\n",
      "best loss: 987.54\t\tLearning rate: 1.438e-02, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  638.4851530125396\n",
      "best loss: 462.72\t\tLearning rate: 5.456e-04, Batch size: 34, Momentum: 9.464e-01\n",
      "Initial loss:  115438.36448887897\n",
      "best loss: 425.96\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 1.577e-01\n",
      "Initial loss:  463.2182791778339\n",
      "best loss: 454.62\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.206e-01\n",
      "Initial loss:  286135.17571040074\n",
      "best loss: 1205.95\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 7.361e-01\n",
      "Initial loss:  373593.2877516155\n",
      "best loss: 1015.67\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 9.464e-01\n",
      "Initial loss:  481.7528691676558\n",
      "best loss: 430.66\t\tLearning rate: 1.000e-08, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  395059.6321400896\n",
      "best loss: 1154.92\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 5.258e-02\n",
      "Initial loss:  132338.67637997822\n",
      "best loss: 557.71\t\tLearning rate: 1.000e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "Initial loss:  453.3027833364895\n",
      "best loss: 446.35\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 1.052e-01\n",
      "Initial loss:  465.15606400334195\n",
      "best loss: 437.97\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 5.784e-01\n",
      "Initial loss:  462.7215597407529\n",
      "best loss: 428.01\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 9.990e-01\n",
      "Initial loss:  457.284535297058\n",
      "best loss: 447.40\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 1.577e-01\n",
      "Initial loss:  97547.46417064531\n",
      "best loss: 33262.88\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 2.103e-01\n",
      "Initial loss:  302158.02991510846\n",
      "best loss: 16947.87\t\tLearning rate: 3.360e+00, Batch size: 14, Momentum: 5.258e-02\n",
      "Initial loss:  141840.8137190121\n",
      "best loss: 447.23\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 5.258e-01\n",
      "Initial loss:  304182.6815328581\n",
      "best loss: 525.40\t\tLearning rate: 8.859e-08, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  290918.44231283874\n",
      "best loss: 42054.76\t\tLearning rate: 1.000e-08, Batch size: 42, Momentum: 0.000e+00\n",
      "Initial loss:  213490.45058177\n",
      "best loss: 1222.31\t\tLearning rate: 1.129e+00, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  470.4696285798963\n",
      "best loss: 455.06\t\tLearning rate: 4.281e-02, Batch size: 37, Momentum: 5.258e-01\n",
      "Initial loss:  334507.9082716952\n",
      "best loss: 823.32\t\tLearning rate: 6.158e-05, Batch size: 14, Momentum: 9.990e-01\n",
      "Initial loss:  454.27247452115745\n",
      "best loss: 448.95\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.464e-01\n",
      "Initial loss:  459.8007414423235\n",
      "best loss: 448.28\t\tLearning rate: 3.793e-01, Batch size: 34, Momentum: 0.000e+00\n",
      "Initial loss:  449.1581308412051\n",
      "best loss: 446.52\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 5.258e-01\n",
      "Initial loss:  480.2233099915713\n",
      "best loss: 448.21\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 5.258e-01\n",
      "Initial loss:  332584.01988119236\n",
      "best loss: 1316.60\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 1.052e-01\n",
      "Initial loss:  456.47613980026426\n",
      "best loss: 447.59\t\tLearning rate: 3.360e+00, Batch size: 27, Momentum: 8.938e-01\n",
      "Initial loss:  248469.0440934593\n",
      "best loss: 529.74\t\tLearning rate: 2.637e-07, Batch size: 4, Momentum: 0.000e+00\n",
      "Initial loss:  454.5771169325637\n",
      "best loss: 445.37\t\tLearning rate: 1.000e-08, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  115928.61303975845\n",
      "best loss: 451.26\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 9.990e-01\n",
      "Initial loss:  96479.64959112003\n",
      "best loss: 445.00\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 2.103e-01\n",
      "Initial loss:  272607.55126226204\n",
      "best loss: 1150.17\t\tLearning rate: 4.281e-02, Batch size: 32, Momentum: 8.938e-01\n",
      "Initial loss:  430624.0578614391\n",
      "best loss: 452.39\t\tLearning rate: 8.859e-08, Batch size: 42, Momentum: 8.938e-01\n",
      "Initial loss:  131779.7119393675\n",
      "best loss: 802.81\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 4.732e-01\n",
      "Initial loss:  461.90468390964014\n",
      "best loss: 452.65\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 3.155e-01\n",
      "Initial loss:  455.01821342664783\n",
      "best loss: 451.53\t\tLearning rate: 2.069e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "Initial loss:  209805.0662294715\n",
      "best loss: 472.83\t\tLearning rate: 7.848e-07, Batch size: 9, Momentum: 2.103e-01\n",
      "Initial loss:  88272.6472282479\n",
      "best loss: 1094.73\t\tLearning rate: 3.793e-01, Batch size: 4, Momentum: 1.577e-01\n",
      "Initial loss:  456.29272080124326\n",
      "best loss: 447.34\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 9.990e-01\n",
      "Initial loss:  450.32929752974144\n",
      "best loss: 445.00\t\tLearning rate: 7.848e-07, Batch size: 32, Momentum: 7.887e-01\n",
      "Initial loss:  239095.8226997394\n",
      "best loss: 961.51\t\tLearning rate: 1.624e-03, Batch size: 12, Momentum: 7.361e-01\n",
      "Initial loss:  118805.6399209651\n",
      "best loss: 450.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "Initial loss:  438.47741465309014\n",
      "best loss: 427.99\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 4.732e-01\n",
      "Initial loss:  452.19375621496414\n",
      "best loss: 448.34\t\tLearning rate: 1.438e-02, Batch size: 44, Momentum: 2.629e-01\n",
      "Initial loss:  454.8106391701049\n",
      "best loss: 447.19\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "Initial loss:  206115.79595754136\n",
      "best loss: 779.94\t\tLearning rate: 1.833e-04, Batch size: 2, Momentum: 6.309e-01\n"
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 438.59\t\tLearning rate: 6.952e-05, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 438.59\t\tLearning rate: 1.833e+00, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 438.59\t\tLearning rate: 6.158e-02, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 6.158e-02, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 7.361e-01\n",
      "best loss: 443.39\t\tLearning rate: 5.456e-06, Batch size: 4, Momentum: 5.784e-01\n",
      "best loss: 443.39\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 2.103e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.833e+00, Batch size: 32, Momentum: 2.629e-01\n",
      "best loss: 447.74\t\tLearning rate: 1.274e-05, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 450.95\t\tLearning rate: 1.438e-01, Batch size: 47, Momentum: 7.887e-01\n",
      "best loss: 450.95\t\tLearning rate: 3.793e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 451.99\t\tLearning rate: 7.848e-01, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.624e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.336e-06, Batch size: 29, Momentum: 2.103e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 1.438e-01, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 451.99\t\tLearning rate: 5.456e-06, Batch size: 29, Momentum: 9.464e-01\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 6.835e-01\n",
      "best loss: 451.99\t\tLearning rate: 6.158e-02, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 451.99\t\tLearning rate: 2.976e-05, Batch size: 29, Momentum: 4.732e-01\n",
      "best loss: 452.84\t\tLearning rate: 1.129e-02, Batch size: 7, Momentum: 4.206e-01\n",
      "best loss: 459.16\t\tLearning rate: 4.833e-03, Batch size: 12, Momentum: 7.887e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 7.361e-01\n",
      "best loss: 459.16\t\tLearning rate: 3.793e-04, Batch size: 12, Momentum: 1.577e-01\n",
      "best loss: 460.83\t\tLearning rate: 1.129e-02, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 460.83\t\tLearning rate: 3.360e-01, Batch size: 14, Momentum: 9.464e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 3.155e-01\n",
      "best loss: 467.16\t\tLearning rate: 2.336e-06, Batch size: 27, Momentum: 5.258e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.129e-02, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 7.848e-01, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 467.16\t\tLearning rate: 1.274e-05, Batch size: 27, Momentum: 6.309e-01\n",
      "best loss: 467.16\t\tLearning rate: 6.952e-05, Batch size: 27, Momentum: 9.990e-01\n",
      "best loss: 469.16\t\tLearning rate: 2.069e-03, Batch size: 9, Momentum: 0.000e+00\n",
      "best loss: 469.16\t\tLearning rate: 1.000e+01, Batch size: 9, Momentum: 4.206e-01\n",
      "best loss: 469.16\t\tLearning rate: 6.952e-05, Batch size: 9, Momentum: 8.938e-01\n",
      "best loss: 469.16\t\tLearning rate: 7.848e-01, Batch size: 9, Momentum: 8.413e-01\n",
      "best loss: 476.95\t\tLearning rate: 1.129e-02, Batch size: 37, Momentum: 0.000e+00\n",
      "best loss: 476.95\t\tLearning rate: 6.158e-02, Batch size: 37, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 2.069e-03, Batch size: 24, Momentum: 4.732e-01\n",
      "best loss: 493.84\t\tLearning rate: 4.281e+00, Batch size: 24, Momentum: 4.206e-01\n",
      "best loss: 503.24\t\tLearning rate: 8.859e-04, Batch size: 22, Momentum: 2.629e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.952e-05, Batch size: 22, Momentum: 5.258e-02\n",
      "best loss: 503.24\t\tLearning rate: 1.274e-05, Batch size: 22, Momentum: 7.887e-01\n",
      "best loss: 503.24\t\tLearning rate: 6.158e-02, Batch size: 22, Momentum: 8.938e-01\n",
      "best loss: 523.34\t\tLearning rate: 2.976e-05, Batch size: 19, Momentum: 8.413e-01\n",
      "best loss: 523.34\t\tLearning rate: 1.624e-04, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 523.34\t\tLearning rate: 6.952e-05, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 525.32\t\tLearning rate: 1.274e-05, Batch size: 17, Momentum: 3.155e-01\n",
      "best loss: 525.32\t\tLearning rate: 4.281e+00, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 525.32\t\tLearning rate: 1.438e-01, Batch size: 17, Momentum: 9.990e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 1.577e-01\n",
      "best loss: 525.32\t\tLearning rate: 6.952e-05, Batch size: 17, Momentum: 4.206e-01\n",
      "best loss: 542.14\t\tLearning rate: 6.952e-05, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 6.952e-05, Batch size: 42, Momentum: 1.052e-01\n",
      "best loss: 547.94\t\tLearning rate: 3.793e-04, Batch size: 42, Momentum: 7.361e-01\n",
      "best loss: 547.94\t\tLearning rate: 7.848e-01, Batch size: 42, Momentum: 1.577e-01\n",
      "best loss: 547.94\t\tLearning rate: 5.456e-06, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 570.19\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 6.835e-01\n",
      "best loss: 570.19\t\tLearning rate: 7.848e-01, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 1.833e+00, Batch size: 44, Momentum: 2.629e-01\n",
      "best loss: 570.19\t\tLearning rate: 6.952e-05, Batch size: 44, Momentum: 6.309e-01\n",
      "best loss: 570.19\t\tLearning rate: 8.859e-04, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 576.42\t\tLearning rate: 2.336e-06, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.000e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 576.42\t\tLearning rate: 5.456e-06, Batch size: 39, Momentum: 8.413e-01\n",
      "best loss: 576.42\t\tLearning rate: 1.624e-04, Batch size: 39, Momentum: 8.938e-01\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
