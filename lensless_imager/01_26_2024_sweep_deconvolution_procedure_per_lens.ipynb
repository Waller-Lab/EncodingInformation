{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping Wiener Deconvolution, 01/24/2024\n",
    "\n",
    "When you randomly tile, you can make the problem much harder for deconvolution. Info is getting pushed out of the FOV and info is getting pulled into the FOV without knowing where it came from. Cropped convolution ends up being a compressive sensing problem. Instead, doing the reconstruction on the padded FOV including the center 32x32 region with a black border. \n",
    "\n",
    "There is no bias in this system. However, poisson noise is being added at each photon count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import sys\n",
    "sys.path.insert(0, '/home/lkabuli_waller/workspace/EncodingInformation/')\n",
    "sys.path.append('/home/lkabuli_waller/workspace/EncodingInformation/imager_experiments/')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from encoding_information.gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "# from image_distribution_models import PixelCNN\n",
    "\n",
    "from cleanplots import *\n",
    "#import jax.numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "from leyla_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoding_information.image_utils import add_noise, extract_patches\n",
    "from encoding_information.models.gaussian_process import StationaryGaussianProcess\n",
    "from encoding_information.models.pixel_cnn import PixelCNN\n",
    "from encoding_information.information_estimation import estimate_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.restoration import wiener, unsupervised_wiener, richardson_lucy\n",
    "import skimage.metrics as skm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PSFs\n",
    "\n",
    "diffuser_psf = load_diffuser_32()\n",
    "one_psf = load_single_lens_uniform(32)\n",
    "two_psf = load_two_lens_uniform(32)\n",
    "three_psf = load_three_lens_uniform(32)\n",
    "four_psf = load_four_lens_uniform(32)\n",
    "five_psf = load_five_lens_uniform(32)\n",
    "aperture_psf = np.copy(diffuser_psf)\n",
    "aperture_psf[:5] = 0\n",
    "aperture_psf[-5:] = 0\n",
    "aperture_psf[:,:5] = 0\n",
    "aperture_psf[:,-5:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_skm_metrics(gt, recon):\n",
    "    # takes in already normalized gt\n",
    "    mse = skm.mean_squared_error(gt, recon)\n",
    "    psnr = skm.peak_signal_noise_ratio(gt, recon)\n",
    "    nmse = skm.normalized_root_mse(gt, recon)\n",
    "    ssim = skm.structural_similarity(gt, recon, data_range=1)\n",
    "    return mse, psnr, nmse, ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed values for reproducibility\n",
    "seed_values_full = np.arange(1, 4)\n",
    "\n",
    "# set photon properties \n",
    "mean_photon_count_list = [20, 40, 60, 80, 100, 150, 200, 250, 300]\n",
    "\n",
    "# set eligible psfs\n",
    "\n",
    "psf_patterns = [None, one_psf, two_psf, three_psf, four_psf, five_psf, diffuser_psf, aperture_psf]\n",
    "psf_names = ['uc', 'one', 'two', 'three', 'four', 'five', 'diffuser', 'aperture']\n",
    "\n",
    "# MI estimator parameters \n",
    "patch_size = 32\n",
    "num_patches = 10000\n",
    "bs = 500\n",
    "max_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_patterns_use = [one_psf, two_psf, three_psf, four_psf, five_psf, diffuser_psf, aperture_psf]\n",
    "psf_names_use = ['one', 'two', 'three', 'four', 'five', 'diffuser', 'aperture']\n",
    "\n",
    "mean_photon_count_list = [300, 250, 200, 150, 100, 80, 60, 40, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for photon_count in mean_photon_count_list:\n",
    "    for psf_idx, psf_use in enumerate(psf_patterns_use):\n",
    "        print('PSF: {}, Photon Count: {}'.format(psf_names_use[psf_idx], photon_count))\n",
    "        seed_value = 1\n",
    "        # make the data and scale by the photon count \n",
    "        (x_train, y_train), (x_test, y_test) = tfk.datasets.cifar10.load_data()\n",
    "        data = np.concatenate((x_train, x_test), axis=0) # make one big glob of data\n",
    "        data = data.astype(np.float64)\n",
    "        data /= np.mean(data)\n",
    "        data *= photon_count # convert to photons with mean value photon_count\n",
    "        max_val = np.max(data)\n",
    "        labels = np.concatenate((y_train, y_test), axis=0) # make one big glob of labels. \n",
    "        # for CIFAR 100, need to convert images to grayscale\n",
    "        if len(data.shape) == 4:\n",
    "            data = tf.image.rgb_to_grayscale(data).numpy() # convert to grayscale\n",
    "            data = data.squeeze()\n",
    "        # zero pad data to be 96 x 96\n",
    "        data_padded = np.zeros((data.shape[0], 96, 96))\n",
    "        data_padded[:, 32:64, 32:64] = data\n",
    "\n",
    "        convolved_data = convolved_dataset(psf_use, data_padded)\n",
    "        convolved_data_noise = add_noise(convolved_data)\n",
    "        # output of this noisy data is a jax array of float32, correct to regular numpy and float64\n",
    "        convolved_data_noise = np.array(convolved_data_noise).astype(np.float64)\n",
    "\n",
    "        mse_psf = []\n",
    "        psnr_psf = []\n",
    "        for i in range(convolved_data_noise.shape[0]):\n",
    "            recon, _ = unsupervised_wiener(convolved_data_noise[i] / max_val, psf_use)\n",
    "            recon = recon[17:49, 17:49] #this is the crop window to look at\n",
    "            mse = skm.mean_squared_error(data[i] / max_val, recon)\n",
    "            psnr = skm.peak_signal_noise_ratio(data[i] / max_val, recon)\n",
    "            mse_psf.append(mse)\n",
    "            psnr_psf.append(psnr)\n",
    "        print('PSF: {}, Mean MSE: {}, Mean PSNR: {}'.format(psf_names_use[psf_idx], np.mean(mse_psf), np.mean(psnr_psf)))\n",
    "        #np.save('unsupervised_wiener_deconvolution/recon_{}_photon_count_{}_psf.npy'.format(photon_count, psf_names_use[psf_idx]), [mse_psf, psnr_psf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating Wiener Deconvolution including fixed seed=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_patterns_use = [one_psf, two_psf, three_psf, four_psf, five_psf, diffuser_psf, aperture_psf]\n",
    "psf_names_use = ['one', 'two', 'three', 'four', 'five', 'diffuser', 'aperture']\n",
    "\n",
    "mean_photon_count_list = [300, 250, 200, 150, 100, 80, 60, 40, 20]\n",
    "\n",
    "seed_value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for photon_count in mean_photon_count_list:\n",
    "    for psf_idx, psf_use in enumerate(psf_patterns_use):\n",
    "        print('PSF: {}, Photon Count: {}'.format(psf_names_use[psf_idx], photon_count))\n",
    "        # make the data and scale by the photon count \n",
    "        (x_train, y_train), (x_test, y_test) = tfk.datasets.cifar10.load_data()\n",
    "        data = np.concatenate((x_train, x_test), axis=0) # make one big glob of data\n",
    "        data = data.astype(np.float64)\n",
    "        data /= np.mean(data)\n",
    "        data *= photon_count # convert to photons with mean value photon_count\n",
    "        max_val = np.max(data)\n",
    "        labels = np.concatenate((y_train, y_test), axis=0) # make one big glob of labels. \n",
    "        # for CIFAR 100, need to convert images to grayscale\n",
    "        if len(data.shape) == 4:\n",
    "            data = tf.image.rgb_to_grayscale(data).numpy() # convert to grayscale\n",
    "            data = data.squeeze()\n",
    "        # zero pad data to be 96 x 96\n",
    "        data_padded = np.zeros((data.shape[0], 96, 96))\n",
    "        data_padded[:, 32:64, 32:64] = data\n",
    "\n",
    "        convolved_data = convolved_dataset(psf_use, data_padded)\n",
    "        convolved_data_noise = add_noise(convolved_data, seed=seed_value)\n",
    "        # output of this noisy data is a jax array of float32, correct to regular numpy and float64\n",
    "        convolved_data_noise = np.array(convolved_data_noise).astype(np.float64)\n",
    "\n",
    "        mse_psf = []\n",
    "        psnr_psf = []\n",
    "        for i in range(convolved_data_noise.shape[0]):\n",
    "            recon, _ = unsupervised_wiener(convolved_data_noise[i] / max_val, psf_use)\n",
    "            recon = recon[17:49, 17:49] #this is the crop window to look at\n",
    "            mse = skm.mean_squared_error(data[i] / max_val, recon)\n",
    "            psnr = skm.peak_signal_noise_ratio(data[i] / max_val, recon)\n",
    "            mse_psf.append(mse)\n",
    "            psnr_psf.append(psnr)\n",
    "        print('PSF: {}, Mean MSE: {}, Mean PSNR: {}'.format(psf_names_use[psf_idx], np.mean(mse_psf), np.mean(psnr_psf)))\n",
    "        #np.save('unsupervised_wiener_deconvolution_fixed_seed/recon_{}_photon_count_{}_psf.npy'.format(photon_count, psf_names_use[psf_idx]), [mse_psf, psnr_psf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive: Detour to figure out jax types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(convolved_data_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_data = convolved_dataset(psf_use, data_padded)\n",
    "convolved_data_noise = add_noise(convolved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(convolved_data), convolved_data.dtype)\n",
    "print(type(convolved_data_noise), convolved_data_noise.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_data_noise_test = np.array(convolved_data_noise).astype(np.float64)\n",
    "print(type(convolved_data_noise_test))\n",
    "recon, _ = unsupervised_wiener(convolved_data_noise_test[0] / max_val, psf_use) #TODO change to convolved_data_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_data_noise_test = convolved_data_noise.astype(np.float64)\n",
    "recon, _ = unsupervised_wiener(convolved_data_noise_test[0] / max_val, psf_use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_jax_flax_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
