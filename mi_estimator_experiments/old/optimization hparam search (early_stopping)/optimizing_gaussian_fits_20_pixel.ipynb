{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 20\n",
    "num_images = 20000\n",
    "num_patches = 1000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  1602746.422515065\n",
      "best loss: 4707.55\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 4.732e-01\n",
      "Initial loss:  1434216.410025181\n",
      "best loss: 609800.56\t\tLearning rate: 1.000e-08, Batch size: 47, Momentum: 2.103e-01\n",
      "Initial loss:  55755.08502441772\n",
      "best loss: 4035.03\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 5.258e-02\n",
      "Initial loss:  870590.6982012092\n",
      "best loss: 4414.16\t\tLearning rate: 1.438e-02, Batch size: 19, Momentum: 7.361e-01\n",
      "Initial loss:  1859.4591420254694\n",
      "best loss: 1912.88\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 6.835e-01\n",
      "Initial loss:  327827.0813128798\n",
      "best loss: 4163.06\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 9.990e-01\n",
      "Initial loss:  312433.0524452216\n",
      "best loss: 1907.54\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 2.629e-01\n",
      "Initial loss:  1144733.7858392452\n",
      "best loss: 1859.83\t\tLearning rate: 7.848e-07, Batch size: 39, Momentum: 0.000e+00\n",
      "Initial loss:  1438979.6445443\n",
      "best loss: 72693.18\t\tLearning rate: 2.976e-08, Batch size: 9, Momentum: 8.938e-01\n",
      "Initial loss:  611375.4406387607\n",
      "best loss: 1820.06\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.990e-01\n",
      "Initial loss:  1449197.5786492277\n",
      "best loss: 1857.83\t\tLearning rate: 2.336e-06, Batch size: 19, Momentum: 0.000e+00\n",
      "Initial loss:  2218119.6028365768\n",
      "best loss: 6261321.12\t\tLearning rate: 3.360e+00, Batch size: 24, Momentum: 0.000e+00\n",
      "Initial loss:  1832649.5160523946\n",
      "best loss: 3367.77\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 5.784e-01\n",
      "Initial loss:  620021.857989561\n",
      "best loss: 290273.88\t\tLearning rate: 2.976e-08, Batch size: 7, Momentum: 0.000e+00\n",
      "Initial loss:  146355.9257350637\n",
      "best loss: 680340.91\t\tLearning rate: 1.000e+01, Batch size: 12, Momentum: 2.629e-01\n",
      "Initial loss:  987261.1367767054\n",
      "best loss: 4367.96\t\tLearning rate: 1.438e-02, Batch size: 22, Momentum: 8.938e-01\n",
      "Initial loss:  856509.7573663054\n",
      "best loss: 1818.00\t\tLearning rate: 7.848e-07, Batch size: 12, Momentum: 4.732e-01\n",
      "Initial loss:  377607.73294783605\n",
      "best loss: 98621.19\t\tLearning rate: 1.833e-04, Batch size: 4, Momentum: 5.258e-02\n",
      "Initial loss:  1960288.4398943586\n",
      "best loss: 720621.27\t\tLearning rate: 4.833e-03, Batch size: 24, Momentum: 1.577e-01\n",
      "Initial loss:  1906.9687945253481\n",
      "best loss: 1819.19\t\tLearning rate: 1.438e-02, Batch size: 29, Momentum: 7.361e-01\n",
      "Initial loss:  2018918.4335444618\n",
      "best loss: 2018.29\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 7.361e-01\n",
      "Initial loss:  1788.07881083603\n",
      "best loss: 1793.12\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 5.258e-01\n",
      "Initial loss:  817269.5552538887\n",
      "best loss: 2923.50\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.155e-01\n",
      "Initial loss:  1309652.3980799015\n",
      "best loss: 4424.38\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.577e-01\n",
      "Initial loss:  201836.04120273783\n",
      "best loss: 2769.37\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 4.206e-01\n",
      "Initial loss:  2122263.9960439745\n",
      "best loss: 1859.41\t\tLearning rate: 2.336e-06, Batch size: 39, Momentum: 1.577e-01\n",
      "Initial loss:  386910.7298818633\n",
      "best loss: 1793.39\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 8.413e-01\n",
      "Initial loss:  1148778.351386103\n",
      "best loss: 146150.28\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 4.732e-01\n",
      "Initial loss:  711116.3056620166\n",
      "best loss: 4857.01\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 2.629e-01\n",
      "Initial loss:  1332891.6447413529\n",
      "best loss: 5862.15\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 9.464e-01\n",
      "Initial loss:  1864091.9237762934\n",
      "best loss: 4824.08\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 8.413e-01\n",
      "Initial loss:  688944.2448455923\n",
      "best loss: 4810.00\t\tLearning rate: 3.793e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "Initial loss:  1796.5313359944478\n",
      "best loss: 1803.68\t\tLearning rate: 6.952e-06, Batch size: 42, Momentum: 3.681e-01\n",
      "Initial loss:  40735.93721910996\n",
      "best loss: 3382.98\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  1408550.4192804075\n",
      "best loss: 324045.95\t\tLearning rate: 2.976e-08, Batch size: 29, Momentum: 1.052e-01\n",
      "Initial loss:  1143343.279081386\n",
      "best loss: 4817.82\t\tLearning rate: 1.129e+00, Batch size: 9, Momentum: 1.577e-01\n",
      "Initial loss:  849649.8450272154\n",
      "best loss: 414110.27\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 1.577e-01\n",
      "Initial loss:  735316.5992924988\n",
      "best loss: 1809.83\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 7.361e-01\n",
      "Initial loss:  965234.8597013906\n",
      "best loss: 3389.36\t\tLearning rate: 5.456e-04, Batch size: 4, Momentum: 2.103e-01\n",
      "Initial loss:  2041582.381431355\n",
      "best loss: 3253.28\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 4.206e-01\n",
      "Initial loss:  2772719.308923301\n",
      "best loss: 3721.22\t\tLearning rate: 6.158e-05, Batch size: 7, Momentum: 9.990e-01\n",
      "Initial loss:  162594.0593972168\n",
      "best loss: 106840.77\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  383296.7031865497\n",
      "best loss: 4289.41\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.361e-01\n",
      "Initial loss:  516155.4211989663\n",
      "best loss: 3174.96\t\tLearning rate: 1.833e-04, Batch size: 50, Momentum: 4.206e-01\n",
      "Initial loss:  613506.9803243842\n",
      "best loss: 2231.81\t\tLearning rate: 6.952e-06, Batch size: 7, Momentum: 1.577e-01\n",
      "Initial loss:  2208836.635568089\n",
      "best loss: 5204.42\t\tLearning rate: 1.129e+00, Batch size: 29, Momentum: 6.835e-01\n",
      "Initial loss:  644340.4648058491\n",
      "best loss: 1849.07\t\tLearning rate: 2.637e-07, Batch size: 22, Momentum: 4.206e-01\n",
      "Initial loss:  1472141.3239225917\n",
      "best loss: 248684.69\t\tLearning rate: 1.000e-08, Batch size: 22, Momentum: 1.052e-01\n",
      "Initial loss:  1762569.1773303242\n",
      "best loss: 5077.55\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 7.361e-01\n",
      "Initial loss:  772010.8982433599\n",
      "best loss: 738649.91\t\tLearning rate: 1.000e+01, Batch size: 12, Momentum: 1.577e-01\n",
      "Initial loss:  1204145.620485982\n",
      "best loss: 2734.73\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 6.835e-01\n",
      "Initial loss:  884692.8371318242\n",
      "best loss: 2976.89\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 8.413e-01\n",
      "Initial loss:  866785.6157149927\n",
      "best loss: 2733.91\t\tLearning rate: 2.336e-06, Batch size: 42, Momentum: 9.990e-01\n",
      "Initial loss:  552738.9611744594\n",
      "best loss: 1419693.32\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 0.000e+00\n",
      "Initial loss:  146740.74680726757\n",
      "best loss: 4654.81\t\tLearning rate: 1.274e-01, Batch size: 17, Momentum: 6.835e-01\n",
      "Initial loss:  1214675.0007026328\n",
      "best loss: 3917.15\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 4.732e-01\n",
      "Initial loss:  2080010.4771213857\n",
      "best loss: 598597.62\t\tLearning rate: 2.976e-08, Batch size: 22, Momentum: 7.887e-01\n",
      "Initial loss:  1217467.2205321419\n",
      "best loss: 3239.02\t\tLearning rate: 6.158e-05, Batch size: 17, Momentum: 7.887e-01\n",
      "Initial loss:  1343513.3964106115\n",
      "best loss: 5045.72\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 1.577e-01\n",
      "Initial loss:  487874.765509259\n",
      "best loss: 2689.33\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "Initial loss:  386666.6735465007\n",
      "best loss: 3160.22\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 3.155e-01\n",
      "Initial loss:  1788.0717154633658\n",
      "best loss: 1780.07\t\tLearning rate: 3.793e-01, Batch size: 14, Momentum: 5.784e-01\n",
      "Initial loss:  478040.4547437397\n",
      "best loss: 5490.60\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 4.206e-01\n",
      "Initial loss:  806118.3405853286\n",
      "best loss: 3182.16\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  1469506.0959026243\n",
      "best loss: 1951.23\t\tLearning rate: 7.848e-07, Batch size: 22, Momentum: 1.052e-01\n",
      "Initial loss:  1260482.742545558\n",
      "best loss: 229533.32\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 7.361e-01\n",
      "Initial loss:  1162594.3493928513\n",
      "best loss: 4778.90\t\tLearning rate: 3.793e-01, Batch size: 9, Momentum: 3.681e-01\n",
      "Initial loss:  1095132.0978939773\n",
      "best loss: 3582.92\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 4.732e-01\n",
      "Initial loss:  1106996.0869402993\n",
      "best loss: 5130.47\t\tLearning rate: 3.793e-01, Batch size: 37, Momentum: 9.464e-01\n",
      "Initial loss:  1491500.2209607037\n",
      "best loss: 2891.88\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 4.732e-01\n",
      "Initial loss:  1019739.6309869493\n",
      "best loss: 3225.35\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 3.681e-01\n",
      "Initial loss:  2254024.559772869\n",
      "best loss: 3918.84\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 6.309e-01\n",
      "Initial loss:  487950.40472387325\n",
      "best loss: 279680.05\t\tLearning rate: 1.129e+00, Batch size: 7, Momentum: 5.258e-02\n",
      "Initial loss:  856365.645785679\n",
      "best loss: 4308.69\t\tLearning rate: 4.281e-02, Batch size: 22, Momentum: 2.103e-01\n",
      "Initial loss:  640279.6645979885\n",
      "best loss: 5644581.27\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.052e-01\n",
      "Initial loss:  552776.401505832\n",
      "best loss: 4224.18\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 4.206e-01\n",
      "Initial loss:  1791.9354303554567\n",
      "best loss: 1802.56\t\tLearning rate: 3.360e+00, Batch size: 44, Momentum: 8.413e-01\n",
      "Initial loss:  115046.30970960292\n",
      "best loss: 4724.36\t\tLearning rate: 3.793e-01, Batch size: 22, Momentum: 2.103e-01\n",
      "Initial loss:  946479.6169873227\n",
      "best loss: 1833.46\t\tLearning rate: 2.637e-07, Batch size: 9, Momentum: 3.681e-01\n",
      "Initial loss:  782730.3315221036\n",
      "best loss: 5059.09\t\tLearning rate: 3.793e-01, Batch size: 47, Momentum: 8.938e-01\n",
      "Initial loss:  987054.1899195177\n",
      "best loss: 5518.49\t\tLearning rate: 3.360e+00, Batch size: 47, Momentum: 8.938e-01\n",
      "Initial loss:  862720.4123932008\n",
      "best loss: 4940.62\t\tLearning rate: 1.129e+00, Batch size: 14, Momentum: 2.629e-01\n",
      "Initial loss:  1084616.0213375576\n",
      "best loss: 2562.47\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 1.052e-01\n",
      "Initial loss:  535591.9147726502\n",
      "best loss: 270267.92\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 3.681e-01\n",
      "Initial loss:  1075027.044506426\n",
      "best loss: 2895.41\t\tLearning rate: 6.158e-05, Batch size: 44, Momentum: 3.155e-01\n",
      "Initial loss:  236293.85368408504\n",
      "best loss: 4087.04\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 5.784e-01\n",
      "Initial loss:  1704896.5026377419\n",
      "best loss: 264646.64\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 4.732e-01\n",
      "Initial loss:  139978.1725967705\n",
      "best loss: 1799.64\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 9.464e-01\n",
      "Initial loss:  240466.39964112017\n",
      "best loss: 5510.92\t\tLearning rate: 1.000e+01, Batch size: 24, Momentum: 5.784e-01\n",
      "Initial loss:  130219.67605973415\n",
      "best loss: 2985.66\t\tLearning rate: 8.859e-08, Batch size: 24, Momentum: 0.000e+00\n",
      "Initial loss:  569715.2685894468\n",
      "best loss: 2153.03\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 2.103e-01\n",
      "Initial loss:  1145779.205975613\n",
      "best loss: 4790.32\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 4.206e-01\n",
      "Initial loss:  1158786.7423052385\n",
      "best loss: 2944.52\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 9.990e-01\n",
      "Initial loss:  1347215.5515110993\n",
      "best loss: 3923.64\t\tLearning rate: 1.624e-03, Batch size: 19, Momentum: 6.835e-01\n",
      "Initial loss:  757722.854458312\n",
      "best loss: 227219.79\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.784e-01\n",
      "Initial loss:  101606.61950101418\n",
      "best loss: 4767.98\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  1158267.8145607675\n",
      "best loss: 1835.12\t\tLearning rate: 2.336e-06, Batch size: 47, Momentum: 2.103e-01\n",
      "Initial loss:  209232.15901060693\n",
      "best loss: 4713.24\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 3.681e-01\n",
      "Initial loss:  1451085.5340686697\n",
      "best loss: 151568.73\t\tLearning rate: 1.624e-03, Batch size: 34, Momentum: 2.629e-01\n",
      "Initial loss:  1676682.7603238204\n",
      "best loss: 5694.42\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 7.887e-01\n",
      "Initial loss:  845510.8667307866\n",
      "best loss: 61024.50\t\tLearning rate: 7.848e-07, Batch size: 29, Momentum: 2.103e-01\n",
      "Initial loss:  1137199.0486969491\n",
      "best loss: 3909.54\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 5.258e-01\n",
      "Initial loss:  652761.9727233232\n",
      "best loss: 2928.31\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 9.990e-01\n",
      "Initial loss:  1792.8197164046278\n",
      "best loss: 1787.82\t\tLearning rate: 3.793e-01, Batch size: 24, Momentum: 9.464e-01\n",
      "Initial loss:  1075755.4025956045\n",
      "best loss: 5143.53\t\tLearning rate: 1.129e+00, Batch size: 50, Momentum: 5.784e-01\n",
      "Initial loss:  1788.3249233325348\n",
      "Iteration 178, validation loss: 1782.6924052468955\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization hparam search (early_stopping)/optimizing_gaussian_fits_20_pixel.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_20_pixel.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_20_pixel.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m patches \u001b[39m=\u001b[39m extract_patches(images, patch_size, num_patches\u001b[39m=\u001b[39mnum_patches, seed\u001b[39m=\u001b[39mi)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_20_pixel.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_20_pixel.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_20_pixel.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m best_loss\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:479\u001b[0m, in \u001b[0;36mrun_optimization\u001b[0;34m(data, momentum, learning_rate, batch_size, eigenvalue_floor, patience, validation_fraction, max_iters)\u001b[0m\n\u001b[1;32m    474\u001b[0m eigvals, eig_vecs, velocity, train_loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m    475\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor, patch_size)\n\u001b[1;32m    477\u001b[0m train_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m--> 479\u001b[0m validation_loss \u001b[39m=\u001b[39m loss_function(eigvals, eig_vecs, mean_vec, validation_data)   \n\u001b[1;32m    480\u001b[0m validation_loss_history\u001b[39m.\u001b[39mappend(validation_loss)\n\u001b[1;32m    482\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, validation loss: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:409\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(eigvals, eig_vecs, mean_vec, data)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_function\u001b[39m(eigvals, eig_vecs, mean_vec, data):\n\u001b[1;32m    408\u001b[0m     cov_mat \u001b[39m=\u001b[39m eig_vecs \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mdiag(eigvals) \u001b[39m@\u001b[39m eig_vecs\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 409\u001b[0m     ll \u001b[39m=\u001b[39m gaussian_likelihood(cov_mat, mean_vec, data)\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_nll(ll)\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:400\u001b[0m, in \u001b[0;36mgaussian_likelihood\u001b[0;34m(cov_mat, mean_vec, batch)\u001b[0m\n\u001b[1;32m    398\u001b[0m log_likelihoods \u001b[39m=\u001b[39m []\n\u001b[1;32m    399\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m--> 400\u001b[0m     ll \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mscipy\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mmultivariate_normal\u001b[39m.\u001b[39;49mlogpdf(sample\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), mean\u001b[39m=\u001b[39;49mmean_vec, cov\u001b[39m=\u001b[39;49mcov_mat)\n\u001b[1;32m    401\u001b[0m     log_likelihoods\u001b[39m.\u001b[39mappend(ll)\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(log_likelihoods)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/scipy/stats/multivariate_normal.py:46\u001b[0m, in \u001b[0;36mlogpdf\u001b[0;34m(x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmultivariate_normal.logpdf got incompatible shapes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m L \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcholesky(cov)\n\u001b[0;32m---> 46\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mvectorize(\n\u001b[1;32m     47\u001b[0m   partial(lax\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mtriangular_solve, lower\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     48\u001b[0m   signature\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(n,n),(n)->(n)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     49\u001b[0m )(L, x \u001b[39m-\u001b[39;49m mean)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...i,...i->...\u001b[39m\u001b[39m'\u001b[39m, y, y) \u001b[39m-\u001b[39m n\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m jnp\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mpi)\n\u001b[1;32m     51\u001b[0m         \u001b[39m-\u001b[39m jnp\u001b[39m.\u001b[39mlog(L\u001b[39m.\u001b[39mdiagonal(axis1\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis2\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/vectorize.py:309\u001b[0m, in \u001b[0;36mvectorize.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     vectorized_func \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mvmap(vectorized_func, in_axes)\n\u001b[0;32m--> 309\u001b[0m result \u001b[39m=\u001b[39m vectorized_func(\u001b[39m*\u001b[39;49msqueezed_args)\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dims_to_expand:\n\u001b[1;32m    312\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/vectorize.py:136\u001b[0m, in \u001b[0;36m_check_output_dims.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 136\u001b[0m   out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    137\u001b[0m   out_shapes \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(jnp\u001b[39m.\u001b[39mshape, out \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m [out])\n\u001b[1;32m    139\u001b[0m   \u001b[39mif\u001b[39;00m expected_output_core_dims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/lax/linalg.py:103\u001b[0m, in \u001b[0;36m_warn_on_positional_kwargs.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m   warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is now a keyword-only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39margument. Support for passing it positionally will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mremoved in an upcoming JAX release.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m                 \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m   kwargs[name] \u001b[39m=\u001b[39m value\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mpos_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/lax/linalg.py:339\u001b[0m, in \u001b[0;36mtriangular_solve\u001b[0;34m(a, b, left_side, lower, transpose_a, conjugate_a, unit_diagonal)\u001b[0m\n\u001b[1;32m    335\u001b[0m out \u001b[39m=\u001b[39m triangular_solve_p\u001b[39m.\u001b[39mbind(\n\u001b[1;32m    336\u001b[0m     a, b, left_side\u001b[39m=\u001b[39mleft_side, lower\u001b[39m=\u001b[39mlower, transpose_a\u001b[39m=\u001b[39mtranspose_a,\n\u001b[1;32m    337\u001b[0m     conjugate_a\u001b[39m=\u001b[39mconjugate_a, unit_diagonal\u001b[39m=\u001b[39munit_diagonal)\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m singleton:\n\u001b[0;32m--> 339\u001b[0m   out \u001b[39m=\u001b[39m out[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m left_side \u001b[39melse\u001b[39;00m out[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m, :]\n\u001b[1;32m    340\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/array.py:349\u001b[0m, in \u001b[0;36mArrayImpl.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m   \u001b[39mif\u001b[39;00m (dispatch\u001b[39m.\u001b[39mis_single_device_sharding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msharding) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    348\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fully_replicated \u001b[39mor\u001b[39;00m _is_reduced_on_dim(idx)):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m lax_numpy\u001b[39m.\u001b[39;49m_rewriting_take(\u001b[39mself\u001b[39;49m, idx)\n\u001b[1;32m    350\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m api\u001b[39m.\u001b[39mdevice_put(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value[idx])\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3908\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3905\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3907\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39mshape)\n\u001b[0;32m-> 3908\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3909\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3917\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3914\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3915\u001b[0m             unique_indices, mode, fill_value):\n\u001b[1;32m   3916\u001b[0m   idx \u001b[39m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m-> 3917\u001b[0m   indexer \u001b[39m=\u001b[39m _index_to_gather(shape(arr), idx)  \u001b[39m# shared with _scatter_update\u001b[39;00m\n\u001b[1;32m   3918\u001b[0m   y \u001b[39m=\u001b[39m arr\n\u001b[1;32m   3920\u001b[0m   \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4109\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   4106\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   4108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4109\u001b[0m   abstract_i \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mget_aval(i)\n\u001b[1;32m   4110\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   4111\u001b[0m   abstract_i \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:1357\u001b[0m, in \u001b[0;36mget_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1355\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39maval\n\u001b[1;32m   1356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_aval(x)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/core.py:1346\u001b[0m, in \u001b[0;36mconcrete_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39mfor\u001b[39;00m typ \u001b[39min\u001b[39;00m \u001b[39mtype\u001b[39m(x)\u001b[39m.\u001b[39m\u001b[39m__mro__\u001b[39m:\n\u001b[1;32m   1345\u001b[0m   handler \u001b[39m=\u001b[39m pytype_aval_mappings\u001b[39m.\u001b[39mget(typ)\n\u001b[0;32m-> 1346\u001b[0m   \u001b[39mif\u001b[39;00m handler: \u001b[39mreturn\u001b[39;00m handler(x)\n\u001b[1;32m   1347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39m__jax_array__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1348\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_aval(x\u001b[39m.\u001b[39m__jax_array__())\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/abstract_arrays.py:75\u001b[0m, in \u001b[0;36m_make_concrete_python_scalar\u001b[0;34m(t, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_concrete_python_scalar\u001b[39m(t, x):\n\u001b[0;32m---> 75\u001b[0m   dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39;49m_scalar_type_to_dtype(t, x)\n\u001b[1;32m     76\u001b[0m   weak_type \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mis_weakly_typed(x)\n\u001b[1;32m     77\u001b[0m   \u001b[39mreturn\u001b[39;00m canonical_concrete_aval(np\u001b[39m.\u001b[39marray(x, dtype\u001b[39m=\u001b[39mdtype), weak_type\u001b[39m=\u001b[39mweak_type)\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/jax/_src/dtypes.py:182\u001b[0m, in \u001b[0;36m_scalar_type_to_dtype\u001b[0;34m(typ, value)\u001b[0m\n\u001b[1;32m    180\u001b[0m dtype \u001b[39m=\u001b[39m canonicalize_dtype(python_scalar_dtypes[typ])\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mint\u001b[39m \u001b[39mand\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m   \u001b[39mif\u001b[39;00m value \u001b[39m<\u001b[39m np\u001b[39m.\u001b[39miinfo(dtype)\u001b[39m.\u001b[39mmin \u001b[39mor\u001b[39;00m value \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39;49miinfo(dtype)\u001b[39m.\u001b[39mmax:\n\u001b[1;32m    183\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOverflowError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPython int \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m too large to convert to \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m dtype\n",
      "File \u001b[0;32m~/mambaforge/envs/phenotypes/lib/python3.10/site-packages/numpy/core/getlimits.py:650\u001b[0m, in \u001b[0;36miinfo.__init__\u001b[0;34m(self, int_type)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, int_type):\n\u001b[1;32m    649\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m numeric\u001b[39m.\u001b[39;49mdtype(int_type)\n\u001b[1;32m    651\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m numeric\u001b[39m.\u001b[39mdtype(\u001b[39mtype\u001b[39m(int_type))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 1780.07\t\tLearning rate: 3.793e-01, Batch size: 14, Momentum: 5.784e-01\n",
      "best loss: 1787.82\t\tLearning rate: 3.793e-01, Batch size: 24, Momentum: 9.464e-01\n",
      "best loss: 1793.12\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 5.258e-01\n",
      "best loss: 1793.39\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 8.413e-01\n",
      "best loss: 1799.64\t\tLearning rate: 2.336e-06, Batch size: 24, Momentum: 9.464e-01\n",
      "best loss: 1802.56\t\tLearning rate: 3.360e+00, Batch size: 44, Momentum: 8.413e-01\n",
      "best loss: 1803.68\t\tLearning rate: 6.952e-06, Batch size: 42, Momentum: 3.681e-01\n",
      "best loss: 1809.83\t\tLearning rate: 8.859e-08, Batch size: 47, Momentum: 7.361e-01\n",
      "best loss: 1818.00\t\tLearning rate: 7.848e-07, Batch size: 12, Momentum: 4.732e-01\n",
      "best loss: 1819.19\t\tLearning rate: 1.438e-02, Batch size: 29, Momentum: 7.361e-01\n",
      "best loss: 1820.06\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 9.990e-01\n",
      "best loss: 1833.46\t\tLearning rate: 2.637e-07, Batch size: 9, Momentum: 3.681e-01\n",
      "best loss: 1835.12\t\tLearning rate: 2.336e-06, Batch size: 47, Momentum: 2.103e-01\n",
      "best loss: 1849.07\t\tLearning rate: 2.637e-07, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 1857.83\t\tLearning rate: 2.336e-06, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 1859.41\t\tLearning rate: 2.336e-06, Batch size: 39, Momentum: 1.577e-01\n",
      "best loss: 1859.83\t\tLearning rate: 7.848e-07, Batch size: 39, Momentum: 0.000e+00\n",
      "best loss: 1907.54\t\tLearning rate: 2.637e-07, Batch size: 50, Momentum: 2.629e-01\n",
      "best loss: 1912.88\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 6.835e-01\n",
      "best loss: 1951.23\t\tLearning rate: 7.848e-07, Batch size: 22, Momentum: 1.052e-01\n",
      "best loss: 2018.29\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 7.361e-01\n",
      "best loss: 2153.03\t\tLearning rate: 2.637e-07, Batch size: 27, Momentum: 2.103e-01\n",
      "best loss: 2231.81\t\tLearning rate: 6.952e-06, Batch size: 7, Momentum: 1.577e-01\n",
      "best loss: 2562.47\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 1.052e-01\n",
      "best loss: 2689.33\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 1.577e-01\n",
      "best loss: 2733.91\t\tLearning rate: 2.336e-06, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 2734.73\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 6.835e-01\n",
      "best loss: 2769.37\t\tLearning rate: 2.069e-05, Batch size: 12, Momentum: 4.206e-01\n",
      "best loss: 2891.88\t\tLearning rate: 2.069e-05, Batch size: 9, Momentum: 4.732e-01\n",
      "best loss: 2895.41\t\tLearning rate: 6.158e-05, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 2923.50\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.155e-01\n",
      "best loss: 2928.31\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 9.990e-01\n",
      "best loss: 2944.52\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 9.990e-01\n",
      "best loss: 2976.89\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 8.413e-01\n",
      "best loss: 2985.66\t\tLearning rate: 8.859e-08, Batch size: 24, Momentum: 0.000e+00\n",
      "best loss: 3160.22\t\tLearning rate: 1.833e-04, Batch size: 32, Momentum: 3.155e-01\n",
      "best loss: 3174.96\t\tLearning rate: 1.833e-04, Batch size: 50, Momentum: 4.206e-01\n",
      "best loss: 3182.16\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "best loss: 3225.35\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 3.681e-01\n",
      "best loss: 3239.02\t\tLearning rate: 6.158e-05, Batch size: 17, Momentum: 7.887e-01\n",
      "best loss: 3253.28\t\tLearning rate: 1.833e-04, Batch size: 29, Momentum: 4.206e-01\n",
      "best loss: 3367.77\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 5.784e-01\n",
      "best loss: 3382.98\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 3389.36\t\tLearning rate: 5.456e-04, Batch size: 4, Momentum: 2.103e-01\n",
      "best loss: 3582.92\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 4.732e-01\n",
      "best loss: 3721.22\t\tLearning rate: 6.158e-05, Batch size: 7, Momentum: 9.990e-01\n",
      "best loss: 3909.54\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 5.258e-01\n",
      "best loss: 3917.15\t\tLearning rate: 4.833e-03, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 3918.84\t\tLearning rate: 4.833e-03, Batch size: 4, Momentum: 6.309e-01\n",
      "best loss: 3923.64\t\tLearning rate: 1.624e-03, Batch size: 19, Momentum: 6.835e-01\n",
      "best loss: 4035.03\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 5.258e-02\n",
      "best loss: 4087.04\t\tLearning rate: 4.833e-03, Batch size: 47, Momentum: 5.784e-01\n",
      "best loss: 4163.06\t\tLearning rate: 1.624e-03, Batch size: 24, Momentum: 9.990e-01\n",
      "best loss: 4224.18\t\tLearning rate: 4.281e-02, Batch size: 42, Momentum: 4.206e-01\n",
      "best loss: 4289.41\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 4308.69\t\tLearning rate: 4.281e-02, Batch size: 22, Momentum: 2.103e-01\n",
      "best loss: 4367.96\t\tLearning rate: 1.438e-02, Batch size: 22, Momentum: 8.938e-01\n",
      "best loss: 4414.16\t\tLearning rate: 1.438e-02, Batch size: 19, Momentum: 7.361e-01\n",
      "best loss: 4424.38\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.577e-01\n",
      "best loss: 4654.81\t\tLearning rate: 1.274e-01, Batch size: 17, Momentum: 6.835e-01\n",
      "best loss: 4707.55\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 4.732e-01\n",
      "best loss: 4713.24\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 3.681e-01\n",
      "best loss: 4724.36\t\tLearning rate: 3.793e-01, Batch size: 22, Momentum: 2.103e-01\n",
      "best loss: 4767.98\t\tLearning rate: 1.274e-01, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 4778.90\t\tLearning rate: 3.793e-01, Batch size: 9, Momentum: 3.681e-01\n",
      "best loss: 4790.32\t\tLearning rate: 3.793e-01, Batch size: 50, Momentum: 4.206e-01\n",
      "best loss: 4810.00\t\tLearning rate: 3.793e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "best loss: 4817.82\t\tLearning rate: 1.129e+00, Batch size: 9, Momentum: 1.577e-01\n",
      "best loss: 4824.08\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 8.413e-01\n",
      "best loss: 4857.01\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 2.629e-01\n",
      "best loss: 4940.62\t\tLearning rate: 1.129e+00, Batch size: 14, Momentum: 2.629e-01\n",
      "best loss: 5045.72\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 1.577e-01\n",
      "best loss: 5059.09\t\tLearning rate: 3.793e-01, Batch size: 47, Momentum: 8.938e-01\n",
      "best loss: 5077.55\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 7.361e-01\n",
      "best loss: 5130.47\t\tLearning rate: 3.793e-01, Batch size: 37, Momentum: 9.464e-01\n",
      "best loss: 5143.53\t\tLearning rate: 1.129e+00, Batch size: 50, Momentum: 5.784e-01\n",
      "best loss: 5204.42\t\tLearning rate: 1.129e+00, Batch size: 29, Momentum: 6.835e-01\n",
      "best loss: 5490.60\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 4.206e-01\n",
      "best loss: 5510.92\t\tLearning rate: 1.000e+01, Batch size: 24, Momentum: 5.784e-01\n",
      "best loss: 5518.49\t\tLearning rate: 3.360e+00, Batch size: 47, Momentum: 8.938e-01\n",
      "best loss: 5694.42\t\tLearning rate: 1.000e+01, Batch size: 22, Momentum: 7.887e-01\n",
      "best loss: 5862.15\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 9.464e-01\n",
      "best loss: 61024.50\t\tLearning rate: 7.848e-07, Batch size: 29, Momentum: 2.103e-01\n",
      "best loss: 72693.18\t\tLearning rate: 2.976e-08, Batch size: 9, Momentum: 8.938e-01\n",
      "best loss: 98621.19\t\tLearning rate: 1.833e-04, Batch size: 4, Momentum: 5.258e-02\n",
      "best loss: 106840.77\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 146150.28\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 4.732e-01\n",
      "best loss: 151568.73\t\tLearning rate: 1.624e-03, Batch size: 34, Momentum: 2.629e-01\n",
      "best loss: 227219.79\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.784e-01\n",
      "best loss: 229533.32\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 7.361e-01\n",
      "best loss: 248684.69\t\tLearning rate: 1.000e-08, Batch size: 22, Momentum: 1.052e-01\n",
      "best loss: 264646.64\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 4.732e-01\n",
      "best loss: 270267.92\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 3.681e-01\n",
      "best loss: 279680.05\t\tLearning rate: 1.129e+00, Batch size: 7, Momentum: 5.258e-02\n",
      "best loss: 290273.88\t\tLearning rate: 2.976e-08, Batch size: 7, Momentum: 0.000e+00\n",
      "best loss: 324045.95\t\tLearning rate: 2.976e-08, Batch size: 29, Momentum: 1.052e-01\n",
      "best loss: 414110.27\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 1.577e-01\n",
      "best loss: 598597.62\t\tLearning rate: 2.976e-08, Batch size: 22, Momentum: 7.887e-01\n",
      "best loss: 609800.56\t\tLearning rate: 1.000e-08, Batch size: 47, Momentum: 2.103e-01\n",
      "best loss: 680340.91\t\tLearning rate: 1.000e+01, Batch size: 12, Momentum: 2.629e-01\n",
      "best loss: 720621.27\t\tLearning rate: 4.833e-03, Batch size: 24, Momentum: 1.577e-01\n",
      "best loss: 738649.91\t\tLearning rate: 1.000e+01, Batch size: 12, Momentum: 1.577e-01\n",
      "best loss: 1419693.32\t\tLearning rate: 3.360e+00, Batch size: 22, Momentum: 0.000e+00\n",
      "best loss: 5644581.27\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.052e-01\n",
      "best loss: 6261321.12\t\tLearning rate: 3.360e+00, Batch size: 24, Momentum: 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
