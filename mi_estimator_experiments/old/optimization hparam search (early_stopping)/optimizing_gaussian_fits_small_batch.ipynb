{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve for Gaussian approximations using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BSCCM\n",
      "Opened BSCCM\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this only works on startup!\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "from gpu_utils import limit_gpu_memory_growth\n",
    "limit_gpu_memory_growth()\n",
    "\n",
    "from cleanplots import *\n",
    "from tqdm import tqdm\n",
    "from information_estimation import *\n",
    "from image_utils import *\n",
    "from gaussian_process_utils import *\n",
    "\n",
    "from led_array.bsccm_utils import *\n",
    "from bsccm import BSCCM\n",
    "from jax import jit\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "bsccm = BSCCM('/home/hpinkard_waller/data/BSCCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, extract patches, and compute cov mats\n",
    "edge_crop = 32\n",
    "patch_size = 10\n",
    "num_images = 20000\n",
    "num_patches = 1000\n",
    "channel = 'LED119'\n",
    "eigenvalue_floor = 1e0\n",
    "\n",
    "images = load_bsccm_images(bsccm, channel=channel, num_images=num_images, edge_crop=edge_crop, median_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search through hyperparameter combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  766508.7981262293\n",
      "best loss: 839.84\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  334167.50536042696\n",
      "best loss: 454.55\t\tLearning rate: 7.848e-07, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  457.36715387610764\n",
      "best loss: 456.71\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  195647.36018990882\n",
      "best loss: 453.17\t\tLearning rate: 6.952e-06, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  456.4432017029313\n",
      "best loss: 459.04\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 5.784e-01\n",
      "Initial loss:  309247.33653273515\n",
      "best loss: 878.40\t\tLearning rate: 1.833e-04, Batch size: 37, Momentum: 9.464e-01\n",
      "Initial loss:  478.88158725723986\n",
      "best loss: 457.70\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 8.938e-01\n",
      "Initial loss:  171884.67195788733\n",
      "best loss: 1375.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 4.732e-01\n",
      "Initial loss:  879570.5153264167\n",
      "best loss: 62453.70\t\tLearning rate: 5.456e-04, Batch size: 19, Momentum: 5.258e-02\n",
      "Initial loss:  489808.8157865608\n",
      "best loss: 1366.30\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 4.206e-01\n",
      "Initial loss:  270962.7236863002\n",
      "best loss: 457.18\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 7.887e-01\n",
      "Initial loss:  109215.81369650288\n",
      "best loss: 735.55\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 3.681e-01\n",
      "Initial loss:  998534.0051830853\n",
      "best loss: 1252.87\t\tLearning rate: 3.793e-01, Batch size: 17, Momentum: 8.938e-01\n",
      "Initial loss:  458.3793896396579\n",
      "best loss: 455.56\t\tLearning rate: 3.793e-01, Batch size: 12, Momentum: 8.938e-01\n",
      "Initial loss:  460759.08203143615\n",
      "best loss: 811.54\t\tLearning rate: 6.158e-05, Batch size: 32, Momentum: 8.413e-01\n",
      "Initial loss:  751634.5466985485\n",
      "best loss: 458.89\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.732e-01\n",
      "Initial loss:  529596.5476599077\n",
      "best loss: 1954.86\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 0.000e+00\n",
      "Initial loss:  306959.2640549124\n",
      "best loss: 1096.44\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.052e-01\n",
      "Initial loss:  781257.914438645\n",
      "best loss: 946.80\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.732e-01\n",
      "Initial loss:  460.8643399647622\n",
      "best loss: 456.84\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 2.103e-01\n",
      "Initial loss:  524768.938934717\n",
      "best loss: 1064.95\t\tLearning rate: 4.281e-02, Batch size: 50, Momentum: 3.155e-01\n",
      "Initial loss:  656513.6425124486\n",
      "best loss: 890.85\t\tLearning rate: 1.624e-03, Batch size: 2, Momentum: 1.052e-01\n",
      "Initial loss:  199774.3356812727\n",
      "best loss: 833.85\t\tLearning rate: 1.833e-04, Batch size: 39, Momentum: 7.887e-01\n",
      "Initial loss:  351134.15001977707\n",
      "best loss: 982.72\t\tLearning rate: 4.833e-03, Batch size: 50, Momentum: 5.258e-01\n",
      "Initial loss:  476.5447281134855\n",
      "best loss: 453.93\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 5.258e-02\n",
      "Initial loss:  330434.7833425578\n",
      "best loss: 778.64\t\tLearning rate: 8.859e-08, Batch size: 34, Momentum: 1.577e-01\n",
      "Initial loss:  322529.82073442906\n",
      "best loss: 1372.19\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 3.155e-01\n",
      "Initial loss:  598165.0040045356\n",
      "best loss: 1084.09\t\tLearning rate: 1.438e-02, Batch size: 7, Momentum: 6.309e-01\n",
      "Initial loss:  453.68055559199405\n",
      "best loss: 458.10\t\tLearning rate: 6.158e-05, Batch size: 19, Momentum: 6.835e-01\n",
      "Initial loss:  71298.70415157588\n",
      "best loss: 939.92\t\tLearning rate: 1.624e-03, Batch size: 29, Momentum: 6.309e-01\n",
      "Initial loss:  455.45837545037455\n",
      "best loss: 459.63\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.681e-01\n",
      "Initial loss:  109527.7707640242\n",
      "best loss: 962.92\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 2.103e-01\n",
      "Initial loss:  262840.19386535475\n",
      "best loss: 1212.67\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 0.000e+00\n",
      "Initial loss:  451.0863697972616\n",
      "best loss: 455.25\t\tLearning rate: 6.952e-06, Batch size: 27, Momentum: 8.938e-01\n",
      "Initial loss:  200748.27567362422\n",
      "best loss: 910.48\t\tLearning rate: 4.833e-03, Batch size: 27, Momentum: 0.000e+00\n",
      "Initial loss:  459.33812898048313\n",
      "best loss: 453.89\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 4.732e-01\n",
      "Initial loss:  456.20183316739275\n",
      "best loss: 455.89\t\tLearning rate: 2.976e-08, Batch size: 50, Momentum: 5.258e-02\n",
      "Initial loss:  1054134.8640697\n",
      "best loss: 1260.40\t\tLearning rate: 1.129e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  165579.99871976636\n",
      "best loss: 918301.82\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 0.000e+00\n",
      "Initial loss:  150830.67803456358\n",
      "best loss: 889.36\t\tLearning rate: 5.456e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "Initial loss:  473.0661640114175\n",
      "best loss: 455.05\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 3.681e-01\n",
      "Initial loss:  233214.64948748812\n",
      "best loss: 140824.29\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 2.629e-01\n",
      "Initial loss:  153302.3558206473\n",
      "best loss: 472.46\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 8.413e-01\n",
      "Initial loss:  134880.9709758117\n",
      "best loss: 1122.54\t\tLearning rate: 4.281e-02, Batch size: 34, Momentum: 8.938e-01\n",
      "Initial loss:  717954.2886193127\n",
      "best loss: 378389.66\t\tLearning rate: 2.976e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "Initial loss:  454.91177845211286\n",
      "best loss: 458.56\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 0.000e+00\n",
      "Initial loss:  715611.6636568361\n",
      "best loss: 891.83\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 7.361e-01\n",
      "Initial loss:  577054.9581760725\n",
      "best loss: 456.10\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 3.155e-01\n",
      "Initial loss:  249362.29876452516\n",
      "best loss: 458.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "Initial loss:  233615.92796772125\n",
      "best loss: 1239.29\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 2.629e-01\n",
      "Initial loss:  1011532.8507023784\n",
      "best loss: 1114.06\t\tLearning rate: 1.274e-01, Batch size: 14, Momentum: 4.206e-01\n",
      "Initial loss:  450.335400203548\n",
      "best loss: 454.40\t\tLearning rate: 6.952e-06, Batch size: 44, Momentum: 8.938e-01\n",
      "Initial loss:  200173.99712339853\n",
      "best loss: 460.70\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 4.206e-01\n",
      "Initial loss:  407447.11941291863\n",
      "best loss: 457.11\t\tLearning rate: 2.336e-06, Batch size: 34, Momentum: 8.413e-01\n",
      "Initial loss:  507752.9168980514\n",
      "best loss: 751.01\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 1.577e-01\n",
      "Initial loss:  604282.3170431704\n",
      "best loss: 1284.99\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 7.361e-01\n",
      "Initial loss:  658047.3592272899\n",
      "best loss: 1311.95\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.577e-01\n",
      "Initial loss:  445.8768681756431\n",
      "best loss: 453.79\t\tLearning rate: 3.360e+00, Batch size: 7, Momentum: 9.990e-01\n",
      "Initial loss:  456.33021597979257\n",
      "best loss: 460.49\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 9.464e-01\n",
      "Initial loss:  775137.3471356884\n",
      "best loss: 741.41\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 6.835e-01\n",
      "Initial loss:  551297.2361027456\n",
      "best loss: 1251.28\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 4.206e-01\n",
      "Initial loss:  453.21573748080533\n",
      "best loss: 455.91\t\tLearning rate: 6.952e-06, Batch size: 47, Momentum: 7.887e-01\n",
      "Initial loss:  183429.7872074833\n",
      "best loss: 923.38\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "Initial loss:  303790.93247658754\n",
      "best loss: 865.42\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 5.258e-01\n",
      "Initial loss:  913727.8768891019\n",
      "best loss: 760.75\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 7.361e-01\n",
      "Initial loss:  499973.533756553\n",
      "best loss: 811.67\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 5.784e-01\n",
      "Initial loss:  618202.7112624963\n",
      "best loss: 1006.78\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 7.887e-01\n",
      "Initial loss:  132240.82678742756\n",
      "best loss: 585.56\t\tLearning rate: 2.336e-06, Batch size: 50, Momentum: 1.052e-01\n",
      "Initial loss:  451.9383642030355\n",
      "best loss: 458.67\t\tLearning rate: 1.274e-01, Batch size: 12, Momentum: 0.000e+00\n",
      "Initial loss:  238150.08273100143\n",
      "best loss: 469.51\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 9.990e-01\n",
      "Initial loss:  147094.39687724382\n",
      "best loss: 1099.82\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "Initial loss:  459.3812435476472\n",
      "best loss: 456.92\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 7.887e-01\n",
      "Initial loss:  206719.44834760414\n",
      "best loss: 148705.91\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "Initial loss:  624236.3484468884\n",
      "best loss: 755.21\t\tLearning rate: 1.833e-04, Batch size: 7, Momentum: 1.052e-01\n",
      "Initial loss:  342612.9263641973\n",
      "best loss: 550.78\t\tLearning rate: 8.859e-08, Batch size: 12, Momentum: 0.000e+00\n",
      "Initial loss:  606330.1013553552\n",
      "best loss: 454.76\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 3.681e-01\n",
      "Initial loss:  253443.3076942564\n",
      "best loss: 1184.25\t\tLearning rate: 3.793e-01, Batch size: 44, Momentum: 3.155e-01\n",
      "Initial loss:  165971.65709921744\n",
      "best loss: 823.28\t\tLearning rate: 5.456e-04, Batch size: 27, Momentum: 6.835e-01\n",
      "Initial loss:  589451.4606699839\n",
      "best loss: 1073.13\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 8.413e-01\n",
      "Initial loss:  110974.67564624603\n",
      "best loss: 466.02\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 6.309e-01\n",
      "Initial loss:  439.6360851530675\n",
      "best loss: 452.22\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "Initial loss:  271884.97174820513\n",
      "best loss: 496.50\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 8.413e-01\n",
      "Initial loss:  502335.072263919\n",
      "best loss: 1390.07\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 5.258e-01\n",
      "Initial loss:  159853.13195760222\n",
      "best loss: 1139.98\t\tLearning rate: 1.274e-01, Batch size: 37, Momentum: 7.361e-01\n",
      "Initial loss:  406291.75755361066\n",
      "best loss: 1220.35\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 7.361e-01\n",
      "Initial loss:  359268.0723294158\n",
      "best loss: 458.88\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 7.361e-01\n",
      "Initial loss:  427590.07544666255\n",
      "best loss: 634.73\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "Initial loss:  775148.7494432735\n",
      "best loss: 200629.95\t\tLearning rate: 2.336e-06, Batch size: 32, Momentum: 2.629e-01\n",
      "Initial loss:  338238.2926893854\n",
      "best loss: 457.70\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 6.309e-01\n",
      "Initial loss:  475.5383934182956\n",
      "best loss: 466.06\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  664440.1558555355\n",
      "best loss: 474.10\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 3.155e-01\n",
      "Initial loss:  359335.3331825144\n",
      "best loss: 80723.26\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  184928.91039282657\n",
      "best loss: 1240.29\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  360360.6282020771\n",
      "best loss: 794.92\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "Initial loss:  444.1631462342258\n",
      "best loss: 454.38\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "Initial loss:  113018.7184148015\n",
      "best loss: 1323.73\t\tLearning rate: 1.000e+01, Batch size: 7, Momentum: 1.577e-01\n",
      "Initial loss:  537106.3258307154\n",
      "best loss: 1001.29\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 3.681e-01\n",
      "Initial loss:  461.65996415101097\n",
      "best loss: 458.61\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.887e-01\n",
      "Initial loss:  357070.7608392465\n",
      "best loss: 489.08\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 6.835e-01\n",
      "Initial loss:  122653.17178158904\n",
      "best loss: 1134.74\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "Initial loss:  97518.64317951213\n",
      "best loss: 909.19\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 6.835e-01\n",
      "Initial loss:  940614.1115180098\n",
      "best loss: 531.07\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 5.258e-01\n",
      "Initial loss:  457.7907263938722\n",
      "best loss: 455.35\t\tLearning rate: 6.158e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  530315.9590537335\n",
      "best loss: 458.26\t\tLearning rate: 2.637e-07, Batch size: 42, Momentum: 9.990e-01\n",
      "Initial loss:  511445.6136799645\n",
      "best loss: 808.12\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 4.732e-01\n",
      "Initial loss:  400136.87385695876\n",
      "best loss: 533.64\t\tLearning rate: 8.859e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "Initial loss:  257638.6798805951\n",
      "best loss: 458.73\t\tLearning rate: 6.952e-06, Batch size: 37, Momentum: 8.413e-01\n",
      "Initial loss:  328919.55453646864\n",
      "best loss: 903.26\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 1.577e-01\n",
      "Initial loss:  136777.16839712014\n",
      "best loss: 474.54\t\tLearning rate: 6.952e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "Initial loss:  791082.5552199677\n",
      "best loss: 1227.76\t\tLearning rate: 1.274e-01, Batch size: 22, Momentum: 9.464e-01\n",
      "Initial loss:  312636.0386217521\n",
      "best loss: 1399.63\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 9.990e-01\n",
      "Initial loss:  242079.8500934119\n",
      "best loss: 459.57\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 3.681e-01\n",
      "Initial loss:  687655.2550459298\n",
      "best loss: 275045.01\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "Initial loss:  536313.6625812388\n",
      "best loss: 1261.67\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 5.784e-01\n",
      "Initial loss:  429561.0488703235\n",
      "best loss: 849.66\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 8.413e-01\n",
      "Initial loss:  113895.10854975415\n",
      "best loss: 1148.44\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.835e-01\n",
      "Initial loss:  326795.2057104381\n",
      "best loss: 919.78\t\tLearning rate: 1.624e-03, Batch size: 27, Momentum: 5.784e-01\n",
      "Initial loss:  1091202.7650655804\n",
      "best loss: 1043.21\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 7.361e-01\n",
      "Initial loss:  452.34405826273917\n",
      "best loss: 456.57\t\tLearning rate: 1.274e-01, Batch size: 29, Momentum: 1.577e-01\n",
      "Initial loss:  454.2759210366577\n",
      "best loss: 453.41\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 8.938e-01\n",
      "Initial loss:  182615.4997567394\n",
      "best loss: 973.37\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 5.258e-02\n",
      "Initial loss:  326981.3684362954\n",
      "best loss: 1055.19\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 9.464e-01\n",
      "Initial loss:  176331.40014664413\n",
      "best loss: 1194.39\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 9.464e-01\n",
      "Initial loss:  66962.33763307445\n",
      "best loss: 573.19\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 3.681e-01\n",
      "Initial loss:  455.2891485598845\n",
      "best loss: 453.52\t\tLearning rate: 2.637e-07, Batch size: 19, Momentum: 5.784e-01\n",
      "Initial loss:  688178.5823927869\n",
      "best loss: 457.82\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 4.206e-01\n",
      "Initial loss:  520224.1794196097\n",
      "best loss: 869.75\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 1.052e-01\n",
      "Initial loss:  595060.234742002\n",
      "best loss: 1357.28\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 6.309e-01\n",
      "Initial loss:  858440.3710703375\n",
      "best loss: 115471.02\t\tLearning rate: 8.859e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "Initial loss:  180975.9086792209\n",
      "best loss: 824.94\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "Initial loss:  262112.54411849467\n",
      "best loss: 1382.23\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "Initial loss:  480902.31911059335\n",
      "best loss: 456.64\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 5.258e-01\n",
      "Initial loss:  912704.6475183131\n",
      "best loss: 973.98\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 9.990e-01\n",
      "Initial loss:  646378.5415872976\n",
      "best loss: 647.87\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 8.413e-01\n",
      "Initial loss:  370265.1939739369\n",
      "best loss: 1121.28\t\tLearning rate: 1.274e-01, Batch size: 47, Momentum: 2.103e-01\n",
      "Initial loss:  166088.3577974834\n",
      "best loss: 1126.22\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 1.052e-01\n",
      "Initial loss:  457.3013186025904\n",
      "best loss: 456.53\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 0.000e+00\n",
      "Initial loss:  579267.4917419511\n",
      "Iteration 241, validation loss: 475.08313893931313\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization hparam search (early_stopping)/optimizing_gaussian_fits_small_batch.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_small_batch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m best_hp_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_small_batch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m patches \u001b[39m=\u001b[39m extract_patches(images, patch_size, num_patches\u001b[39m=\u001b[39mnum_patches, seed\u001b[39m=\u001b[39mi)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_small_batch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history \u001b[39m=\u001b[39m run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_small_batch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m best_loss \u001b[39m<\u001b[39m best_hp_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwaller-fuoco.eecs.berkeley.edu/home/hpinkard_waller/GitRepos/EncodingInformation/modeling_and_mi_estimation/optimization%20hparam%20search%20%28early_stopping%29/optimizing_gaussian_fits_small_batch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     best_hp_loss \u001b[39m=\u001b[39m best_loss\n",
      "File \u001b[0;32m~/GitRepos/EncodingInformation/gaussian_process_utils.py:474\u001b[0m, in \u001b[0;36mrun_optimization\u001b[0;34m(data, momentum, learning_rate, batch_size, eigenvalue_floor, patience, validation_fraction, max_iters)\u001b[0m\n\u001b[1;32m    471\u001b[0m key, subkey \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m    472\u001b[0m batch \u001b[39m=\u001b[39m train_data[batch_indices]\n\u001b[0;32m--> 474\u001b[0m eigvals, eig_vecs, velocity, train_loss \u001b[39m=\u001b[39m optmization_step(eigvals, eig_vecs, velocity, \n\u001b[1;32m    475\u001b[0m                                                      batch, mean_vec, momentum, learning_rate, eigenvalue_floor, patch_size)\n\u001b[1;32m    477\u001b[0m train_loss_history\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m    479\u001b[0m validation_loss \u001b[39m=\u001b[39m loss_function(eigvals, eig_vecs, mean_vec, validation_data)   \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(1, -8, 20)\n",
    "batch_sizes = np.linspace(2, 50, 20).astype(int)\n",
    "momentums = np.linspace(0, 0.999, 20)\n",
    "\n",
    "# generate tuples of random hyperparameters\n",
    "hyperparameter_tuples = []\n",
    "for i in range(10000):\n",
    "    lr = onp.random.choice(learning_rates)\n",
    "    bs = onp.random.choice(batch_sizes)\n",
    "    m = onp.random.choice(momentums)\n",
    "    hyperparameter_tuples.append((lr, bs, m))\n",
    "\n",
    "results = {}\n",
    "for i, (learning_rate, batch_size, momentum) in enumerate(hyperparameter_tuples):\n",
    "    best_hp_loss = np.inf\n",
    "\n",
    "    patches = extract_patches(images, patch_size, num_patches=num_patches, seed=i)\n",
    "    best_cov_mat, cov_mat_initial, mean_vec, best_loss, train_loss_history, val_loss_history = run_optimization(patches, momentum, learning_rate, batch_size, eigenvalue_floor=1e-3)\n",
    "\n",
    "    if best_loss < best_hp_loss:\n",
    "        best_hp_loss = best_loss\n",
    "        best_hp = (learning_rate, batch_size, momentum)\n",
    "        \n",
    "    # collect results\n",
    "    results[(learning_rate, batch_size, momentum)] = best_loss\n",
    "\n",
    "    # print hyperparameters and their best loss\n",
    "    print(f\"best loss: {best_loss:.2f}\\t\\tLearning rate: {learning_rate:.3e}, Batch size: {batch_size}, Momentum: {momentum:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 452.22\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 453.17\t\tLearning rate: 6.952e-06, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 453.41\t\tLearning rate: 1.000e+01, Batch size: 37, Momentum: 8.938e-01\n",
      "best loss: 453.52\t\tLearning rate: 2.637e-07, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 453.79\t\tLearning rate: 3.360e+00, Batch size: 7, Momentum: 9.990e-01\n",
      "best loss: 453.89\t\tLearning rate: 2.069e-05, Batch size: 4, Momentum: 4.732e-01\n",
      "best loss: 453.93\t\tLearning rate: 2.336e-06, Batch size: 2, Momentum: 5.258e-02\n",
      "best loss: 454.38\t\tLearning rate: 2.976e-08, Batch size: 2, Momentum: 3.681e-01\n",
      "best loss: 454.40\t\tLearning rate: 6.952e-06, Batch size: 44, Momentum: 8.938e-01\n",
      "best loss: 454.55\t\tLearning rate: 7.848e-07, Batch size: 2, Momentum: 8.413e-01\n",
      "best loss: 454.76\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 3.681e-01\n",
      "best loss: 455.05\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 3.681e-01\n",
      "best loss: 455.25\t\tLearning rate: 6.952e-06, Batch size: 27, Momentum: 8.938e-01\n",
      "best loss: 455.35\t\tLearning rate: 6.158e-05, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 455.56\t\tLearning rate: 3.793e-01, Batch size: 12, Momentum: 8.938e-01\n",
      "best loss: 455.89\t\tLearning rate: 2.976e-08, Batch size: 50, Momentum: 5.258e-02\n",
      "best loss: 455.91\t\tLearning rate: 6.952e-06, Batch size: 47, Momentum: 7.887e-01\n",
      "best loss: 456.10\t\tLearning rate: 6.952e-06, Batch size: 29, Momentum: 3.155e-01\n",
      "best loss: 456.53\t\tLearning rate: 2.069e-05, Batch size: 2, Momentum: 0.000e+00\n",
      "best loss: 456.57\t\tLearning rate: 1.274e-01, Batch size: 29, Momentum: 1.577e-01\n",
      "best loss: 456.64\t\tLearning rate: 2.336e-06, Batch size: 44, Momentum: 5.258e-01\n",
      "best loss: 456.71\t\tLearning rate: 3.360e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 456.84\t\tLearning rate: 3.793e-01, Batch size: 19, Momentum: 2.103e-01\n",
      "best loss: 456.92\t\tLearning rate: 4.833e-03, Batch size: 14, Momentum: 7.887e-01\n",
      "best loss: 457.11\t\tLearning rate: 2.336e-06, Batch size: 34, Momentum: 8.413e-01\n",
      "best loss: 457.18\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 7.887e-01\n",
      "best loss: 457.70\t\tLearning rate: 1.274e-01, Batch size: 24, Momentum: 8.938e-01\n",
      "best loss: 457.70\t\tLearning rate: 2.069e-05, Batch size: 32, Momentum: 6.309e-01\n",
      "best loss: 457.82\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 4.206e-01\n",
      "best loss: 458.10\t\tLearning rate: 6.158e-05, Batch size: 19, Momentum: 6.835e-01\n",
      "best loss: 458.26\t\tLearning rate: 2.637e-07, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 458.37\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 4.732e-01\n",
      "best loss: 458.56\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 0.000e+00\n",
      "best loss: 458.61\t\tLearning rate: 1.438e-02, Batch size: 50, Momentum: 7.887e-01\n",
      "best loss: 458.67\t\tLearning rate: 1.274e-01, Batch size: 12, Momentum: 0.000e+00\n",
      "best loss: 458.73\t\tLearning rate: 6.952e-06, Batch size: 37, Momentum: 8.413e-01\n",
      "best loss: 458.88\t\tLearning rate: 7.848e-07, Batch size: 19, Momentum: 7.361e-01\n",
      "best loss: 458.89\t\tLearning rate: 6.952e-06, Batch size: 50, Momentum: 4.732e-01\n",
      "best loss: 459.04\t\tLearning rate: 4.281e-02, Batch size: 47, Momentum: 5.784e-01\n",
      "best loss: 459.57\t\tLearning rate: 2.336e-06, Batch size: 22, Momentum: 3.681e-01\n",
      "best loss: 459.63\t\tLearning rate: 6.158e-05, Batch size: 12, Momentum: 3.681e-01\n",
      "best loss: 460.49\t\tLearning rate: 2.637e-07, Batch size: 2, Momentum: 9.464e-01\n",
      "best loss: 460.70\t\tLearning rate: 7.848e-07, Batch size: 34, Momentum: 4.206e-01\n",
      "best loss: 466.02\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 6.309e-01\n",
      "best loss: 466.06\t\tLearning rate: 4.833e-03, Batch size: 19, Momentum: 5.784e-01\n",
      "best loss: 469.51\t\tLearning rate: 1.000e-08, Batch size: 9, Momentum: 9.990e-01\n",
      "best loss: 472.46\t\tLearning rate: 8.859e-08, Batch size: 27, Momentum: 8.413e-01\n",
      "best loss: 474.10\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 474.54\t\tLearning rate: 6.952e-06, Batch size: 24, Momentum: 2.103e-01\n",
      "best loss: 489.08\t\tLearning rate: 2.637e-07, Batch size: 17, Momentum: 6.835e-01\n",
      "best loss: 496.50\t\tLearning rate: 1.000e-08, Batch size: 24, Momentum: 8.413e-01\n",
      "best loss: 531.07\t\tLearning rate: 2.069e-05, Batch size: 47, Momentum: 5.258e-01\n",
      "best loss: 533.64\t\tLearning rate: 8.859e-08, Batch size: 7, Momentum: 2.103e-01\n",
      "best loss: 550.78\t\tLearning rate: 8.859e-08, Batch size: 12, Momentum: 0.000e+00\n",
      "best loss: 573.19\t\tLearning rate: 2.976e-08, Batch size: 37, Momentum: 3.681e-01\n",
      "best loss: 585.56\t\tLearning rate: 2.336e-06, Batch size: 50, Momentum: 1.052e-01\n",
      "best loss: 634.73\t\tLearning rate: 6.952e-06, Batch size: 39, Momentum: 9.464e-01\n",
      "best loss: 647.87\t\tLearning rate: 2.069e-05, Batch size: 34, Momentum: 8.413e-01\n",
      "best loss: 735.55\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 3.681e-01\n",
      "best loss: 741.41\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 6.835e-01\n",
      "best loss: 751.01\t\tLearning rate: 1.833e-04, Batch size: 34, Momentum: 1.577e-01\n",
      "best loss: 755.21\t\tLearning rate: 1.833e-04, Batch size: 7, Momentum: 1.052e-01\n",
      "best loss: 760.75\t\tLearning rate: 2.069e-05, Batch size: 39, Momentum: 7.361e-01\n",
      "best loss: 778.64\t\tLearning rate: 8.859e-08, Batch size: 34, Momentum: 1.577e-01\n",
      "best loss: 794.92\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 5.258e-02\n",
      "best loss: 808.12\t\tLearning rate: 1.833e-04, Batch size: 9, Momentum: 4.732e-01\n",
      "best loss: 811.54\t\tLearning rate: 6.158e-05, Batch size: 32, Momentum: 8.413e-01\n",
      "best loss: 811.67\t\tLearning rate: 1.000e-08, Batch size: 50, Momentum: 5.784e-01\n",
      "best loss: 823.28\t\tLearning rate: 5.456e-04, Batch size: 27, Momentum: 6.835e-01\n",
      "best loss: 824.94\t\tLearning rate: 5.456e-04, Batch size: 29, Momentum: 2.629e-01\n",
      "best loss: 833.85\t\tLearning rate: 1.833e-04, Batch size: 39, Momentum: 7.887e-01\n",
      "best loss: 849.66\t\tLearning rate: 1.833e-04, Batch size: 22, Momentum: 8.413e-01\n",
      "best loss: 865.42\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 5.258e-01\n",
      "best loss: 869.75\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 1.052e-01\n",
      "best loss: 878.40\t\tLearning rate: 1.833e-04, Batch size: 37, Momentum: 9.464e-01\n",
      "best loss: 889.36\t\tLearning rate: 5.456e-04, Batch size: 47, Momentum: 3.155e-01\n",
      "best loss: 890.85\t\tLearning rate: 1.624e-03, Batch size: 2, Momentum: 1.052e-01\n",
      "best loss: 891.83\t\tLearning rate: 1.000e-08, Batch size: 44, Momentum: 7.361e-01\n",
      "best loss: 903.26\t\tLearning rate: 1.624e-03, Batch size: 7, Momentum: 1.577e-01\n",
      "best loss: 909.19\t\tLearning rate: 5.456e-04, Batch size: 24, Momentum: 6.835e-01\n",
      "best loss: 910.48\t\tLearning rate: 4.833e-03, Batch size: 27, Momentum: 0.000e+00\n",
      "best loss: 919.78\t\tLearning rate: 1.624e-03, Batch size: 27, Momentum: 5.784e-01\n",
      "best loss: 923.38\t\tLearning rate: 5.456e-04, Batch size: 2, Momentum: 6.309e-01\n",
      "best loss: 939.92\t\tLearning rate: 1.624e-03, Batch size: 29, Momentum: 6.309e-01\n",
      "best loss: 946.80\t\tLearning rate: 4.833e-03, Batch size: 22, Momentum: 4.732e-01\n",
      "best loss: 962.92\t\tLearning rate: 4.833e-03, Batch size: 34, Momentum: 2.103e-01\n",
      "best loss: 973.37\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 5.258e-02\n",
      "best loss: 973.98\t\tLearning rate: 1.624e-03, Batch size: 42, Momentum: 9.990e-01\n",
      "best loss: 982.72\t\tLearning rate: 4.833e-03, Batch size: 50, Momentum: 5.258e-01\n",
      "best loss: 1001.29\t\tLearning rate: 1.438e-02, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 1006.78\t\tLearning rate: 4.833e-03, Batch size: 9, Momentum: 7.887e-01\n",
      "best loss: 1043.21\t\tLearning rate: 1.438e-02, Batch size: 2, Momentum: 7.361e-01\n",
      "best loss: 1055.19\t\tLearning rate: 1.438e-02, Batch size: 12, Momentum: 9.464e-01\n",
      "best loss: 1064.95\t\tLearning rate: 4.281e-02, Batch size: 50, Momentum: 3.155e-01\n",
      "best loss: 1073.13\t\tLearning rate: 1.438e-02, Batch size: 17, Momentum: 8.413e-01\n",
      "best loss: 1084.09\t\tLearning rate: 1.438e-02, Batch size: 7, Momentum: 6.309e-01\n",
      "best loss: 1096.44\t\tLearning rate: 1.274e-01, Batch size: 39, Momentum: 1.052e-01\n",
      "best loss: 1099.82\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 7.887e-01\n",
      "best loss: 1114.06\t\tLearning rate: 1.274e-01, Batch size: 14, Momentum: 4.206e-01\n",
      "best loss: 1121.28\t\tLearning rate: 1.274e-01, Batch size: 47, Momentum: 2.103e-01\n",
      "best loss: 1122.54\t\tLearning rate: 4.281e-02, Batch size: 34, Momentum: 8.938e-01\n",
      "best loss: 1126.22\t\tLearning rate: 3.793e-01, Batch size: 32, Momentum: 1.052e-01\n",
      "best loss: 1134.74\t\tLearning rate: 1.274e-01, Batch size: 7, Momentum: 5.258e-01\n",
      "best loss: 1139.98\t\tLearning rate: 1.274e-01, Batch size: 37, Momentum: 7.361e-01\n",
      "best loss: 1148.44\t\tLearning rate: 4.281e-02, Batch size: 24, Momentum: 6.835e-01\n",
      "best loss: 1184.25\t\tLearning rate: 3.793e-01, Batch size: 44, Momentum: 3.155e-01\n",
      "best loss: 1194.39\t\tLearning rate: 4.281e-02, Batch size: 17, Momentum: 9.464e-01\n",
      "best loss: 1212.67\t\tLearning rate: 2.637e-07, Batch size: 47, Momentum: 0.000e+00\n",
      "best loss: 1220.35\t\tLearning rate: 3.793e-01, Batch size: 42, Momentum: 7.361e-01\n",
      "best loss: 1227.76\t\tLearning rate: 1.274e-01, Batch size: 22, Momentum: 9.464e-01\n",
      "best loss: 1239.29\t\tLearning rate: 1.129e+00, Batch size: 19, Momentum: 2.629e-01\n",
      "best loss: 1240.29\t\tLearning rate: 3.360e+00, Batch size: 29, Momentum: 2.629e-01\n",
      "best loss: 1251.28\t\tLearning rate: 3.793e-01, Batch size: 2, Momentum: 4.206e-01\n",
      "best loss: 1252.87\t\tLearning rate: 3.793e-01, Batch size: 17, Momentum: 8.938e-01\n",
      "best loss: 1260.40\t\tLearning rate: 1.129e+00, Batch size: 32, Momentum: 3.681e-01\n",
      "best loss: 1261.67\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 5.784e-01\n",
      "best loss: 1284.99\t\tLearning rate: 1.129e+00, Batch size: 37, Momentum: 7.361e-01\n",
      "best loss: 1311.95\t\tLearning rate: 1.000e+01, Batch size: 29, Momentum: 1.577e-01\n",
      "best loss: 1323.73\t\tLearning rate: 1.000e+01, Batch size: 7, Momentum: 1.577e-01\n",
      "best loss: 1357.28\t\tLearning rate: 1.000e+01, Batch size: 19, Momentum: 6.309e-01\n",
      "best loss: 1366.30\t\tLearning rate: 1.000e+01, Batch size: 44, Momentum: 4.206e-01\n",
      "best loss: 1372.19\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 3.155e-01\n",
      "best loss: 1375.07\t\tLearning rate: 1.000e+01, Batch size: 34, Momentum: 4.732e-01\n",
      "best loss: 1382.23\t\tLearning rate: 8.859e-08, Batch size: 14, Momentum: 5.258e-01\n",
      "best loss: 1390.07\t\tLearning rate: 1.000e-08, Batch size: 39, Momentum: 5.258e-01\n",
      "best loss: 1399.63\t\tLearning rate: 3.360e+00, Batch size: 17, Momentum: 9.990e-01\n",
      "best loss: 1954.86\t\tLearning rate: 2.976e-08, Batch size: 39, Momentum: 0.000e+00\n",
      "best loss: 62453.70\t\tLearning rate: 5.456e-04, Batch size: 19, Momentum: 5.258e-02\n",
      "best loss: 115471.02\t\tLearning rate: 8.859e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 140824.29\t\tLearning rate: 2.637e-07, Batch size: 39, Momentum: 2.629e-01\n",
      "best loss: 148705.91\t\tLearning rate: 8.859e-08, Batch size: 2, Momentum: 5.258e-01\n",
      "best loss: 200629.95\t\tLearning rate: 2.336e-06, Batch size: 32, Momentum: 2.629e-01\n",
      "best loss: 275045.01\t\tLearning rate: 2.637e-07, Batch size: 14, Momentum: 1.052e-01\n",
      "best loss: 378389.66\t\tLearning rate: 2.976e-08, Batch size: 4, Momentum: 5.258e-01\n",
      "best loss: 918301.82\t\tLearning rate: 1.129e+00, Batch size: 27, Momentum: 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# print the hyperparameters ranked from best to worst\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "for hp, loss in sorted_results:\n",
    "    print(f\"best loss: {loss:.2f}\\t\\tLearning rate: {hp[0]:.3e}, Batch size: {hp[1]}, Momentum: {hp[2]:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenotypes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
